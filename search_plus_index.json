{"./":{"url":"./","title":"Introduction","keywords":"","body":"uCore OS for LoongArch32 实验指导书 "},"lab0.html":{"url":"lab0.html","title":"Lab 0","keywords":"","body":"实验零：操作系统实验准备 "},"lab0/lab0_1_goals.html":{"url":"lab0/lab0_1_goals.html","title":"实验目的","keywords":"","body":"实验目的： 了解操作系统开发实验环境(Docker) 熟悉命令行方式的编译、调试工程 掌握基于硬件模拟器的调试技术 熟悉C语言编程和指针的概念 了解LoongArch32汇编语言 "},"lab0/lab0_2_prepare.html":{"url":"lab0/lab0_2_prepare.html","title":"准备知识","keywords":"","body":"准备知识 "},"lab0/lab0_2_1_about_labs.html":{"url":"lab0/lab0_2_1_about_labs.html","title":"了解OS实验","keywords":"","body":"了解OS实验 写一个操作系统难吗？别被现在上百万行的Linux和Windows操作系统吓倒。当年Thompson乘他老婆带着小孩度假留他一人在家时，写了UNIX；当年Linus还是一个21岁大学生时完成了Linux雏形。站在这些巨人的肩膀上，我们能否也尝试一下做“巨人”的滋味呢？ MIT的Frans Kaashoek等在2006年参考PDP-11上的UNIX Version 6写了一个可在X86上跑的操作系统xv6（基于MIT License），用于学生学习操作系统。我们可以站在他们的肩膀上，基于xv6的设计，尝试着一步一步完成一个从“空空如也”到“五脏俱全”的“麻雀”操作系统—ucore，此“麻雀”包含虚存管理、进程管理、处理器调度、同步互斥、进程间通信、文件系统等主要内核功能，总的内核代码量（C+asm）不会超过5K行。充分体现了“小而全”的指导思想。 ucore的运行环境可以是真实的LoongArch32计算机，不过考虑到调试和开发的方便，我们可采用LoongArch32硬件模拟器，比如QEMU等。ucore的开发环境主要是GCC中的gcc、gas、ld和MAKE等工具，对于代码编辑则可以采用Visual Studio Code等工具。对于软件的版本控制可以采用Git、SVN等工具。而对于不熟练这些工具的同学需要在版本之间进行简单的比较也可以使用Meld等软件。调试（deubg）实验有助于发现设计中的错误，可采用gdb（配合qemu）等调试工具软件。并可整个实验的运行环境和开发环境已经使用Docker打包，推荐使用Docker来直接运行我们的环境。 那我们准备如何一步一步来实现ucore呢？根据一个操作系统的设计实现过程，我们可以有如下的实验步骤： 用于了解操作系统启动前的状态和要做的准备工作，了解运行操作系统的硬件支持，理解例外，包括常规例外（例如中断、系统调用、其它异常等）与TLB例外； 物理内存管理子系统，理解LoongArch32的内存管理模式，了解操作系统如何管理物理内存； 虚拟内存管理子系统，通过页表机制实现TLB的填充以及缺页故障处理等； 内核线程子系统，用于了解如何创建相对与用户进程更加简单的内核态线程，如果对内核线程进行动态管理等； 用户进程管理子系统，用于了解用户态进程创建、执行、切换和结束的动态管理过程，了解在用户态通过系统调用得到内核态的内核服务的过程； 处理器调度子系统，用于理解操作系统的调度过程和调度算法； 同步互斥与进程间通信子系统，了解进程间如何进行信息交换和共享，并了解同步互斥的具体实现以及对系统性能的影响，研究死锁产生的原因，以及如何避免死锁； 文件系统，了解文件系统的具体实现，与进程管理等的关系，了解缓存对操作系统IO访问的性能改进，了解虚拟文件系统（VFS）、buffer cache和disk driver之间的关系。 其中每个开发步骤都是建立在上一个步骤之上的，就像搭积木，从一个一个小木块，最终搭出来一个小房子。在搭房子的过程中，完成从理解操作系统原理到实践操作系统设计与实现的探索过程。这个房子最终的建筑架构和建设进度如下图所示： 图1 ucore系统结构图 "},"lab0/lab0_2_2_environment.html":{"url":"lab0/lab0_2_2_environment.html","title":"设置实验环境","keywords":"","body":"设置实验环境 我们参考了MIT的xv6、Harvard的OS161和Linux等设计了ucore OS实验，所有OS实验需在Linux下运行。 "},"lab0/lab0_2_2_1_lab_steps.html":{"url":"lab0/lab0_2_2_1_lab_steps.html","title":"开发OS实验的简单步骤","keywords":"","body":"开发OS lab实验的简单步骤 按照每个实验指导书中的编译方法部分，将Makefile中的LAB CONFIG区域修改为当前实验需要的部分。 执行make clean 根据指导书，参阅uCore已有代码，了解uCore中一些模块的基本工作流程，然后完成每个练习的要求。 执行make qemu -j 16，运行当前实验，检查运行情况。 若运行不成功，可以分别在两个终端中使用make debug与make gdb，使用gdb对内核进行调试。 "},"lab0/lab0_2_2_2_docker_experiment.html":{"url":"lab0/lab0_2_2_2_docker_experiment.html","title":"通过Docker实现LoongArch32实验环境","keywords":"","body":"通过Docker使用LoongArch32编译环境 为了方便大家的使用，减少不必要的环境配置中带来的奇怪问题的负担，我们将环境统一打包为了一个Docker容器，这样就免去了自己配置环境的麻烦。 考虑到近年来ARM处理器普及，有的同学可能会使用ARM架构处理器的电脑来完成实验，但我们提供的LoongArch32编译环境是针对x86-64架构的，因此需要使用qemu-user进行转译。 例如在Apple M1处理器的电脑上使用Debian/Ubuntu操作系统的虚拟机或者是使用树莓派完成实验，请先安装： sudo apt install qemu-user qemu-user-static gcc-x86-64-linux-gnu binutils-x86-64-linux-gnu binutils-x86-64-linux-gnu-dbg build-essential 对于大家熟悉的x86-64架构上的Ubuntu系统，可以依次执行以下命令完成Docker的安装： 注意：其它Debian系发行版用户请正确将Ubuntu替换为你所使用的发行版，对于ARM处理器用户请将amd64替换为arm64（较老版本的树莓派为armhf）。 sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/debian \\ $(lsb_release -cs) \\ stable\" sudo apt-get update sudo apt-get install docker-ce 在Docker安装完成后，推荐将当前用户加入Docker组中，方便我们进行后续操作： sudo usermod -aG docker $USER newgrp docker # 注：在注销重新登录之前，所有的操作都需要先执行newgrp docker，推荐在这一步完成之后重启电脑 在Docker安装完成后，可以使用docker load来导入我们打包完成的运行环境。 cd $(存放la32-ucore-env.tgz的文件夹) docker load -i la32-ucore-env.tgz 之后，我们可以使用docker run开始运行我们的容器了，这里我们还做了一步额外的操作是挂载当前系统的用户相关文件以及用户主目录，这样可以在容器内直接使用当前用户的主目录： docker run -it \\ --name la32-env \\ --user=$(id -u $USER):$(id -g $USER) \\ --net=host \\ --workdir=\"/home/$USER\" \\ --volume=\"/home/$USER:/home/$USER\" \\ --volume=\"/etc/group:/etc/group:ro\" \\ --volume=\"/etc/passwd:/etc/passwd:ro\" \\ --volume=\"/etc/shadow:/etc/shadow:ro\" \\ --volume=\"/etc/sudoers.d:/etc/sudoers.d:ro\" \\ la32-ucore-env 参数解释： --rm 运行后销毁 --name 给容器命名 --user 切换到指定的用户和用户组 --net=host 与主机共享网络（默认采用Docker自带的NAT网卡，也就不可以直接用127.0.0.1连到容器） --env 设置环境变量 --workdir 进入容器后的工作文件夹 --volume 挂载到容器的文件夹 如果出现容器运行失败的情况，会导致容器出现残留，对于残留的容器我们可以使用sudo docker ps -a进行查看，若存在残留可以使用docker rm ($容器id)进行删除。 之后这就进入了我们提供的容器中，使用cd命令进入到ucore-loongarch32文件夹，然后输入make qemu可以观察操作系统运行情况，若运行成功则说明容器配置正确。 在Docker容器还在运行的时候，我们可以通过docker exec的方式继续启动一个该容器的终端，方便在容器内进行其它操作，使用命令docker exec -it la32-env /bin/zsh，其中/bin/zsh可以换成自己喜欢的Shell，如Bash。 而针对使用VSCode的同学，我们推荐可以直接安装VSCode的git插件，然后直接将VSCode挂到Docker容器中，操作如下： "},"lab0/lab0_2_2_3_1_softwares.html":{"url":"lab0/lab0_2_2_3_1_softwares.html","title":"实验中可能使用的软件","keywords":"","body":"实验中可能使用的软件 编辑器 这里我们推荐使用Visual Studio Code作为编辑器，原因是它可以很方便地使用Docker插件方便我们对LoongArch32的运行环境进行操作。 git 在本次实验中，我们采用Git作为版本控制工具，这样我们可以很方便地记录我们每次的修改，并在遇到问题的时候还原回自己的先前修改的版本。 "},"lab0/lab0_2_3_tools.html":{"url":"lab0/lab0_2_3_tools.html","title":"了解编程开发调试的基本工具","keywords":"","body":"了解编程开发调试的基本工具 在Ubuntu Linux中的C语言编程主要基于GNU C的语法，通过gcc来编译并生成最终执行文件。 "},"lab0/lab0_2_3_1_gcc_usage.html":{"url":"lab0/lab0_2_3_1_gcc_usage.html","title":"gcc的基本用法","keywords":"","body":"gcc的基本用法 我们提供的LoongArch32编译环境的Docker中已经提供了针对x86-64的GCC以及针对LoongArch32的GCC。 "},"lab0/lab0_2_3_1_1_compile_c_prog.html":{"url":"lab0/lab0_2_3_1_1_compile_c_prog.html","title":"编译简单的 C 程序","keywords":"","body":"编译简单的 C 程序 C 语言经典的入门例子是 Hello World，下面是一示例代码： #include int main(void) { printf(\"Hello, world!\\n\"); return 0; } 我们假定该代码存为文件‘hello.c’。要用 gcc 编译该文件，使用下面的命令： $ gcc -Wall hello.c -o hello 该命令将文件‘hello.c’中的代码编译为机器码并存储在可执行文件 ‘hello’中。机器码的文件名是通过 -o 选项指定的。该选项通常作为命令行中的最后一个参数。如果被省略，输出文件默认为 ‘a.out’。 注意到如果当前目录中与可执行文件重名的文件已经存在，它将被复盖。 选项 -Wall 开启编译器几乎所有常用的警告──强烈建议你始终使用该选项。编译器有很多其他的警告选项，但 -Wall 是最常用的。默认情况下GCC 不会产生任何警告信息。当编写 C 或 C++ 程序时编译器警告非常有助于检测程序存在的问题。 本例中，编译器使用了 -Wall 选项而没产生任何警告，因为示例程序是完全合法的。 要运行该程序，输入可执行文件的路径如下： $ ./hello Hello, world! 这将可执行文件载入内存，并使 CPU 开始执行其包含的指令。 路径 ./ 指代当前目录，因此 ./hello 载入并执行当前目录下的可执行文件 ‘hello’。 "},"lab0/lab0_2_3_1_2_la32_asm.html":{"url":"lab0/lab0_2_3_1_2_la32_asm.html","title":"LoongArch32汇编基本语法","keywords":"","body":"LoongArch32 汇编语法 这一节请参考LoongArch32指令集手册。 "},"lab0/lab0_2_3_1_3_gcc_inline_asm.html":{"url":"lab0/lab0_2_3_1_3_gcc_inline_asm.html","title":"GCC基本内联汇编","keywords":"","body":"GCC基本内联汇编 GCC 提供了两内内联汇编语句（inline asm statements）：基本内联汇编语句（basic inline asm statement)和扩展内联汇编语句（extended inline asm statement）。GCC基本内联汇编很简单，一般是按照下面的格式： asm(\"statements\"); 例如： asm(\"nop\"); \"asm\" 和 \"__asm__\" 的含义是完全一样的。如果有多行汇编，则每一行都要加上 \"\\n\\t\"。其中的 “\\n” 是换行符，\"\\t” 是 tab 符，在每条命令的 结束加这两个符号，是为了让 gcc 把内联汇编代码翻译成一般的汇编代码时能够保证换行和留有一定的空格。对于基本asm语句，GCC编译出来的汇编代码就是双引号里的内容。例如： asm( \"li $t0, 1\\n\\t\" \"li $t1, 2\\n\\t\" \"addi.w $t0, $t1, $t2\" ); 实际上gcc在处理汇编时，是要把asm(...)的内容\"打印\"到汇编文件中，所以格式控制字符是必要的。 在上面的例子中，由于我们在内联汇编中改变了 t0 和 t1 的值，但是由于 gcc 的特殊的处理方法，即先形成汇编文件，再交给 GAS 去汇编，所以 GAS 并不知道我们已经改变了 t0 和 t1 的值，如果程序的上下文需要 t0 或 t1 作其他内存单元或变量的暂存，就会产生没有预料的多次赋值，引起严重的后果。对于变量 _boo也存在一样的问题。为了解决这个问题，就要用到扩展 GCC 内联汇编语法。 参考： GCC Manual， 版本为5.0.0 pre-release,6.43节（How to Use Inline Assembly Language in C Code） GCC-Inline-Assembly-HOWTO "},"lab0/lab0_2_3_1_4_extend_gcc_asm.html":{"url":"lab0/lab0_2_3_1_4_extend_gcc_asm.html","title":"GCC扩展内联汇编","keywords":"","body":"GCC扩展内联汇编 使用GCC扩展内联汇编的例子如下： #define read_a0() ({ \\ unsigned int __dummy; \\ __asm__( \\ \"move $a0, %0\\n\\t\" \\ :\"=r\" (__dummy)); \\ __dummy; \\ }) 它代表什么含义呢？这需要从其基本格式讲起。GCC扩展内联汇编的基本格式是： asm [volatile] ( Assembler Template : Output Operands [ : Input Operands [ : Clobbers ] ]) 其中，__asm__ 表示汇编代码的开始，其后可以跟 __volatile__（这是可选项），其含义是避免 “asm” 指令被删除、移动或组合，在执行代码时，如果不希望汇编语句被 gcc 优化而改变位置，就需要在 asm 符号后添加 volatile 关键词：asm volatile(...)；或者更详细地说明为：__asm__ __volatile__(...)；然后就是小括弧，括弧中的内容是具体的内联汇编指令代码。 \"\" 为汇编指令部分，例如，\"movl %%cr0,%0\\n\\t\"。数字前加前缀 “％“，如％1，％2等表示使用寄存器的样板操作数。可以使用的操作数总数取决于具体CPU中通用寄存器的数 量，如LoongArch32可以有32个。指令中有几个操作数，就说明有几个变量需要与寄存器结合，由gcc在编译时根据后面输出部分和输入部分的约束条件进行相应的处理。 输出部分（output operand list），用以规定对输出变量（目标操作数）如何与寄存器结合的约束（constraint）,输出部分可以有多个约束，互相以逗号分开。每个约束以“＝”开头，接着用一个字母来表示操作数的类型，然后是关于变量结合的约束。例如，上例中： :\"=r\" (__dummy) “＝r”表示相应的目标操作数（指令部分的%0）可以使用任何一个通用寄存器，并且变量__dummy 存放在这个寄存器中，但如果是： :“＝m”(__dummy) “＝m”就表示相应的目标操作数是存放在内存单元__dummy中。表示约束条件的字母很多，下表给出几个主要的约束字母及其含义： 字母含义 m, v, o内存单元 R任何通用寄存器 I, h直接操作数 G任意 I常数（0～31） 输入部分（input operand list）：输入部分与输出部分相似，但没有“＝”。如果输入部分一个操作数所要求使用的寄存器，与前面输出部分某个约束所要求的是同一个寄存器，那就把对应操作数的编号（如“1”，“2”等）放在约束条件中。在后面的例子中，可看到这种情况。修改部分（clobber list,也称 乱码列表）:这部分常常以“memory”为约束条件，以表示操作完成后内存中的内容已有改变，如果原来某个寄存器的内容来自内存，那么现在内存中这个单元的内容已经改变。乱码列表通知编译器，有些寄存器或内存因内联汇编块造成乱码，可隐式地破坏了条件寄存器的某些位（字段）。 注意，指令部分为必选项，而输入部分、输出部分及修改部分为可选项，当输入部分存在，而输出部分不存在时，冒号“：”要保留，当“memory”存在时，三个冒号都要保留。 参考： GCC Manual， 版本为5.0.0 pre-release,6.43节（How to Use Inline Assembly Language in C Code） GCC-Inline-Assembly-HOWTO "},"lab0/lab0_2_3_2_make_makefile.html":{"url":"lab0/lab0_2_3_2_make_makefile.html","title":"make和Makefile","keywords":"","body":"make和Makefile GNU make(简称make)是一种代码维护工具，在大中型项目中，它将根据程序各个模块的更新情况，自动的维护和生成目标代码。 make命令执行时，需要一个 makefile （或Makefile）文件，以告诉make命令需要怎么样的去编译和链接程序。首先，我们用一个示例来说明makefile的书写规则。以便给大家一个感兴认识。这个示例来源于gnu的make使用手册，在这个示例中，我们的工程有8个c文件，和3个头文件，我们要写一个makefile来告诉make命令如何编译和链接这几个文件。我们的规则是： 如果这个工程没有编译过，那么我们的所有c文件都要编译并被链接。 如果这个工程的某几个c文件被修改，那么我们只编译被修改的c文件，并链接目标程序。 如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的c文件，并链接目标程序。 只要我们的makefile写得够好，所有的这一切，我们只用一个make命令就可以完成，make命令会自动智能地根据当前的文件修改的情况来确定哪些文件需要重编译，从而自己编译所需要的文件和链接目标程序。 2.3.2.1.1 makefile的规则 在讲述这个makefile之前，还是让我们先来粗略地看一看makefile的规则。 target ... : prerequisites ... command ... ... target也就是一个目标文件，可以是object file，也可以是执行文件。还可以是一个标签（label）。prerequisites就是，要生成那个target所需要的文件或是目标。command也就是make需要执行的命令（任意的shell命令）。 这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在 command中。如果prerequisites中有一个以上的文件比target文件要新，那么command所定义的命令就会被执行。这就是makefile的规则。也就是makefile中最核心的内容。 "},"lab0/lab0_2_3_3_gdb.html":{"url":"lab0/lab0_2_3_3_gdb.html","title":"gdb使用","keywords":"","body":"gdb使用 gdb 是功能强大的调试程序，可完成如下的调试任务： 设置断点 监视程序变量的值 程序的单步(step in/step over)执行 显示/修改变量的值 显示/修改寄存器 查看程序的堆栈情况 远程调试 调试线程 在可以使用 gdb 调试程序之前，必须使用 -g 或 –ggdb编译选项编译源文件。运行 gdb 调试程序时通常使用如下的命令： gdb progname 在 gdb 提示符处键入help，将列出命令的分类，主要的分类有： aliases：命令别名 breakpoints：断点定义； data：数据查看； files：指定并查看文件； internals：维护命令； running：程序执行； stack：调用栈查看； status：状态查看； tracepoints：跟踪程序执行。 键入 help 后跟命令的分类名，可获得该类命令的详细清单。gdb的常用命令如下表所示。 表 gdb 的常用命令 break FILENAME:NUM在特定源文件特定行上设置断点 clear FILENAME:NUM删除设置在特定源文件特定行上的断点 run运行调试程序 step单步执行调试程序，不会直接执行函数 next单步执行调试程序，会直接执行函数 backtrace显示所有的调用栈帧。该命令可用来显示函数的调用顺序 where continue继续执行正在调试的程序 display EXPR每次程序停止后显示表达式的值,表达式由程序定义的变量组成 file FILENAME装载指定的可执行文件进行调试 help CMDNAME显示指定调试命令的帮助信息 info break显示当前断点列表，包括到达断点处的次数等 info files显示被调试文件的详细信息 info func显示被调试程序的所有函数名称 info prog显示被调试程序的执行状态 info local显示被调试程序当前函数中的局部变量信息 info var显示被调试程序的所有全局和静态变量名称 kill终止正在被调试的程序 list显示被调试程序的源代码 quit退出 gdb gdb调试实例 下面以一个有错误的例子程序来介绍gdb的使用： /*bugging.c*/ #include #include static char buff [256]; static char* string; int main () { printf (\"Please input a string: \"); gets (string); printf (\"\\nYour string is: %s\\n\", string); } 这个程序是接受用户的输入，然后将用户的输入打印出来。该程序使用了一个未经过初始化的字符串地址 string，因此，编译并运行之后，将出现 \"Segment Fault\"错误： $ gcc -o bugging -g bugging.c $ ./bugging Please input a string: asdf Segmentation fault (core dumped) 为了查找该程序中出现的问题，我们利用 gdb，并按如下的步骤进行： [1] 运行 “gdb bugging” ，加载 bugging 可执行文件； $gdb bugging [2] 执行装入的 bugging 命令； (gdb) run [3] 使用 where 命令查看程序出错的地方； (gdb) where [4] 利用 list 命令查看调用 gets 函数附近的代码； (gdb) list [5] 在 gdb 中，我们在第 11 行处设置断点，看看是否是在第11行出错； (gdb) break 11 [6] 程序重新运行到第 11 行处停止，这时程序正常，然后执行单步命令next； (gdb) next [7] 程序确实出错，能够导致 gets 函数出错的因素就是变量 string。重新执行测试程，用 print 命令查看 string 的值； (gdb) run (gdb) print string (gdb) $1=0x0 [8] 问题在于string指向的是一个无效指针，修改程序，在10行和11行之间增加一条语句 “string=buff; ”，重新编译程序，然后继续运行，将看到正确的程序运行结果。 用gdb查看源代码可以用list命令，但是这个不够灵活。可以使用\"layout src\"命令，或者按Ctrl-X再按A，就会出现一个窗口可以查看源代码。也可以用使用-tui参数，这样进入gdb里面后就能直接打开代码查看窗口。其他代码窗口相关命令： info win显示窗口的大小 layout next切换到下一个布局模式 layout prev切换到上一个布局模式 layout src只显示源代码 layout asm只显示汇编代码 layout split显示源代码和汇编代码 layout regs增加寄存器内容显示 focus cmd/src/asm/regs/next/prev切换当前窗口 refresh刷新所有窗口 tui reg next显示下一组寄存器 tui reg system显示系统寄存器 update更新源代码窗口和当前执行点 winheight name +/- line调整name窗口的高度 tabset nchar设置tab为nchar个字符 "},"lab0/lab0_2_3_4_further.html":{"url":"lab0/lab0_2_3_4_further.html","title":"进一步的相关内容","keywords":"","body":"进一步的相关内容 请同学网上搜寻相关资料学习： gcc tools相关文档 版本管理软件（如GIT）的使用 … "},"lab0/lab0_2_4_debug_with_emulator.html":{"url":"lab0/lab0_2_4_debug_with_emulator.html","title":"基于硬件模拟器实现源码级调试","keywords":"","body":"基于硬件模拟器实现源码级调试 "},"lab0/lab0_2_4_1_install_qemu.html":{"url":"lab0/lab0_2_4_1_install_qemu.html","title":"安装硬件模拟器QEMU","keywords":"","body":"安装硬件模拟器QEMU 在我们提供的LoongArch32的Docker镜像中已经包含了一个QEMU。因此这里只要使用了给定的Docker镜像就可以直接使用了。 "},"lab0/lab0_2_4_2_qemu_usage.html":{"url":"lab0/lab0_2_4_2_qemu_usage.html","title":"使用硬件模拟器QEMU","keywords":"","body":"使用硬件模拟器QEMU "},"lab0/lab0_2_4_2_1_qemu_runtime_arguments.html":{"url":"lab0/lab0_2_4_2_1_qemu_runtime_arguments.html","title":"运行参数","keywords":"","body":"运行参数 在提供的Docker中，在命令行中可以直接使用 qemu-system-loongson32 命令运行程序。qemu 运行可以有多参数，格式如： qemu-system-loongson32 [options] 部分参数说明： -m ls3a5k32 模拟ls3a5k32机器 -monitor tcp::4288,server,nowait 将monitor监听在tcp4288端口，这样我们可以通过nc来连接 -serial stdio 将模拟的硬件串口连接到stdio（标准输入输出），这样我们可以直接在命令行与QEMU虚拟机内的串口进行交互 -m 256 设置内存256M -nographic 禁止使用图形输出，方便终端调试。 -kernel obj/ucore-kernel-initrd 指定QEMU运行时载入obj/ucore-kernel-initrd的内核 -S 在启动时不立刻启动CPU -s 相当于-gdb tcp::1234，将gdbserver监听到本地的1234端口。 例如，如果只是要简单地运行qemu，可以输入： qemu-system-loongson32 -kernel obj/ucore-kernel-initrd -M ls3a5k32 -m 256 -nographic 注意：没有指定-monitor参数的情况下可以使用Ctrl+A进入monitor。若在这种情况需要退出QEMU，无法直接使用Ctrl+C，需要先Ctrl+A然后按X。 "},"lab0/lab0_2_4_4_gdb_qemu_debug_ucore.html":{"url":"lab0/lab0_2_4_4_gdb_qemu_debug_ucore.html","title":"结合gdb和qemu源码级调试ucore","keywords":"","body":"结合gdb和qemu源码级调试ucore "},"lab0/lab0_2_4_4_1_make_obj.html":{"url":"lab0/lab0_2_4_4_1_make_obj.html","title":"编译可调试的目标文件","keywords":"","body":"编译可调试的目标文件 为了使得编译出来的代码是能够被gdb这样的调试器调试，我们需要在使用gcc编译源文件的时候添加参数：\"-g\"。这样编译出来的目标文件中才会包含可以用于调试器进行调试的相关符号信息。 "},"lab0/lab0_2_4_4_2_ucore_make.html":{"url":"lab0/lab0_2_4_4_2_ucore_make.html","title":"ucore 代码编译","keywords":"","body":"ucore 代码编译 编译过程：在解压缩后的 ucore 源码包中使用 make 命令即可。例如如果要运行位于sys_code中我们已经完成的代码： chy@laptop: ~/sys_code$ make qemu -j 16 在sys_code目录下的obj目录中，生成了最终目标文件： ucore-kernel-initrd：包含磁盘部分的Kernel的文件 还生成了其他很多文件，这里就不一一列举了。 注意，这里的-j 16指的是编译时最多开启16线程，这里推荐将16修改为你使用的电脑CPU的逻辑线程数。 "},"lab0/lab0_2_4_4_3_remote_debug.html":{"url":"lab0/lab0_2_4_4_3_remote_debug.html","title":"使用远程调试","keywords":"","body":"使用远程调试 为了与qemu配合进行源代码级别的调试，需要先让qemu进入等待gdb调试器的接入并且还不能让qemu中的CPU执行，因此启动qemu的时候，我们需要使用参数-S –s这两个参数来做到这一点。在使用了前面提到的参数启动qemu之后，qemu中的CPU并不会马上开始执行，这时我们启动gdb，然后在gdb命令行界面下，使用下面的命令连接到qemu： (gdb) target remote 127.0.0.1:1234 然后输入c（也就是continue）命令之后，qemu会继续执行下去，但是gdb由于不知道任何符号信息，并且也没有下断点，是不能进行源码级的调试的。为了让gdb获知符号信息，需要指定调试目标文件，gdb中使用file命令： (gdb) file ./bin/kernel 之后gdb就会载入这个文件中的符号信息了。 通过gdb可以对ucore代码进行调试，以lab0中调试memset函数为例： (1) 运行 qemu-system-loongson32 -kernel obj/ucore-kernel-initrd -M ls3a5k32 -m 256 -nographic -S -s (2) 运行 gdb并与qemu进行连接 gdb obj/ucore-kernel-initrd (3) 设置断点并执行 (4) qemu 单步调试。 运行过程以及结果如下： 窗口一窗口二 ➜ ucore-loongarch32 git:(master) qemu-system-loongson32 -kernel obj/ucore-kernel-initrd -M ls3a5k32 -m 256 -nographic -S -s mips_ls3a7a_init: num_nodes 1 mips_ls3a7a_init: node 0 mem 0x10000000 *****zl 1, mask0 memory_offset = 0x78; cpu_offset = 0xc88; system_offset = 0xce8; irq_offset = 0x3058; interface_offset = 0x30b8; boot_params_buf is param len=0x89f0 env a8f00020 ➜ ~ docker exec -it la32-env /bin/zsh ➜ ~ echo \"set auto-load safe-path /\" >> ~/.gdbinit ➜ ~ cd ucore-loongarch32 ➜ ucore-loongarch32 git:(master) loongarch32-linux-gnu-gdb obj/ucore-kernel-initrd For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\"... Reading symbols from obj/ucore-kernel-initrd...done. 0xa0000000 in wrs_kernel_text_start () (gdb) break memset Breakpoint 1 at 0xa0001314: file kern/libs/string.c, line 277. (gdb) c Continuing. Breakpoint 1, memset (s=0xa1ffc000, c=0 '\\000', n=4096) at kern/libs/string.c:277 277 char *p = s; (gdb) "},"lab0/lab0_2_4_4_4_gdb_config_file.html":{"url":"lab0/lab0_2_4_4_4_gdb_config_file.html","title":"使用gdb配置文件","keywords":"","body":"使用gdb配置文件 在上面可以看到，为了进行源码级调试，需要输入较多的东西，很麻烦。为了方便，可以将这些命令存在脚本中，并让gdb在启动的时候自动载入。 以lab0为例，实验根目录下自带了一个.gdbinit文件，并输入下面的内容： target remote 127.0.0.1:1234 file obj/ucore-kernel-initrd gdb启动的时候会自动加载用户主目录下的.gdbinit文件与当前执行目录下的.gdbinit文件，也可以这样手动载入： $ gdb -x .gdbinit 如果觉得这个命令太长，可以将这个命令存入一个文件中，当作脚本来执行。 另外，如果直接使用上面的命令，那么得到的界面是一个纯命令行的界面，不够直观。 如果想获得上图那样的效果，只需要再加上参数-tui就行了，比如： gdb -tui -x .gdbinit "},"lab0/lab0_2_5_cpu_hardware.html":{"url":"lab0/lab0_2_5_cpu_hardware.html","title":"了解处理器硬件","keywords":"","body":"了解处理器硬件 要想深入理解ucore，就需要了解支撑ucore运行的硬件环境，即了解处理器体系结构（了解硬件对ucore带来影响）和机器指令集（读懂ucore的汇编）。 该版本的ucore目前支持的硬件环境是基于LoongArch32架构的计算机系统。更多的硬件相关内容（比如保护模式等）将随着实现ucore的过程逐渐展开介绍。 "},"lab0/lab0_2_5_1_la32_modes.html":{"url":"lab0/lab0_2_5_1_la32_modes.html","title":"LoongArch32运行模式","keywords":"","body":"LoongArch32的运行模式 根据LoongArch32文档介绍，运行模式包括两种特权级：PLV0与PLV3，分别是内核态与用户态，这使得一般应用不能破坏操作系统内核和执行特权指令。 该模式信息存储于LoongArch32架构的CSR寄存器中，CRMD存储的是当前运行状态的信息，PRMD存储的是出现例外后，例外之前的状态信息。（由于例外必须进入内核态进行处理）。 此外一个非常重要的特性是，我们可以通过ertn指令将PRMD寄存器的结果还原到CRMD寄存器。这就让操作系统处理例外变得更加简单。 "},"lab0/lab0_2_5_2_la32_mem.html":{"url":"lab0/lab0_2_5_2_la32_mem.html","title":"LoongArch32内存架构","keywords":"","body":"LoongArch32内存结构 地址是访问内存空间的索引。一般而言，内存地址有两个：一个是CPU通过总线访问物理内存用到的物理地址，一个是我们编写的应用程序所用到的逻辑地址（也有人称为虚拟地址）。比如如下C代码片段： int boo=1; int *foo=&a; 这里的boo是一个整型变量，foo变量是一个指向boo地址的整型指针变量，foo中储存的内容就是boo的逻辑地址。 LoongArch32是32位的处理器，即可以寻址的物理内存地址空间为2^32=4G字节。为更好理解面向LoongArch32处理器的ucore操作系统，需要用到两个地址空间的概念：物理地址和逻辑地址。物理内存地址空间是处理器提交到总线上用于访问计算机系统中的内存和外设的最终地址。一个计算机系统中只有一个物理地址空间。在LoongArch32架构中，逻辑地址空间由DMW与页表机制共同配置。 这里的DMW是物理地址配置窗口，它可以直接将我们指定的物理地址的高3位转换为虚拟地址的高3位，并配置Cache一致性相关设置，显然我们可以发现，这样一个窗口的大小也就是2^(32-3) Bytes，也就是512MB。而对于内核来说，如果将内核所使用的地址空间直接映射到物理内存，那么程序编写起来更为方便。在LoongArch32架构中，DMW只有2个。 而页表机制则是现在操作系统广泛使用的一个内存管理方式，LoongArch32架构只支持4KB和4MB两种页面大小。然而这与我们熟悉的一些主流架构采用的设置一个页表基地址寄存器（如x86上的CR3），然后处理器自动在这个地址按照架构规定的页表格式进行页表的查找与TLB填充不同，LoongArch32架构采用的是软件填充TLB的方式。而当TLB中不存在我们所需要寻找的页面时，则会触发例外然后交给操作系统内核完成TLB的填充。因此相对于x86、ARM、RISC-V等架构，这一填充页表的方式效率相对较低，但易于教学使用，硬件上实现较为简单，LoongArch(不带32)则是与主流架构一样采用了硬件填充页表的方式。此外，在LoongArch32架构中采用了奇偶页表的形式，每个页面号在二进制下最低位为0和1的两个页面作为一组奇偶页面。在TLB中两个奇偶页面作为一个表项，它们可以指向不同的物理页帧，但这也导致了两个虚拟虚拟页面必须同时分配。因此为了简化该问题，uCore操作系统的LoongArch32版本采用了两个虚拟页面连续存放的方式进行。等同于将原本的4KB页面直接变成了8KB。 这里需要注意的是，DMW机制优先于页表机制。 "},"lab0/lab0_2_5_3_la32_regs.html":{"url":"lab0/lab0_2_5_3_la32_regs.html","title":"LoongArch32寄存器","keywords":"","body":"LoongArch32寄存器 LoongArch32的通用寄存器有32个，记为r0~r31。其中0号寄存器的值恒为0。但这些寄存器也根据ABI有一定的特殊用途，表如下： 寄存器号 助记符 用途 0 zero 恒定为0 1 ra 返回地址 2 tp 3 sp 栈指针 4 v0 | a0 函数调用的返回值或第1个参数共用 5 v1 | a1 函数调用的第2返回值或第2个参数共用 6 a2 第3个参数 7 a3 第4个参数 8 a4 第5个参数 9 a5 第6个参数 10 a6 第7个参数 11 a7 第8个参数 12 t0 第1个临时寄存器 13 t1 第2个临时寄存器 14 t2 第3个临时寄存器 15 t3 第4个临时寄存器 16 t4 第5个临时寄存器 17 t5 第6个临时寄存器 18 t6 第7个临时寄存器 19 t7 第8个临时寄存器 20 t8 第9个临时寄存器 21 reserved_reg 暂时保留，无特定用途 22 fp 栈帧指针 23 s0 暂存寄存器0 24 s1 暂存寄存器1 25 s2 暂存寄存器2 26 s3 暂存寄存器3 27 s4 暂存寄存器4 28 s5 暂存寄存器5 29 s6 暂存寄存器6 30 s7 暂存寄存器7 31 s8 暂存寄存器8 除此之外，还有CSR状态控制寄存器，这一部分寄存器本身不重要，但是其功能配合异常处理与特权指令非常重要，这一部分请读者越多LoongArch32文档的第7章部分。 "},"lab0/lab0_2_6_ucore_programming.html":{"url":"lab0/lab0_2_6_ucore_programming.html","title":"了解ucore编程方法和通用数据结构","keywords":"","body":"了解ucore编程方法和通用数据结构 "},"lab0/lab0_2_6_1_oop.html":{"url":"lab0/lab0_2_6_1_oop.html","title":"面向对象编程方法","keywords":"","body":"面向对象编程方法 uCore设计中采用了一定的面向对象编程方法。虽然C 语言对面向对象编程并没有原生支持，但没有原生支持并不等于我们不能用 C 语言写面向对象程序。需要注意，我们并不需要用 C语言模拟出一个常见 C++ 编译器已经实现的对象模型。如果是这样，还不如直接采用C++编程。 uCore的面向对象编程方法，目前主要是采用了类似C++的接口（interface）概念，即是让实现细节不同的某类内核子系统（比如物理内存分配器、调度器，文件系统等）有共同的操作方式，这样虽然内存子系统的实现千差万别，但它的访问接口是不变的。这样不同的内核子系统之间就可以灵活组合在一起，实现风格各异，功能不同的操作系统。接口在 C 语言中，表现为一组函数指针的集合。放在 C++ 中，即为虚表。接口设计的难点是如果找出各种内核子系统的共性访问/操作模式，从而可以根据访问模式提取出函数指针列表。 比如对于uCore内核中的物理内存管理子系统，首先通过分析内核中其他子系统可能对物理内存管理子系统，明确物理内存管理子系统的访问/操作模式，然后我们定义了pmm_manager数据结构（位于lab2/kern/mm/pmm.h）如下： // pmm_manager is a physical memory management class. A special pmm manager - XXX_pmm_manager // only needs to implement the methods in pmm_manager class, then XXX_pmm_manager can be used // by ucore to manage the total physical memory space. struct pmm_manager { // XXX_pmm_manager's name const char *name; // initialize internal description&management data structure // (free block list, number of free block) of XXX_pmm_manager void (*init)(void); // setup description&management data structcure according to // the initial free physical memory space void (*init_memmap)(struct Page *base, size_t n); // allocate >=n pages, depend on the allocation algorithm struct Page *(*alloc_pages)(size_t n); // free >=n pages with \"base\" addr of Page descriptor structures(memlayout.h) void (*free_pages)(struct Page *base, size_t n); // return the number of free pages size_t (*nr_free_pages)(void); // check the correctness of XXX_pmm_manager void (*check)(void); }; 这样基于此数据结构，我们可以实现不同连续内存分配算法的物理内存管理子系统，而这些物理内存管理子系统需要编写算法，把算法实现在此结构中定义的init（初始化）、init_memmap（分析空闲物理内存并初始化管理）、alloc_pages（分配物理页）、free_pages（释放物理页）函数指针所对应的函数中。而其他内存子系统需要与物理内存管理子系统交互时，只需调用特定物理内存管理子系统所采用的pmm_manager数据结构变量中的函数指针即可 "},"lab0/lab0_2_6_2_generic_data_structure.html":{"url":"lab0/lab0_2_6_2_generic_data_structure.html","title":"通用数据结构双向循环链表","keywords":"","body":"通用数据结构 "},"lab0/lab0_2_6_2_1_linked_list.html":{"url":"lab0/lab0_2_6_2_1_linked_list.html","title":"双向循环链表","keywords":"","body":"双向循环链表 在“数据结构”课程中，如果创建某种数据结构的双循环链表，通常采用的办法是在这个数据结构的类型定义中有专门的成员变量 data, 并且加入两个指向该类型的指针next和prev。例如： typedef struct foo { ElemType data; struct foo *prev; struct foo *next; } foo_t; 双向循环链表的特点是尾节点的后继指向首节点，且从任意一个节点出发，沿两个方向的任何一个，都能找到链表中的任意一个节点的data数据。由双向循环列表形成的数据链如下所示： 这种双向循环链表数据结构的一个潜在问题是，虽然链表的基本操作是一致的，但由于每种特定数据结构的类型不一致，需要为每种特定数据结构类型定义针对这个数据结构的特定链表插入、删除等各种操作，会导致代码冗余。 在uCore内核中使用了大量的双向循环链表结构来组织数据，包括空闲内存块列表、内存页链表、进程列表、设备链表、文件系统列表等的数据组织（在[labX/libs/list.h]实现），但其具体实现借鉴了Linux内核的双向循环链表实现，与“数据结构”课中的链表数据结构不太一样。下面将介绍这一数据结构的设计与操作函数。 uCore的双向链表结构定义为： struct list_entry { struct list_entry *prev, *next; }; 需要注意uCore内核的链表节点list_entry没有包含传统的data数据域，，而是在具体的数据结构中包含链表节点。以空闲内存块列表为例，空闲块链表的头指针定义（位于kern/mm/memlayout.h中）为： /* free_area_t - maintains a doubly linked list to record free (unused) pages */ typedef struct { list_entry_t free_list; // the list header unsigned int nr_free; // # of free pages in this free list } free_area_t; 而每一个空闲块链表节点定义（位于kern/mm/memlayout）为： /* * * struct Page - Page descriptor structures. Each Page describes one * physical page. In kern/mm/pmm.h, you can find lots of useful functions * that convert Page to other data types, such as phyical address. * */ struct Page { atomic_t ref; // page frame's reference counter …… list_entry_t page_link; // free list link }; 这样以free_area_t结构的数据为双向循环链表的链表头指针，以Page结构的数据为双向循环链表的链表节点，就可以形成一个完整的双向循环链表，如下图所示： 图 空闲块双向循环链表 从上图中我们可以看到，这种通用的双向循环链表结构避免了为每个特定数据结构类型定义针对这个数据结构的特定链表的麻烦，而可以让所有的特定数据结构共享通用的链表操作函数。在实现对空闲块链表的管理过程（参见kern/mm/default_pmm.c）中，就大量使用了通用的链表插入，链表删除等操作函数。有关这些链表操作函数的定义如下。 (1) 初始化 uCore只定义了链表节点，并没有专门定义链表头，那么一个双向循环链表是如何建立起来的呢？让我们来看看list_init这个内联函数（inline funciton）： static inline void list_init(list_entry_t *elm) { elm->prev = elm->next = elm; } 参看文件default_pmm.c的函数default_init，当我们调用list_init(&(free_area.free_list))时，就声明一个名为free_area.free_list的链表头时，它的next、prev指针都初始化为指向自己，这样，我们就有了一个表示空闲内存块链的空链表。而且我们可以用头指针的next是否指向自己来判断此链表是否为空，而这就是内联函数list_empty的实现。 (2) 插入 对链表的插入有两种操作，即在表头插入（list_add_after）或在表尾插入（list_add_before）。因为双向循环链表的链表头的next、prev分别指向链表中的第一个和最后一个节点，所以，list_add_after和list_add_before的实现区别并不大，实际上uCore分别用list_add(elm, listelm, listelm->next)和list_add(elm, listelm->prev, listelm)来实现在表头插入和在表尾插入。而__list_add的实现如下： static inline void __list_add(list_entry_t *elm, list_entry_t *prev, list_entry_t *next) { prev->next = next->prev = elm; elm->next = next; elm->prev = prev; } 从上述实现可以看出在表头插入是插入在listelm之后，即插在链表的最前位置。而在表尾插入是插入在listelm->prev之后，即插在链表的最后位置。注：list_add等于list_add_after。 (3) 删除 当需要删除空闲块链表中的Page结构的链表节点时，可调用内联函数list_del，而list_del进一步调用了__list_del来完成具体的删除操作。其实现为： static inline void list_del(list_entry_t *listelm) { __list_del(listelm->prev, listelm->next); } static inline void __list_del(list_entry_t *prev, list_entry_t *next) { prev->next = next; next->prev = prev; } 如果要确保被删除的节点listelm不再指向链表中的其他节点，这可以通过调用list_init函数来把listelm的prev、next指针分别自身，即将节点置为空链状态。这可以通过list_del_init函数来完成。 (4) 访问链表节点所在的宿主数据结构 通过上面的描述可知，list_entry_t通用双向循环链表中仅保存了某特定数据结构中链表节点成员变量的地址，那么如何通过这个链表节点成员变量访问到它的所有者（即某特定数据结构的变量）呢？Linux为此提供了针对数据结构XXX的le2XXX(le, member)的宏，其中le，即list entry的简称，是指向数据结构XXX中list_entry_t成员变量的指针，也就是存储在双向循环链表中的节点地址值， member则是XXX数据类型中包含的链表节点的成员变量。例如，我们要遍历访问空闲块链表中所有节点所在的基于Page数据结构的变量，则可以采用如下编程方式（基于kern/mm/default_pmm.c）： //free_area是空闲块管理结构，free_area.free_list是空闲块链表头 free_area_t free_area; list_entry_t * le = &free_area.free_list; //le是空闲块链表头指针 while((le=list_next(le)) != &free_area.free_list) { //从第一个节点开始遍历 struct Page *p = le2page(le, page_link); //获取节点所在基于Page数据结构的变量 …… } le2page宏（定义位于kern/mm/memlayout.h）的使用相当简单： // convert list entry to page #define le2page(le, member) \\ to_struct((le), struct Page, member) 而相比之下，它的实现用到的to_struct宏和offsetof宏（定义位于kern/include/defs.h）则有一些难懂： /* Return the offset of 'member' relative to the beginning of a struct type */ #define offsetof(type, member) \\ ((size_t)(&((type *)0)->member)) /* * * to_struct - get the struct from a ptr * @ptr: a struct pointer of member * @type: the type of the struct this is embedded in * @member: the name of the member within the struct * */ #define to_struct(ptr, type, member) \\ ((type *)((char *)(ptr) - offsetof(type, member))) 这里采用了一个利用gcc编译器技术的技巧，即先求得数据结构的成员变量在本宿主数据结构中的偏移量，然后根据成员变量的地址反过来得出属主数据结构的变量的地址。 我们首先来看offsetof宏，size_t最终定义与CPU体系结构相关，本实验都采用Loongarch32 CPU，顾size_t等价于 unsigned int。 ((type *)0)->member的设计含义是什么？其实这是为了求得数据结构的成员变量在本宿主数据结构中的偏移量。为了达到这个目标，首先将0地址强制\"转换\"为type数据结构（比如struct Page）的指针，再访问到type数据结构中的member成员（比如page_link）的地址，即是type数据结构中member成员相对于数据结构变量的偏移量。在offsetof宏中，这个member成员的地址（即“&((type *)0)->member)”）实际上就是type数据结构中member成员相对于数据结构变量的偏移量。对于给定一个结构，offsetof(type,member)是一个常量，to_struct宏正是利用这个不变的偏移量来求得链表数据项的变量地址。接下来再分析一下to_struct宏，可以发现 to_struct宏中用到的ptr变量是链表节点的地址，把它减去offsetof宏所获得的数据结构内偏移量，即就得到了包含链表节点的属主数据结构的变量的地址。 "},"lab0/lab0_ref_ucore-tools.html":{"url":"lab0/lab0_ref_ucore-tools.html","title":"附录A.ucore实验中的常用工具","keywords":"","body":"ucore实验中的常用工具 在ucore实验中，一些基本的常用工具如下： 命令行shell: bash shell -- 有对文件和目录操作的各种命令，如ls、cd、rm、pwd... 系统维护工具：apt、git apt：安装管理各种软件，主要在debian, ubuntu linux系统中 git：开发软件的版本维护工具 源码阅读与编辑工具：eclipse-CDT、understand、gedit、vim Eclipse-CDT：基于Eclipse的C/C++集成开发环境、跨平台、丰富的分析理解代码的功能，可与qemu结合，联机源码级Debug uCore OS。 Understand：商业软件、跨平台、丰富的分析理解代码的功能，Windows上有类似的sourceinsight软件 gedit：Linux中的常用文本编辑，Windows上有类似的notepad vim: Linux/unix中的传统编辑器，类似有emacs等，可通过exuberant-ctags、cscope等实现代码定位 源码比较和打补丁工具：diff、meld，用于比较不同目录或不同文件的区别, patch是打补丁工具 diff, patch是命令行工具，使用简单 meld是图形界面的工具，功能相对直观和方便，类似的工具还有 kdiff3、diffmerge、P4merge 开发编译调试工具：gcc 、gdb 、make gcc：C语言编译器 gdb：执行程序调试器 ld：链接器 objdump：对ELF格式执行程序文件进行反编译、转换执行格式等操作的工具 nm：查看执行文件中的变量、函数的地址 readelf：分析ELF格式的执行程序文件 make：软件工程管理工具， make命令执行时，需要一个 makefile 文件，以告诉make命令如何去编译和链接程序 dd：读写数据到文件和设备中的工具 硬件模拟器：qemu -- qemu可模拟多种CPU硬件环境，本实验中，用于模拟一台LoongArch32的计算机系统。 markdown文本格式的编写和阅读工具(比如阅读ucore_docs) 编写工具 haroopad 阅读工具 gitbook 上述工具的使用方法在线信息 apt-get http://wiki.ubuntu.org.cn/Apt-get%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97 git github http://www.cnblogs.com/cspku/articles/Git_cmds.html http://www.worldhello.net/gotgithub/index.html diff patch http://www.ibm.com/developerworks/cn/linux/l-diffp/index.html http://www.cnblogs.com/itech/archive/2009/08/19/1549729.html gcc http://wiki.ubuntu.org.cn/Gcchowto http://wiki.ubuntu.org.cn/Compiling_Cpp http://wiki.ubuntu.org.cn/C_Cpp_IDE http://wiki.ubuntu.org.cn/C%E8%AF%AD%E8%A8%80%E7%AE%80%E8%A6%81%E8%AF%AD%E6%B3%95%E6%8C%87%E5%8D%97 gdb http://wiki.ubuntu.org.cn/%E7%94%A8GDB%E8%B0%83%E8%AF%95%E7%A8%8B%E5%BA%8F make & makefile http://wiki.ubuntu.com.cn/index.php?title=%E8%B7%9F%E6%88%91%E4%B8%80%E8%B5%B7%E5%86%99Makefile&variant=zh-cn http://blog.csdn.net/a_ran/article/details/43937041 shell http://wiki.ubuntu.org.cn/Shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80 http://wiki.ubuntu.org.cn/%E9%AB%98%E7%BA%A7Bash%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97 understand http://blog.csdn.net/qwang24/article/details/4064975 vim http://www.httpy.com/html/wangluobiancheng/Perljiaocheng/2014/0613/93894.html http://wenku.baidu.com/view/4b004dd5360cba1aa811da77.html meld https://linuxtoy.org/archives/meld-2.html qemu http://wenku.baidu.com/view/04c0116aa45177232f60a2eb.html Eclipse-CDT http://blog.csdn.net/anzhu_111/article/details/5946634 haroopad http://pad.haroopress.com/ gitbook https://github.com/GitbookIO/gitbook https://www.gitbook.com/ "},"lab0/lab0_ref_ucore-resource.html":{"url":"lab0/lab0_ref_ucore-resource.html","title":"附录B.ucore实验参考资料","keywords":"","body":"参考资料 一些资料信息来源于 http://pdos.csail.mit.edu/6.828/2014/reference.html UNIX general info Youtube Unix intro The UNIX Time-Sharing System, Dennis M. Ritchie and Ken L.Thompson,. Bell System Technical Journal 57, number 6, part 2 (July-August 1978) pages 1905-1930. The Evolution of the Unix Time-sharing System, Dennis M. Ritchie, 1979. The C programming language (second edition) by Kernighan and Ritchie. Prentice Hall, Inc., 1988. ISBN 0-13-110362-8, 1998. building or reading a small OS How to make an Operating System xv6 book 中文 自己动手写操作系统于渊 著,电子工业出版社,2005 Linux-0.11内核完全注释 赵炯，2009 oldlinux论坛 osdev.org some OS course 6.828: Operating Systems Engineering - in MIT CS-537: Introduction to Operating Systems - in WISC 16550 UART Serial Port PC16550D Universal Asynchronous Receiver/Transmitter with FIFOs, National Semiconductor, 1995. http://byterunner.com/16550.html, Byterunner Technologies. Interfacing the Serial / RS232 Port,, Craig Peacock, August 2001. "},"lab1.html":{"url":"lab1.html","title":"Lab 1","keywords":"","body":"实验一：系统软件启动过程 "},"lab1/lab1_1_goals.html":{"url":"lab1/lab1_1_goals.html","title":"实验目的","keywords":"","body":"实验目的： 操作系统是一个软件，也需要通过某种机制加载并运行它。对于LoongArch32的计算机来说，上电复位最初启动的是一个BIOS软件（例如PMON），该BIOS软件能够支持从网络加载ELF格式的操作系统内核，从而开始启动我们已经编译好的uCore内核。 而对于QEMU虚拟机而言，我们可以直接使用-kernel来指定我们需要加载的内核的ELF文件，从而直接完成了内核的载入过程，并直接从ELF的入口点开始启动。 ucore OS软件 编译运行ucore OS的过程 ucore OS的启动过程 调试ucore OS的方法 函数调用关系：在汇编级了解函数调用栈的结构和处理过程 中断管理：与软件相关的中断处理 外设管理：时钟 "},"lab1/lab1_2_labs.html":{"url":"lab1/lab1_2_labs.html","title":"实验内容","keywords":"","body":"实验内容： lab1中包含一个OS。这lab1中的OS只是一个可以处理时钟中断和显示字符的幼儿园级别OS。 "},"lab1/lab1_2_1_exercise.html":{"url":"lab1/lab1_2_1_exercise.html","title":"练习","keywords":"","body":"练习 为了实现lab1的目标，lab1提供了4个基本练习，要求完成实验报告。 注意有“LAB1”的注释，代码中所有需要完成的地方（challenge除外）都有“LAB1”和“YOUR CODE”的注释 对实验报告的要求： 基于markdown格式来完成，以文本方式为主。 填写各个基本练习中要求完成的报告内容 完成实验后，请分析ucore_lab中提供的参考答案，并请在实验报告中说明你的实现与参考答案的区别 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点） 列出你认为OS原理中很重要，但在实验中没有对应上的知识点 "},"lab1/lab1_2_1_1_ex1.html":{"url":"lab1/lab1_2_1_1_ex1.html","title":"练习1","keywords":"","body":"练习1：理解通过make生成执行文件的过程。（要求在报告中写出对下述问题的回答） 列出本实验各练习中对应的OS原理的知识点，并说明本实验中的实现部分如何对应和体现了原理中的基本概念和关键知识点。 在此练习中，大家需要通过静态分析代码来了解： 操作系统镜像文件ucore-kernel.elf是如何一步一步生成的？(需要比较详细地解释Makefile中每一条相关命令和命令参数的含义，以及说明命令导致的结果) 补充材料： 如何调试Makefile 当执行make时，一般只会显示输出，不会显示make到底执行了哪些命令。 如想了解make执行了哪些命令，我们在Makefile中设置了V=@参数，@在Makefile中用于隐藏执行的命令，若想要显示，我们可以在每次make执行时添加参数V=来避免隐藏执行的命令： $ make clean $ make \"V=\" 要获取更多有关make的信息，可上网查询，并请执行 $ man make "},"lab1/lab1_2_1_2_ex2.html":{"url":"lab1/lab1_2_1_2_ex2.html","title":"练习2","keywords":"","body":"练习2：使用qemu执行并调试lab1中的软件。（要求在报告中简要写出练习过程） 为了熟悉使用qemu和gdb进行的调试工作，我们进行如下的小练习： 从uCore内核的入口点开始，单步跟踪内核初始化的执行 自己找一个内核中的代码位置，设置断点并进行测试。 补充材料： 我们主要通过硬件模拟器qemu来进行各种实验。在实验的过程中我们可能会遇上各种各样的问题，调试是必要的。qemu支持使用gdb进行的强大而方便的调试。所以用好qemu和gdb是完成各种实验的基本要素。 默认的gdb需要进行一些额外的配置才进行qemu的调试任务。qemu和gdb之间使用网络端口1234进行通讯。在打开qemu进行模拟之后，执行gdb并输入 target remote localhost:1234 即可连接qemu，此时qemu会进入停止状态，听从gdb的命令。 另外，我们可能需要qemu在一开始便进入等待模式，则我们不再使用make qemu开始系统的运行，而使用make debug来完成这项工作。这样qemu便不会在gdb尚未连接的时候擅自运行了。 gdb的地址断点 在gdb命令行中，使用b *[地址]便可以在指定内存地址设置断点，当qemu中的cpu执行到指定地址时，便会将控制权交给gdb。 关于代码的反汇编 有可能gdb无法正确获取当前qemu执行的汇编指令，通过如下配置可以在每次gdb命令行前强制反汇编当前的指令，在gdb命令行或配置文件中添加： define hook-stop x/i $pc end 即可 gdb的单步命令 在gdb中，有next, nexti, step, stepi等指令来单步调试程序，他们功能各不相同，区别在于单步的“跨度”上。 next 单步到程序源代码的下一行，不进入函数。 nexti 单步一条机器指令，不进入函数。 step 单步到下一个不同的源代码行（包括进入函数）。 stepi 单步一条机器指令。 "},"lab1/lab1_2_1_3_ex3.html":{"url":"lab1/lab1_2_1_3_ex3.html","title":"练习3","keywords":"","body":"练习3：分析内核启动后在启用映射地址翻译之前需要在CSR中写入哪些必要配置？。（要求在报告中写出分析） 提示：需要阅读小节“LoongArch32存储管理”和kern/init/entry.S源码，了解内核如何进行DMW的配置，需要了解： DMW有几个，可配置项有哪些？ 对于一个地址映射窗口，什么情况下可以启用一致可缓存？什么情况下必须强序非缓存？（提示：可以了解MMIO与DMA对于内存序以及是否缓存的要求。） 在LoongArch32架构中，页表与DMW同时匹配时，哪个优先级更高？ "},"lab1/lab1_2_1_4_ex4.html":{"url":"lab1/lab1_2_1_4_ex4.html","title":"练习4","keywords":"","body":"练习4：完善例外初始化和处理 （需要编程） 请完成编码工作和回答如下问题： 结合LoongArch32的文档，列出LoongArch32有哪些例外？以及这些例外有哪两个例外向量入口？ 请编程完善kern/driver/clock.c中的时钟中断处理函数clock_int_handler，在对时钟中断进行处理的部分填写trap函数中处理时钟中断的部分，使操作系统每遇到100次时钟中断后，调用kprintf，向屏幕上打印一行文字”100 ticks”。 请编程完善kern/driver/console.c中的串口中断处理函数serial_int_handler，在接收到一个字符后读取该字符，并调用kprintf输出该字符。 要求完成问题2、3提出的相关函数实现，提交改进后的源代码包（可以编译执行），并在实验报告中简要说明实现过程，并写出对问题1的回答。完成这问题2和3要求的部分代码后，运行整个系统，可以看到大约每1秒会输出一次”100 ticks”，而按下的键也会在屏幕上显示。 提示：可阅读小节“中断与异常”。 "},"lab1/lab1_2_2_0_bootlab_compile.html":{"url":"lab1/lab1_2_2_0_bootlab_compile.html","title":"编译方法","keywords":"","body":"编译方法 Makefile修改 在Makefile中取消LAB1 := -DLAB1_EX4 -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT(第6行)的注释 LAB1 := -DLAB1_EX4 -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT # LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3 # LAB3 := -DLAB3_EX1 -DLAB3_EX2 # LAB4 := -DLAB4_EX1 -DLAB4_EX2 # LAB5 := -DLAB5_EX1 -DLAB5_EX2 # LAB6 := -DLAB6_EX2 # LAB7 := -DLAB7_EX1 #-D_SHOW_PHI # LAB8 := -DLAB8_EX1 -DLAB8_EX2 -D_SHOW_100_TICKS选项可在终端每是毫秒打印一行\"100 ticks\" -D_SHOW_SERIAL_INPUT选项会在终端打印键盘的输入 编译并运行代码的命令如下： make make qemu -j 16 补全代码后可以得到如下显示界面（仅供参考） chenyu$ make qemu -j 16 (THU.CST) os is loading ... Special kernel symbols: entry 0xA00000A0 (phys) etext 0xA001F000 (phys) edata 0xA0151820 (phys) end 0xA0154B00 (phys) Kernel executable memory footprint: 1239KB LAB1 Check - Please press your keyboard manually and see what happend. 100 ticks 100 ticks 100 ticks 100 ticks 100 ticks 100 ticks 100 ticks got input 100 ticks got input s got input d got input g 100 ticks got input s got input g 100 ticks "},"lab1/lab1_3_booting.html":{"url":"lab1/lab1_3_booting.html","title":"从机器启动到操作系统运行的过程","keywords":"","body":"从机器启动到操作系统运行的过程 "},"lab1/lab1_3_1_bios_booting.html":{"url":"lab1/lab1_3_1_bios_booting.html","title":"BIOS启动过程","keywords":"","body":"BIOS启动过程 当计算机加电后，一般不直接执行操作系统，而是执行系统初始化软件完成基本IO初始化和引导加载功能。简单地说，系统初始化软件就是在操作系统内核运行之前运行的一段小软件。通过这段小软件，我们可以初始化硬件设备、建立系统的内存空间映射图，从而将系统的软硬件环境带到一个合适的状态，以便为最终调用操作系统内核准备好正确的环境。最终引导加载程序把操作系统内核映像加载到RAM中，并将系统控制权传递给它。 对于绝大多数计算机系统而言，操作系统和应用软件是存放在磁盘（硬盘/软盘）、光盘、EPROM、ROM、Flash等可在掉电后继续保存数据的存储介质上。计算机启动后，CPU一开始会到一个特定的地址开始执行指令，这个特定的地址存放了系统初始化软件，负责完成计算机基本的IO初始化，这是系统加电后运行的第一段软件代码。 对于LoongArch32体系结构而言，真实的硬件上电后会开始启动BIOS，该BIOS可以被自己刷写。而在ChipLab教学计算机中采用的是PMON2000作为BIOS，它具有网络功能，可以通过网卡从网络上使用tftp协议载入ELF格式的操作系统内核加载到内存，然后从ELF的入口点启动。 而我们实验采用的QEMU环境，则是抛弃了BIOS这一过程。在QEMU上直接使用-kernel参数指定内核的ELF文件，本质上就是完成了BIOS所做的加载内核的过程，直接从ELF文件的入口点开始启动操作系统。 "},"lab1/lab1_3_2_booting_os.html":{"url":"lab1/lab1_3_2_booting_os.html","title":"操作系统启动过程","keywords":"","body":"操作系统启动过程 当bootloader通过把ucore在系统加载到内存后，就转跳到ucore操作系统在内存中的入口位置（kern/start.S中的start的地址），这样ucore就接管了整个控制权。当前的ucore功能很简单，只完成基本的内存管理和外设中断管理。ucore主要完成的工作包括： 配置DMW，使得操作系统拥有可用的地址空间（位于kern/init/entry.S，其它部分均为C语言程序，整体位于kern/init/init.c） 初始化终端； 显示字符串； 设置例外两个例外向量； 执行while（1）死循环。 以后的实验中会大量涉及各个函数直接的调用关系，以及由于中断处理导致的异步现象，可能对大家实现操作系统和改正其中的错误有很大影响。而理解好函数调用关系的建立机制和中断处理机制，对后续实验会有很大帮助。 "},"lab1/lab1_3_2_1_address_space.html":{"url":"lab1/lab1_3_2_1_address_space.html","title":"地址空间","keywords":"","body":"地址空间 LoongArch32的地址空间涉及两种地址： 逻辑地址（Logical Address,应用程序员看到的地址，在操作系统原理上称为虚拟地址，以后提到虚拟地址就是指逻辑地址） 物理地址（Physical Address, 实际的物理内存地址）。 (1) 逻辑地址空间 从应用程序的角度看，逻辑地址空间就是应用程序员编程所用到的地址空间，比如下面的程序片段： int val=100; int * point=&val; 其中指针变量point中存储的即是一个逻辑地址。 (2) 物理地址空间 从操作系统的角度看，CPU、内存硬件（通常说的“内存条”）和各种外设是它主要管理的硬件资源而内存硬件和外设分布在物理地址空间中。物理地址空间就是一个“大数组”，CPU通过索引（物理地址）来访问这个“大数组”中的内容。物理地址是指CPU提交到内存总线上用于访问计算机内存和外设的最终地址。 物理地址空间的大小取决于CPU实现的物理地址位数，LoongArch32计算机中，CPU的物理地址空间取决于处理器配置的PALEN。而对于外设，则是固定配置于0x1f000000~0x1fffffff。例如我们如果配置QEMU的内存为256M，那么物理地址空间如下： +------------------+ 图6 LoongArch32计算机系统的物理地址空间 (3) 地址空间的翻译 LoongArch32架构地址空间有两种翻译模式： 直接地址翻译模式（CSR.CRMD中的DA=1且PG=0时） 简单地使物理地址=虚拟地址的前PALEN位，后续补0。 注：PALEN与处理器配置有关。 映射地址翻译模式（CSR.CRMD中的DA=0且PG=1时） 先查看DMW地址映射窗口，若匹配则按照DMW配置的翻译。不匹配则继续。 然后检查TLB是否存在该页表项，若匹配则按照TLB对应配置翻译，不匹配则产生TLB例外由操作系统内核根据页表完成TLB填充操作。 "},"lab1/lab1_3_2_2_interrupt_exception.html":{"url":"lab1/lab1_3_2_2_interrupt_exception.html","title":"中断与异常","keywords":"","body":"中断与异常 操作系统需要对计算机系统中的各种外设进行管理，这就需要CPU和外设能够相互通信才行。一般外设的速度远慢于CPU的速度。如果让操作系统通过CPU“主动关心”外设的事件，即采用通常的轮询(polling)机制，则太浪费CPU资源了。所以需要操作系统和CPU能够一起提供某种机制，让外设在需要操作系统处理外设相关事件的时候，能够“主动通知”操作系统，即打断操作系统和应用的正常执行，让操作系统完成外设的相关处理，然后再恢复操作系统和应用的正常执行。在操作系统中，这种机制称为中断机制。中断机制给操作系统提供了处理意外情况的能力，同时它也是实现进程/线程抢占式调度的一个重要基石。但中断的引入会导致对操作系统的理解更加困难。 在LoongArch32架构中，中断属于异常(Exception)的一种，uCore内核目前处理的异常包括以下类型： 中断 (EX_IRQ,CSR.ESTAT.Ecode=0) Load操作页无效 (EX_TLBL,CSR.ESTAT.Ecode=1) Store操作页无效(EX_TLBS,CSR.ESTAT.Ecode=2) TLB 重填 (EX_TLBR,CSR.ESTAT.Ecode=31) 指令不存在 (EX_RI,CSR.ESTAT.Ecode=13) 指令特权等级错误(EX_IPE,CSR.ESTAT.Ecode=14) 系统调用 (EX_SYS,CSR.ESTAT.Ecode=11) 地址错误例外 (EX_ADE,CSR.ESTAT.Ecode=8) 例如地址没有对齐 LoongArch32架构的处理器也提供了两个例外入口。分别是常规例外与TLB例外。由于TLB例外涉及重填页表的工作，因此必须为物理地址。而常规例外入口则可以根据目前处理器的运行状态选择使用虚拟地址或物理地址。 注意：这里我们所使用的QEMU在直接地址翻译模式下，会抹除CSR.RFBASE地址的高3位，因此我们不需要关心TLB重填时地址访问的地址的问题，可以直接修改CSR.CRMD来开启映射地址翻译模式，然后当做虚拟地址一样处理即可。 这两个例外入口也存储在CSR寄存器中，名称分别为CSR.EBASE与CSR.RFBASE。当例外产生时，处理器会进行如下操作： 将CSR.CRMD的PLV、IE分别存到CSR.PRMD的PPLV和IE中，然后将CSR.CRMD的PLV置为0，IE置为0。 将触发例外指令的PC值记录到CSR.ERA中 跳转到例外入口处取值。（如果是TLB相关例外跳转到CSR.RFBASE，否则为CSR.EBASE） 然后将PC跳转到对应的例外入口地址处，交给软件完成例外的处理操作。 当例外处理结束后，软件应该执行ERTN从例外状态返回，该指令会完成如下操作： 将CSR.PRMD中的PPLV、PIE值回复到CSR.CRMD的PLV、IE中。 跳转到CSR.ERA所记录的地址处取指。 "},"lab1/lab1_3_2_3_lab1_interrupt.html":{"url":"lab1/lab1_3_2_3_lab1_interrupt.html","title":"lab1中对中断的处理实现","keywords":"","body":"lab1中对例外的处理实现 (1) 外设基本初始化设置 Lab1实现了中断初始化和对键盘、串口、时钟外设进行中断处理。串口的初始化函数serial_init（位于/kern/driver/console.c）中涉及中断初始化工作的很简单： ...... // 使能串口1接收字符后产生中断 outb(COM1 + COM_IER, COM_IER_RDI); ...... // 通过中断控制器使能串口1中断 pic_enable(IRQ_COM1); 时钟是一种有着特殊作用的外设，其作用并不仅仅是计时。在后续章节中将讲到，正是由于有了规律的时钟中断，才使得无论当前CPU运行在哪里，操作系统都可以在预先确定的时间点上获得CPU控制权。这样当一个应用程序运行了一定时间后，操作系统会通过时钟中断获得CPU控制权，并可把CPU资源让给更需要CPU的其他应用程序。时钟的初始化函数clock_init（位于kern/driver/clock.c中）完成了对时钟控制器的初始化： ...... unsigned long timer_config; unsigned long period = 200000000; period = period / HZ; timer_config = period & LISA_CSR_TMCFG_TIMEVAL; timer_config |= (LISA_CSR_TMCFG_PERIOD | LISA_CSR_TMCFG_EN); __lcsr_csrwr(timer_config, LISA_CSR_TMCFG); pic_enable(TIMER0_IRQ); (2) 例外初始化设置 对于LoongArch32架构的计算机来说，操作系统初始化中断首先需要完成例外处理程序的基地址设置。这一部分程序写在了kern/init/init.c中的setup_exception_vector函数。 其中，该函数中使用的__exception_vector地址位于文件kern/trap/vectors.S。为了简化，它直接跳转到了kern/trap/exception.S中的ramExcHandle_general处。 (3) 例外的处理过程 trap函数（定义在trap.c中）是对例外进行处理的过程，所有的例外在经过中断入口函数ramExcHandle_general预处理后 (定义在 exception.S中) ，都会跳转到这里。在处理过程中，根据不同的例外类型，进行相应的处理。在相应的处理过程结束以后，trap将会返回，被中断的程序会继续运行。整个中断处理流程大致如下： 1)产生例外后，CPU硬件完成了如下操作： 将CSR.CRMD的PLV、IE分别存到CSR.PRMD的PPLV和IE中，然后将CSR.CRMD的PLV置为0，IE置为0。 将触发例外指令的PC值记录到CSR.ERA中 跳转到例外入口处取值。（如果是TLB相关例外跳转到CSR.RFBASE，否则为CSR.EBASE） 2)经过例外向量的跳转，进入exception.S中的ramExcHandle_general。 在这里会完成例外现场的保存操作，切换到内核栈，将当前处理器的所有通用寄存器压入内核栈中，并使用CSR中的KS0和KS1寄存器用来辅助保存数据（否则保存过程中必然导致一些特定寄存器的修改）。 保存的数据按照trapfame结构进行，位于kern/trap/loongarch_trapframe.h。 3) 然后跳转进入kern/trap/trap.c中的loongarch_trap函数，开始了C语言程序的内核的处理。 这个函数中，会根据例外的类型完成例外的分类并进行处理，具体见kern/trap/trap.c。 4) 当loongarch_trap这一函数处理完毕后（处理过程可能包含对trapframe的修改），会返回到之前的汇编程序（exception.S中），完成寄存器状态的还原，然后使用ertn指令结束例外处理，恢复程序的执行。 至此，对整个lab1中的主要部分的背景知识和实现进行了阐述。请大家能够根据前面的练习要求完成所有的练习。 "},"lab2.html":{"url":"lab2.html","title":"Lab 2 物理内存管理","keywords":"","body":"物理内存管理 实验一过后大家做出来了一个可以启动的系统，实验二主要涉及操作系统的物理内存管理。操作系统为了使用内存，还需高效地管理内存资源。在实验二中大家会了解并且自己动手完成一个简单的物理内存管理系统。 "},"lab2/lab2_3_1_phymemlab_goal.html":{"url":"lab2/lab2_3_1_phymemlab_goal.html","title":"实验目的","keywords":"","body":"实验目的 理解基于页表的转换机制 理解页表的建立和使用方法 理解物理内存的管理方法 "},"lab2/lab2_3_2_phymemlab_contents.html":{"url":"lab2/lab2_3_2_phymemlab_contents.html","title":"实验内容","keywords":"","body":"实验内容 本次实验包含三个部分。首先了解如何发现系统中的物理内存；然后了解如何建立对物理内存的初步管理，即了解连续物理内存管理；最后了解页表相关的操作，即如何建立页表来实现虚拟内存到物理内存之间的映射。本实验里面实现的内存管理还是非常基本的，并没有涉及到对实际机器的优化，比如针对 cache 的优化等。如果大家有余力，尝试完成扩展练习。 "},"lab2/lab2_3_2_1_phymemlab_exercise.html":{"url":"lab2/lab2_3_2_1_phymemlab_exercise.html","title":"练习","keywords":"","body":"为了实现lab2的目标，lab2提供了3个基本练习和2个扩展练习，要求完成实验报告。 注意有“LAB2”的注释，代码中所有需要完成的地方（challenge除外）都有“LAB2”和“YOUR CODE”的注释 对实验报告的要求： 基于markdown格式来完成，以文本方式为主 填写各个基本练习中要求完成的报告内容 完成实验后，请分析ucore_lab中提供的参考答案，并请在实验报告中说明你的实现与参考答案的区别 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点） 列出你认为OS原理中很重要，但在实验中没有对应上的知识点 练习1：实现 first-fit 连续物理内存分配算法（需要编程） 在实现first fit内存分配算法的回收函数时，要考虑地址连续的空闲块之间的合并操作。提示:在建立空闲页块链表时，需要按照空闲页块起始地址来排序，形成一个有序的链表。可能会修改default_pmm.c中的default_init，default_init_memmap，default_alloc_pages，default_free_pages等相关函数。请仔细查看和理解default_pmm.c中的注释。 注意，目前实验提供的代码已经可以运行，但希望读者在理解的基础上能自己重新实现有关函数。 请在实验报告中简要说明你的设计实现过程。请回答如下问题： 你的first fit算法是否有进一步的改进空间 练习2：实现寻找虚拟地址对应的页表项（需要编程） 通过设置页表和对应的页表项，可建立虚拟内存地址和物理内存地址的对应关系。其中的get_pte函数是设置页表项环节中的一个重要步骤。此函数找到一个虚地址对应的二级页表项的内核虚地址，如果此二级页表项不存在，则分配一个包含此项的二级页表。本练习需要补全get_pte函数（kern/mm/pmm.c中），实现其功能。请仔细查看和理解get_pte函数中的注释。get_pte函数的调用关系图如下所示： 图1 get_pte函数的调用关系图 请在实验报告中简要说明你的设计实现过程。请回答如下问题： 请描述页目录项（Page Directory Entry）和页表项（Page Table Entry）中每个组成部分的含义以及对ucore而言的潜在用处。 如果ucore执行过程中访问内存，出现了页访问异常，请问硬件要做哪些事情？ 练习3：释放某虚地址所在的页并取消对应二级页表项的映射（需要编程） 当释放一个包含某虚地址的物理内存页时，需要让对应此物理内存页的管理数据结构Page做相关的清除处理，使得此物理内存页成为空闲；另外还需把表示虚地址与物理地址对应关系的二级页表项清除。请仔细查看和理解page_remove_pte函数中的注释。为此，需要补全在kern/mm/pmm.c中的page_remove_pte函数。page_remove_pte函数的调用关系图如下所示： 图2 page_remove_pte函数的调用关系图 请在实验报告中简要说明你的设计实现过程。请回答如下问题： 数据结构Page的全局变量（其实是一个数组）的每一项与页表中的页目录项和页表项有无对应关系？如果有，其对应关系是啥？ 如果希望虚拟地址与物理地址相等，则需要如何修改lab2，完成此事？ 鼓励通过编程来具体完成这个问题 扩展练习Challenge：buddy system（伙伴系统）分配算法（需要编程） Buddy System算法把系统中的可用存储空间划分为存储块(Block)来进行管理, 每个存储块的大小必须是2的n次幂(Pow(2, n)), 即1, 2, 4, 8, 16, 32, 64, 128... 参考伙伴分配器的一个极简实现， 在ucore中实现buddy system分配算法，要求有比较充分的测试用例说明实现的正确性，需要有设计文档。 扩展练习Challenge：任意大小的内存单元slub分配算法（需要编程） slub算法，实现两层架构的高效内存单元分配，第一层是基于页大小的内存分配，第二层是在第一层基础上实现基于任意大小的内存分配。可简化实现，能够体现其主体思想即可。 参考linux的slub分配算法/，在ucore中实现slub分配算法。要求有比较充分的测试用例说明实现的正确性，需要有设计文档。 Challenges是选做，做一个就很好了。完成Challenge的同学可单独提交Challenge。完成得好的同学可获得最终考试成绩的加分。 "},"lab2/lab2_3_2_2_phymemlab_compile.html":{"url":"lab2/lab2_3_2_2_phymemlab_compile.html","title":"编译方法","keywords":"","body":"编译方法 Makefile修改 在Makefile中取消LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3(第7行)的注释 LAB1 := -DLAB1_EX4 # -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3 # LAB3 := -DLAB3_EX1 -DLAB3_EX2 # LAB4 := -DLAB4_EX1 -DLAB4_EX2 # LAB5 := -DLAB5_EX1 -DLAB5_EX2 # LAB6 := -DLAB6_EX2 # LAB7 := -DLAB7_EX1 #-D_SHOW_PHI # LAB8 := -DLAB8_EX1 -DLAB8_EX2 编译并运行代码的命令如下： make make qemu -j 16 补全代码后可以得到如下显示界面（仅供参考） chenyu$ make qemu -j 16 (THU.CST) os is loading ... Special kernel symbols: entry 0xA00000A0 (phys) etext 0xA0020000 (phys) edata 0xA0153370 (phys) end 0xA0156650 (phys) Kernel executable memory footprint: 1242KB memory management: default_pmm_manager memory map: [A0000000, A2000000] freemem start at: A0197000 free pages: 00001E69 ## 00000020 check_alloc_page() succeeded! check_pgdir() succeeded! check_boot_pgdir() succeeded! check_slab() succeeded! kmalloc_init() succeeded! LAB2 Check Pass! 通过上图，我们可以看到ucore在显示其entry（入口地址）、etext（代码段截止处地址）、edata（数据段截止处地址）、和end（ucore截止处地址）的值后，探测出计算机系统中的物理内存的布局。然后会显示内存范围和空闲内存的起始地址，显示可以分为多少个page。接下来会执行各种我们设置的检查，最后响应时钟中断。 "},"lab2/lab2_3_3_phymem_manage.html":{"url":"lab2/lab2_3_3_phymem_manage.html","title":"物理内存管理","keywords":"","body":"物理内存管理 接下来将首先对实验的执行流程做个介绍，并进一步介绍如何探测物理内存的大小与布局，如何以页为单位来管理计算机系统中的物理内存，如何设计物理内存页的分配算法，最后比较详细地分析了页式内存管理的过程。 "},"lab2/lab2_3_3_1_phymemlab_overview.html":{"url":"lab2/lab2_3_3_1_phymemlab_overview.html","title":"实验执行流程概述","keywords":"","body":" 实验执行流程概述 本次实验主要完成ucore内核对物理内存的管理工作。 kern_init函数在完成一些输出并对lab1实验结果的检查后，将进入物理内存管理初始化的工作，即调用pmm_init函数完成物理内存的管理，这也是我们lab2的内容。接着执行intr_enable函数开启中断，这些工作与lab1的中断异常工作的内容是相同的。 为了完成物理内存管理，这里首先需要探测可用的物理内存资源；了解到物理内存位于什么地方，有多大。 这里细心的读者一定会有疑问，在翻阅uCore(LoongArch32版本)代码后发现内存大小似乎是写死的32M，并没有像x86等架构一样通过e820去得到一个可用内存的大小，从而操作系统似乎没有完成物理内存探测的操作。确实是这样，在许多RISC架构下，通常启动系统内核会读取一个叫做设备树(Device Tree)的文件，来描述硬件上各个设备的地址和特性，设备树中也包括了内存大小，而并不是交给系统内核自己去探测。我们也可以见到许多ARM和RISC-V架构的Linux开发板，不同内存容量的版本要使用不同的镜像，也是因为设备树不同导致的。而为了简化系统的设计，我们并没有实现设备树的读取，因此采用了写死的32M。 在确定了物理内存大小后，就以固定页面大小来划分整个物理内存空间，并准备以此为最小内存分配单位来管理整个物理内存，管理在内核运行过程中每页内存，设定其可用状态（free的，used的，还是reserved的），这其实就对应了我们在课本上讲到的连续内存分配概念和原理的具体实现；接着ucore kernel就要建立页表，启动分页机制，当缺页异常发生时，就会跳转到内核的异常处理地址上，由内核完成TLB填充，根据页表项描述的虚拟页（Page）与物理页帧（Page Frame）的对应关系完成CPU对内存的读、写和执行操作。这一部分其实就对应了我们在课本上讲到内存映射、页表、多级页表等概念和原理的具体实现。 在代码分析上，建议根据执行流程来直接看源代码，并可采用GDB源码调试的手段来动态地分析ucore的执行过程。内存管理相关的总体控制函数是pmm_init函数，它完成的主要工作包括： 初始化物理内存页管理器框架pmm_manager； 建立空闲的page链表，这样就可以分配以页（4KB）为单位的空闲内存了； 检查物理内存页分配算法； 建立一一映射关系的二级页表； 使能分页机制； 检查页表建立是否正确； 另外，主要注意的相关代码内容包括： 管理每个物理页的Page数据结构（在mm/memlayout.h中），这个数据结构也是实现连续物理内存分配算法的关键数据结构，可通过此数据结构来完成空闲块的链接和信息存储，而基于这个数据结构的管理物理页数组起始地址就是全局变量pages，具体初始化此数组的函数位于page_init函数中； 用于实现连续物理内存分配算法的物理内存页管理器框架pmm_manager，这个数据结构定义了实现内存分配算法的关键函数指针，而同学们需要完成这些函数的具体实现； 设定二级页表和建立页表项以完成虚实地址映射关系，这与硬件相关，且用到不少内联函数，源代码相对难懂一些。具体完成页表和页表项建立的重要函数是boot_map_segment函数，而get_pte函数是完成虚实映射关键的关键。 "},"lab2/lab2_3_3_3_phymem_pagelevel.html":{"url":"lab2/lab2_3_3_3_phymem_pagelevel.html","title":"以页为单位管理物理内存","keywords":"","body":" 以页为单位管理物理内存 在获得可用物理内存范围后，系统需要建立相应的数据结构来管理以物理页（按4KB对齐，且大小为4KB的物理内存单元）为最小单位的整个物理内存，以配合后续涉及的分页管理机制。每个物理页可以用一个Page数据结构来表示。由于一个物理页需要占用一个Page结构的空间，Page结构在设计时须尽可能小，以减少对内存的占用。Page的定义在kern/mm/memlayout.h中。以页为单位的物理内存分配管理的实现在kern/default_pmm.c中。 为了与以后的分页机制配合，我们首先需要建立对整个计算机的每一个物理页的属性用结构Page来表示，它包含了映射此物理页的虚拟页个数，描述物理页属性的flags和双向链接各个Page结构的page_link双向链表。 struct Page { int ref; // page frame's reference counter uint32_t flags; // array of flags that describe the status of the page frame unsigned int property; // used in buddy system, stores the order (the X in 2^X) of the continuous memory block int zone_num; // used in buddy system, the No. of zone which the page belongs to list_entry_t page_link;// free list link list_entry_t swap_link; // swap hash link }; 这里看看Page数据结构的各个成员变量有何具体含义。ref表示这页被页表的引用记数（在“实现分页机制”一节会讲到）。如果这个页被页表引用了，即在某页表中有一个页表项设置了一个虚拟页到这个Page管理的物理页的映射关系，就会把Page的ref加一；反之，若页表项取消，即映射关系解除，就会把Page的ref减一。flags表示此物理页的状态标记，进一步查看kern/mm/memlayout.h中的定义，可以看到： /* Flags describing the status of a page frame */ #define PG_reserved 0 // the page descriptor is reserved for kernel or unusable #define PG_property 1 // the member 'property' is valid #define PG_slab 2 // page frame is included in a slab #define PG_dirty 3 // the page has been modified #define PG_swap 4 // the page is in the active or inactive page list (and swap hash table) #define PG_active 5 // the page is in the active page list 这表示flags目前用到了六个bit表示页目前具有的六种属性，bit 0表示此页是否被保留（reserved），如果是被保留的页，则bit 0会设置为1，且不能放到空闲页链表中，即这样的页不是空闲页，不能动态分配与释放。比如目前内核代码占用的空间就属于这样“被保留”的页。在本实验中，bit 1表示此页是否是free的，如果设置为1，表示这页是free的，可以被分配；如果设置为0，表示这页已经被分配出去了，不能被再二次分配。 另外，本实验这里取的名字PG_property比较不直观，主要是因为我们可以设计不同的页分配算法（best fit, buddy system等），那么这个PG_property就有不同的含义了。 在本实验中，Page数据结构的成员变量property用来记录某连续内存空闲块的大小（即地址连续的空闲页的个数）。这里需要注意的是用到此成员变量的这个Page比较特殊，是这个连续内存空闲块地址最小的一页（即头一页，Head Page）。连续内存空闲块利用这个页的成员变量property来记录在此块内的空闲页的个数。这里取的名字property也不是很直观，原因与上面类似，在不同的页分配算法中，property有不同的含义。 Page数据结构的成员变量page_link是便于把多个连续内存空闲块链接在一起的双向链表指针（可回顾在lab0实验指导书中有关双向链表数据结构的介绍）。这里需要注意的是用到此成员变量的这个Page比较特殊，是这个连续内存空闲块地址最小的一页（即头一页，Head Page）。连续内存空闲块利用这个页的成员变量page_link来链接比它地址小和大的其他连续内存空闲块。 在初始情况下，也许这个物理内存的空闲物理页都是连续的，这样就形成了一个大的连续内存空闲块。但随着物理页的分配与释放，这个大的连续内存空闲块会分裂为一系列地址不连续的多个小连续内存空闲块，且每个连续内存空闲块内部的物理页是连续的。那么为了有效地管理这些小连续内存空闲块。所有的连续内存空闲块可用一个双向链表管理起来，便于分配和释放，为此定义了一个free_area_t数据结构，包含了一个list_entry结构的双向链表指针和记录当前空闲页的个数的无符号整型变量nr_free。其中的链表指针指向了空闲的物理页。 /* free_area_t - maintains a doubly linked list to record free (unused) pages */ typedef struct { list_entry_t free_list; // the list header unsigned int nr_free; // # of free pages in this free list } free_area_t; 有了这两个数据结构，ucore就可以管理起来整个以页为单位的物理内存空间。接下来需要解决两个问题： 管理页级物理内存空间所需的Page结构的内存空间从哪里开始，占多大空间？ 空闲内存空间的起始地址在哪里？ 对于这两个问题，我们首先根据memlayout.h给出的核心态内存基地址KERNBASE（0xa0000000）和KMEMSIZE（512M）计算出最大的物理内存地址KERNTOP（定义在memlayout.h中），最大物理内存地址maxpa等于KERNTOP（定义在page_init函数中的局部变量），在该实验中Page size为4096 bytes即2^12 bytes，所以需要管理的物理页的个数npage的计算方式如下： #define KERNTOP (KERNBASE + KMEMSIZE) maxpa = KERNTOP npage = KMEMSIZE >> PGSHIFT # PGSHIFT = 12 这样，我们就可以预估出管理页级物理内存空间所需的Page结构的内存空间所需的内存大小为： sizeof(struct Page) * npage 由于加载内核的结束地址（用全局指针变量end记录）以上的空间没有被使用，所以我们可以把end按页大小为边界取整后，作为管理页级物理内存空间所需的Page结构的内存空间，记为： pages = (struct Page *)ROUNDUP_2N((void *)end, PGSHIFT); 为了简化起见，从地址0到地址pages+ sizeof(struct Page) *npage)结束的物理内存空间设定为已占用物理内存空间（起始0~640KB的空间是空闲的），地址pages+sizeof(struct Page) *npage)以上的空间为空闲物理内存空间，这时的空闲空间起始地址为 uintptr_t freemem = PADDR((uintptr_t)pages + sizeof(struct Page) * npage); 为此我们需要把这两部分空间给标识出来。首先，对于所有物理空间，通过如下语句即可实现占用标记： for (i = 0; i 然后，根据探测到的空闲物理空间，通过如下语句即可实现空闲标记： //获得空闲空间的起始地址begin和结束地址end ... init_memmap(pa2page(mbegin), (mend - mbegin) >> PGSHIFT ); 其实SetPageReserved只需把物理地址对应的Page结构中的flags标志设置为PG_reserved，表示这些页已经被使用了，将来不能被用于分配。而init_memmap函数则是把空闲物理页对应的Page结构中的flags和引用计数ref清零，并加到free_area.free_list指向的双向列表中，为将来的空闲页管理做好初始化准备工作。 关于内存分配的操作系统原理方面的知识有很多，但在本实验中只实现了最简单的内存页分配算法。相应的实现在default_pmm.c中的default_alloc_pages函数和default_free_pages函数，相关实现很简单，这里就不具体分析了，直接看源码，应该很好理解。 其实实验二在内存分配和释放方面最主要的作用是建立了一个物理内存页管理器框架，这实际上是一个函数指针列表，定义如下： struct pmm_manager { const char *name; // XXX_pmm_manager's name void (*init)(void); // initialize internal description&management data structure // (free block list, number of free block) of XXX_pmm_manager void (*init_memmap)(struct Page *base, size_t n); // setup description&management data structcure according to // the initial free physical memory space struct Page *(*alloc_pages)(size_t n); // allocate >=n pages, depend on the allocation algorithm void (*free_pages)(struct Page *base, size_t n); // free >=n pages with \"base\" addr of Page descriptor structures(memlayout.h) size_t (*nr_free_pages)(void); // return the number of free pages void (*check)(void); // check the correctness of XXX_pmm_manager }; 重点是实现init_memmap/ alloc_pages/free_pages这三个函数。 "},"lab2/lab2_3_3_4_phymem_allocation.html":{"url":"lab2/lab2_3_3_4_phymem_allocation.html","title":"物理内存页分配算法实现","keywords":"","body":"物理内存页分配算法实现 如果要在ucore中实现连续物理内存分配算法，则需要考虑的事情比较多，相对课本上的物理内存分配算法描述要复杂不少。下面介绍一下如果要实现一个FirstFit内存分配算法的大致流程。 lab2的第一部分是完成first_fit的分配算法。原理FirstFit内存分配算法上很简单，但要在ucore中实现，需要充分了解和利用ucore已有的数据结构和相关操作、关键的一些全局变量等。 关键数据结构和变量 first_fit分配算法需要维护一个查找有序（地址按从小到大排列）空闲块（以页为最小单位的连续地址空间）的数据结构，而双向链表是一个很好的选择。 kernel/include/list.h定义了可挂接任意元素的通用双向链表结构和对应的操作，所以需要了解如何使用这个文件提供的各种函数，从而可以完成对双向链表的初始化/插入/删除等。 kern/mm/memlayout.h中定义了一个 free_area_t 数据结构，包含成员结构 list_entry_t free_list; // the list header 空闲块双向链表的头 unsigned int nr_free; // # of free pages in this free list 空闲块的总数（以页为单位） 显然，我们可以通过此数据结构来完成对空闲块的管理。而buddy_pmm.c中定义的free_area变量就是干这个事情的。 kern/mm/pmm.h中定义了一个通用的分配算法的函数列表，用pmm_manager表示。其中init函数就是用来初始化free_area变量的,first_fit分配算法可直接重用buddy_init函数的实现。init_memmap函数需要根据现有的内存情况构建空闲块列表的初始状态。何时应该执行这个函数呢？ 通过分析代码，可以知道： kern_init --> pmm_init-->page_init-->init_memmap--> pmm_manager->init_memmap 所以，default_init_memmap需要根据page_init函数中传递过来的参数（某个连续地址的空闲块的起始页，页个数）来建立一个连续内存空闲块的双向链表。这里有一个假定page_init函数是按地址从小到大的顺序传来的连续内存空闲块的。链表头是free_area.free_list，链表项是Page数据结构的base->page_link。这样我们就依靠Page数据结构中的成员变量page_link形成了连续内存空闲块列表。 设计实现 default_init_memmap函数将根据每个物理页帧的情况来建立空闲页链表，且空闲页块应该是根据地址高低形成一个有序链表。根据上述变量的定义，default_init_memmap可大致实现如下： default_init_memmap(struct Page *base, size_t n) { assert(n > 0); struct Page *p = base; for (; p != base + n; p ++) { assert(PageReserved(p)); p->flags = p->property = 0; set_page_ref(p, 0); } base->property = n; SetPageProperty(base); nr_free += n; list_add_before(&free_list, &(base->page_link)); } 如果要分配一个页，那要考虑哪些呢？这里就需要考虑实现default_alloc_pages函数，注意参数n表示要分配n个页。另外，需要注意实现时尽量多考虑一些边界情况，这样确保软件的鲁棒性。比如 if (n > nr_free) { return NULL; } 这样可以确保分配不会超出范围。也可加一些assert函数，在有错误出现时，能够迅速发现。比如 n应该大于0，我们就可以加上 assert(n > 0); 这样在nproperty可以了解此空闲块的大小。如果>=n，这就找到了！如果 default_alloc_pages(size_t n) { assert(n > 0); if (n > nr_free) { return NULL; } struct Page *page = NULL; list_entry_t *le = &free_list; // TODO: optimize (next-fit) while ((le = list_next(le)) != &free_list) { struct Page *p = le2page(le, page_link); if (p->property >= n) { page = p; break; } } if (page != NULL) { if (page->property > n) { struct Page *p = page + n; p->property = page->property - n; SetPageProperty(p); list_add_after(&(page->page_link), &(p->page_link)); } list_del(&(page->page_link)); nr_free -= n; ClearPageProperty(page); } return page; } default_free_pages函数的实现其实是default_alloc_pages的逆过程，不过需要考虑空闲块的合并问题。这里就不再细讲了。注意，上诉代码只是参考设计，不是完整的正确设计。更详细的说明位于lab2/kernel/mm/default_pmm.c的注释中。希望同学能够顺利完成本实验的第一部分。 "},"lab2/lab2_3_3_5_paging.html":{"url":"lab2/lab2_3_3_5_paging.html","title":"实现分页机制","keywords":"","body":"实现分页机制 在本实验中，需要重点了解和实现基于页表的页机制和以页为单位的物理内存管理方法和分配算法等。下面比较详细地介绍了实现分页机制的过程。 "},"lab2/lab2_3_3_5_2_key_problems_in_seg_page.html":{"url":"lab2/lab2_3_3_5_2_key_problems_in_seg_page.html","title":"建立段页式管理中需要考虑的关键问题","keywords":"","body":"建立段页式管理中需要考虑的关键问题 为了实现分页机制，需要建立好虚拟内存和物理内存的页映射关系。此过程涉及硬件细节，不同的地址映射关系组合，相对比较复杂。总体而言，我们需要思考如下问题： 对于哪些物理内存空间需要建立页映射关系？ 具体的页映射关系是什么？ 页目录表的起始地址设置在哪里？ 页表的起始地址设置在哪里，需要多大空间？ 如何设置页目录表项的内容？ 如何设置页表项的内容？ "},"lab2/lab2_3_3_5_4_maping_relations.html":{"url":"lab2/lab2_3_3_5_4_maping_relations.html","title":"系统执行中地址映射的四个阶段","keywords":"","body":"系统执行中地址映射的过程 在lab1和lab2中都会涉及如何建立映射关系的操作，在龙芯架构中MMU支持两种地址翻译模式：直接地址翻译模式和映射地址翻译模式。直接地址翻译模式下物理地址默认直接等于虚拟地址的[PALEN-1:0]位（不足补0）。当处理器核的MMU处于映射地址翻译模式，具体又分为直接映射地址翻译模式和页表映射地址翻译模式。直接映射地址模式是通过直接映射配置窗口机制完成虚实地址的直接映射，映射地址翻译模式通过页表完成映射。 在lab1和lab2中我们在ucore的入口函数kernel_entry（entry.S文件中）中设置了CSR.CRMD的DA=0且PG=1，即处理器核的MMU处于映射地址翻译模式。同时，我们将CSR.DWM0寄存器的值设置为0xa0000001，表示 0xa0000000-0xbfffffff段的虚拟地址通过直接映射地址翻译模式映射到0x00000000-0x1fffffff的物理地址。将CSR.DWM1寄存器的值设置为0x80000011，表示0x80000000-0x9fffffff段的虚拟地址用过直接映射地址翻译模式映射到0x00000000-0x1fffffff的物理地址。除了这两段虚拟地址之外的虚拟地址都是通过页表映射地址翻译模式进行映射。 下面，我们来看看如何改变内核的起始地址。观察一下链接脚本，即tools/kernel.ld文件： OUTPUT_ARCH(loongarch) ENTRY(kernel_entry) SECTIONS { . = 0xa0000000; .text : { . = ALIGN(4); wrs_kernel_text_start = .; _wrs_kernel_text_start = .; *(.startup) *(.text) *(.text.*) *(.gnu.linkonce.t*) *(.mips16.fn.*) *(.mips16.call.*) /* for MIPS */ *(.rodata) *(.rodata.*) *(.gnu.linkonce.r*) *(.rodata1) . = ALIGN(4096); *(.ramexv) } 从上述代码可以看出ld工具形成的ucore的起始虚拟地址从0xa0000000开始，由于这个段的地址是直接映射地址段，所以其起始的物理地址为0x00000000，即 phy addr = CSR.DMW0[31:29] : virtual addr[28:0] 第一个阶段从kernel_entry函数开始到pmm_init函数执行之前，ucore采用直接映射地址方式进行地址翻译，翻译方式如上。注意，由于0x80000000-0x9fffffff和0xa0000000-0xbfffffff才能进行直接映射，所以内核的大小不能超过512M。 第二个阶段（创建初始页目录表，开启分页模式）从pmm_init函数被调用开始，在pmm_init函数中创建了boot_pgdir，初始化了页目录表，正式开始了页表映射地址翻译模式。 "},"lab2/lab2_3_3_5_3_setup_paging_map.html":{"url":"lab2/lab2_3_3_5_3_setup_paging_map.html","title":"建立虚拟页和物理页帧的地址映射关系","keywords":"","body":"建立虚拟页和物理页帧的地址映射关系 建立二级页表 在LoongArch32采用了与MIPS相同的软件定义页表的方式，因此内核可以自由采取页表的定义方式。由于该版本uCore自x86架构移植而来，因此沿用了x86的类似方式采用二级页表来建立逻辑地址与物理地址之间的映射关系。由于我们已经具有了一个物理内存页管理器default_pmm_manager，支持动态分配和释放内存页的功能，我们就可以用它来获得所需的空闲物理页。在二级页表结构中，页目录表占4KB空间，可通过alloc_page函数获得一个空闲物理页作为页目录表（Page Directory Table，PDT）。同理，ucore也通过这种类似方式获得一个页表（Page Table，PT）所需的4KB空间。 整个页目录表和页表所占空间大小取决与二级页表要管理和映射的物理页数。假定当前物理内存0~16MB，每物理页（也称Page Frame）大小为4KB，则有4096个物理页，也就意味这有4个页目录项和4096个页表项需要设置。一个页目录项（Page Directory Entry，PDE）和一个页表项（Page Table Entry，PTE）占4B。即使是4个页目录项也需要一个完整的页目录表（占4KB）。而4096个页表项需要16KB（即4096*4B）的空间，也就是4个物理页，16KB的空间。所以对16MB物理页建立一一映射的16MB虚拟页，需要5个物理页，即20KB的空间来形成二级页表。 完成前一节所述的前两个阶段的地址映射变化后，为把0~KERNSIZE（明确ucore设定实际物理内存不能超过KERNSIZE值，即512MB，131072个物理页）的物理地址一一映射到页目录项和页表项的内容，其大致流程如下： 指向页目录表的指针已存储在boot_pgdir变量中。 调用boot_map_segment函数进一步建立一一映射关系，具体处理过程以页为单位进行设置，即 linear addr = phy addr + 0xa0000000 设一个32bit线性地址la有一个对应的32bit物理地址pa，如果在以la的高10位为索引值的页目录项中的存在位（PTE_P）为0，表示缺少对应的页表空间，则可通过alloc_page获得一个空闲物理页给页表，页表起始物理地址是按4096字节对齐的，这样填写页目录项的内容为 页目录项内容 = (页表起始物理地址 & ~0x0FFF) | PTE_U | PTE_W | PTE_P 进一步对于页表中以线性地址la的中10位为索引值对应页表项的内容为 页表项内容 = (pa & ~0x0FFF) | PTE_P | PTE_W 其中： PTE_U：位3，表示用户态的软件可以读取对应地址的物理内存页内容 PTE_W：位2，表示物理内存页内容可写 PTE_P：位1，表示物理内存页存在 ucore的内存管理经常需要查找页表：给定一个虚拟地址，找出这个虚拟地址在二级页表中对应的项。通过更改此项的值可以方便地将虚拟地址映射到另外的页上。可完成此功能的这个函数是get_pte函数。它的原型为 pte_t *get_pte(pde_t *pgdir, uintptr_t la, bool create) 下面的调用关系图可以比较好地看出get_pte在实现上述流程中的位置： 图6 get_pte调用关系图 这里涉及到三个类型pte_t、pde_t和uintptr_t。通过参见mm/mmlayout.h和libs/types.h，可知它们其实都是unsigned int类型。在此做区分，是为了分清概念。 pde_t全称为 page directory entry，也就是一级页表的表项（注意：pgdir实际不是表项，而是一级页表本身。实际上应该新定义一个类型pgd_t来表示一级页表本身）。pte_t全称为 page table entry，表示二级页表的表项。uintptr_t表示为线性地址，由于段式管理只做直接映射，所以它也是逻辑地址。 pgdir给出页表起始地址。通过查找这个页表，我们需要给出二级页表中对应项的地址。虽然目前我们只有boot_pgdir一个页表，但是引入进程的概念之后每个进程都会有自己的页表。 有可能根本就没有对应的二级页表的情况，所以二级页表不必要一开始就分配，而是等到需要的时候再添加对应的二级页表。如果在查找二级页表项时，发现对应的二级页表不存在，则需要根据create参数的值来处理是否创建新的二级页表。如果create参数为0，则get_pte返回NULL；如果create参数不为0，则get_pte需要申请一个新的物理页（通过alloc_page来实现，可在mm/pmm.h中找到它的定义），再在一级页表中添加页目录项指向表示二级页表的新物理页。注意，新申请的页必须全部设定为零，因为这个页所代表的虚拟地址都没有被映射。 当建立从一级页表到二级页表的映射时，需要注意设置控制位。这里应该设置同时设置上PTE_U、PTE_W和PTE_P（定义在mm/mmu.h）。如果原来就有二级页表，或者新建立了页表，则只需返回对应项的地址即可。 虚拟地址只有映射上了物理页才可以正常的读写。在完成映射物理页的过程中，除了要像上面那样在页表的对应表项上填上相应的物理地址外，还要设置正确的控制位。 只有当一级二级页表的项都设置了用户写权限后，用户才能对对应的物理地址进行读写。所以我们可以在一级页表先给用户写权限，再在二级页表上面根据需要限制用户的权限，对物理页进行保护。由于一个物理页可能被映射到不同的虚拟地址上去（譬如一块内存在不同进程间共享），当这个页需要在一个地址上解除映射时，操作系统不能直接把这个页回收，而是要先看看它还有没有映射到别的虚拟地址上。这是通过查找管理该物理页的Page数据结构的成员变量ref（用来表示虚拟页到物理页的映射关系的个数）来实现的，如果ref为0了，表示没有虚拟页到物理页的映射关系了，就可以把这个物理页给回收了，从而这个物理页是free的了，可以再被分配。page_insert函数将物理页映射在了页表上。可参看page_insert函数的实现来了解ucore内核是如何维护这个变量的。当不需要再访问这块虚拟地址时，可以把这块物理页回收并在将来用在其他地方。取消映射由page_remove来做，这其实是page_insert的逆操作。 建立好一一映射的二级页表结构后，由于分页机制在前一节所述的前两个阶段已经开启，分页机制到此初始化完毕。当执行完毕gdt_init函数后，新的段页式映射已经建立好了。 在pmm_init函数建立完实现物理内存一一映射和页目录表自映射的页目录表和页表后，ucore看到的内核虚拟地址空间如下图所示： Virtual memory map: Permissions kernel/user 4G ------------------> +---------------------------------+ | | | Mapped(2G) | | | KERNTOP ------------> +---------------------------------+ 0xBFFF_FFFF | | | Unmapped cached(DMW0 512M) | | | KERNBASE-------------> +---------------------------------+ 0xa0000000 | | | Unmapped uncached(DMW1 512M) | RW/-- KMEMSIZE | | +---------------------------------+ 0x80000000 | | | User Mapped | | | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 0x00000000 "},"lab2/lab2_3_7_phymemlab_concepts.html":{"url":"lab2/lab2_3_7_phymemlab_concepts.html","title":"附录A. 链接地址/虚地址/物理地址/加载地址以及edata/end/text的含义","keywords":"","body":"链接地址/虚地址/物理地址/加载地址以及edata/end/text的含义 链接脚本简介 ucore kernel各个部分由组成kernel的各个.o或.a文件构成，且各个部分在内存中地址位置由ld工具根据kernel.ld链接脚本（linker script）来设定。ld工具使用命令-T指定链接脚本。链接脚本主要用于规定如何把输入文件（各个.o或.a文件）内的section放入输出文件（lab2/bin/kernel，即ELF格式的ucore内核）内， 并控制输出文件内各部分在程序地址空间内的布局。下面简单分析一下/lab2/tools/kernel.ld，来了解一下ucore内核的地址布局情况。kernel.ld的内容如下所示： OUTPUT_ARCH(loongarch) ENTRY(kernel_entry) SECTIONS { . = 0xa0000000; .text : { . = ALIGN(4); wrs_kernel_text_start = .; _wrs_kernel_text_start = .; *(.startup) *(.text) *(.text.*) *(.gnu.linkonce.t*) *(.mips16.fn.*) *(.mips16.call.*) /* for MIPS */ *(.rodata) *(.rodata.*) *(.gnu.linkonce.r*) *(.rodata1) . = ALIGN(4096); *(.ramexv) } . = ALIGN(16); wrs_kernel_text_end = .; _wrs_kernel_text_end = .; etext = .; _etext = .; .stab : { . = ALIGN(4); PROVIDE(__STAB_BEGIN__ = .); *(.stab) PROVIDE(__STAB_END__ = .); BYTE(0) /* Force the linker to allocate space for this section */ . = ALIGN(4); } .stabstr : { . = ALIGN(4); PROVIDE(__STABSTR_BEGIN__ = .); *(.stabstr) PROVIDE(__STABSTR_END__ = .); BYTE(0) /* Force the linker to allocate space for this section */ . = ALIGN(4); } .data ALIGN(4) : { wrs_kernel_data_start = .; _wrs_kernel_data_start = .; *(.data) *(.data.*) *(.gnu.linkonce.d*) *(.data1) *(.eh_frame) *(.gcc_except_table) . = ALIGN(8); _gp = . + 0x7ff0; /* set gp for MIPS startup code */ /* got*, dynamic, sdata*, lit[48], and sbss should follow _gp */ *(.got.plt) *(.got) *(.dynamic) *(.got2) *(.sdata) *(.sdata.*) *(.lit8) *(.lit4) . = ALIGN(16); } . = ALIGN(16); edata = .; _edata = .; wrs_kernel_data_end = .; _wrs_kernel_data_end = .; .bss ALIGN(4) : { wrs_kernel_bss_start = .; _wrs_kernel_bss_start = .; *(.sbss) *(.scommon) *(.dynbss) *(.bss) *(COMMON) . = ALIGN(16); } . = ALIGN(16); end = .; _end = .; wrs_kernel_bss_end = .; _wrs_kernel_bss_end = .; } 其实从链接脚本的内容，可以大致猜出它指定告诉链接器的各种信息： 内核加载地址：0xa0000000 入口（起始代码）地址： ENTRY(kern_entry) cpu机器类型：loongarch 其最主要的信息是告诉链接器各输入文件的各section应该怎么组合：应该从哪个地址开始放，各个section以什么顺序放，分别怎么对齐等等，最终组成输出文件的各section。除此之外，linker script还可以定义各种符号（如.text、.data、.bss等），形成最终生成的一堆符号的列表（符号表），每个符号包含了符号名字，符号所引用的内存地址，以及其他一些属性信息。符号实际上就是一个地址的符号表示，其本身不占用的程序运行的内存空间。 链接地址/加载地址/虚地址/物理地址 ucore 设定了ucore运行中的虚地址空间，具体设置可看lab2/kern/mm/memlayout.h 中描述的\"Virtual memory map\"图，可以了解虚地址和物理地址的对应关系。lab2/tools/kernel.ld描述的是执行代码的链接地址（link_addr），比如内核起始地址是0x80000000，这是一个虚地址。所以我们可以认为链接地址等于虚地址。当内核开始执行时我们采用直接地址映射方式将虚拟地址映射到物理地址，其映射方式如下： phy addr = CSR.DMW0[31:29] : virtual addr[28:0] 即虚地址和物理地址之间有一个偏移。 edata/end/text的含义 在基于ELF执行文件格式的代码中，存在一些对代码和数据的表述，基本概念如下： BSS段（bss segment）：指用来存放程序中未初始化的全局变量的内存区域。BSS是英文Block Started by Symbol的简称。BSS段属于静态内存分配。 数据段（data segment）：指用来存放程序中已初始化的全局变量的一块内存区域。数据段属于静态内存分配。 代码段（code segment/text segment）：指用来存放程序执行代码的一块内存区域。这部分区域的大小在程序运行前就已经确定，并且内存区域通常属于只读,某些架构也允许代码段为可写，即允许修改程序。在代码段中，也有可能包含一些只读的常数变量，例如字符串常量等。 在lab2/kern/init/init.c的kern_init函数中，声明了外部全局变量： extern char edata[], end[]; 但搜寻所有源码文件*.[ch]，没有发现有这两个变量的定义。那这两个变量从哪里来的呢？其实在lab2/tools/kernel.ld中，可以看到如下内容： … .text : { *(.text .stub .text.* .gnu.linkonce.t.*) } … .data : { *(.data) } … PROVIDE(edata = .); … .bss : { *(.bss) } … PROVIDE(end = .); … 这里的“.”表示当前地址，“.text”表示代码段起始地址，“.data”也是一个地址，可以看出，它即代表了代码段的结束地址，也是数据段的起始地址。类推下去，“edata”表示数据段的结束地址，“.bss”表示数据段的结束地址和BSS段的起始地址，而“end”表示BSS段的结束地址。 这样回头看kerne_init中的外部全局变量，可知edata[]和end[]这些变量是ld根据kernel.ld链接脚本生成的全局变量，表示相应段的起始地址或结束地址等，它们不在任何一个.S、.c或.h文件中定义。 "},"lab3.html":{"url":"lab3.html","title":"Lab 3","keywords":"","body":"实验三：虚拟内存管理 做完实验二后，大家可以了解并掌握物理内存管理中的连续空间分配算法的具体实现以及如何建立二级页表。本次实验是在实验二的基础上，借助于页表机制和实验一中涉及的中断异常处理机制，完成TLB Refill异常处理和Page Fault处理的实现。 "},"lab3/lab3_1_goals.html":{"url":"lab3/lab3_1_goals.html","title":"实验目的","keywords":"","body":"实验目的 了解TLB的初始化 了解LoongArch32架构上的软件重填TLB的实现 了解虚拟内存的Page Fault异常处理实现 "},"lab3/lab3_2_lab2.html":{"url":"lab3/lab3_2_lab2.html","title":"实验内容","keywords":"","body":"实验内容 本次实验是在实验二的基础上，借助于TLB机制和实验一中涉及的异常处理机制，完成TLB相关的异常处理实现。 "},"lab3/lab3_2_1_exercises.html":{"url":"lab3/lab3_2_1_exercises.html","title":"练习","keywords":"","body":"练习 为了实现lab3的目标，lab2提供了2个基本练习，要求完成实验报告。 注意有“LAB3”的注释，代码中所有需要完成的地方（challenge除外）都有“LAB3”和“YOUR CODE”的注释 对实验报告的要求： 基于markdown格式来完成，以文本方式为主 填写各个基本练习中要求完成的报告内容 完成实验后，请分析ucore_lab中提供的参考答案，并请在实验报告中说明你的实现与参考答案的区别 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点） 列出你认为OS原理中很重要，但在实验中没有对应上的知识点 练习1：完成TLB的重填代码（需要编程） 请读者阅读LoongArch32文档中的存储管理和控制状态寄存器有关部分，学习LoongArch32架构的内存管理机制，包括软件填充TLB，TLB项的TLBELO和TLBEHI各个bit的含义，然后完成位于kern/mm/la32_tlb.c中的代码。 这里需要补充的代码包括2个函数： pte2tlblow是从根据uCore的PTE获得相应的页表权限，以及它的物理地址，然后填到TLBELO中。 tlb_refill 要完成LoongArch32虚双页表的适配（即虚拟地址第13位分别为0和1的相邻两个页面放在同一个页表项中），并调用pte2tlblow得到TLBELO所要填写的内容，以及根据虚拟地址得到TLBEHI所要填写的内容，最后调用tlb_replace_random完成TLB重填。 练习2：给未被映射的地址映射上物理页（需要编程） 完成do_pgfault（mm/vmm.c）函数，给未被映射的地址映射上物理页。设置访问权限的时候需要参考页面所在 VMA 的权限，同时需要注意映射物理页时需要操作内存控制结构所指定的页表，而不是内核的页表。注意：在LAB3 EXERCISE 2处填写代码。执行 make　qemu 后，如果通过check_pgfault函数的测试后，会有“check_pgfault()succeeded!”的输出，表示练习1、2基本正确。 请在实验报告中简要说明你的设计实现过程。请回答如下问题： 请描述页目录项（Page Directory Entry）和页表项（Page Table Entry）中组成部分对ucore实现页替换算法的潜在用处。 如果ucore的缺页服务例程在执行过程中访问内存，出现了页访问异常，请问硬件要做哪些事情？ "},"lab3/lab3_2_2_vmmlab_compile.html":{"url":"lab3/lab3_2_2_vmmlab_compile.html","title":"编译方法","keywords":"","body":"编译方法 Makefile修改 在Makefile中取消LAB3 := -DLAB3_EX1 -DLAB3_EX2(第8行)的注释 LAB1 := -DLAB1_EX4 # -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3 LAB3 := -DLAB3_EX1 -DLAB3_EX2 # LAB4 := -DLAB4_EX1 -DLAB4_EX2 # LAB5 := -DLAB5_EX1 -DLAB5_EX2 # LAB6 := -DLAB6_EX2 # LAB7 := -DLAB7_EX1 #-D_SHOW_PHI # LAB8 := -DLAB8_EX1 -DLAB8_EX2 编译并运行代码的命令如下： make make qemu -j 16 补全代码后可以得到如下显示界面（仅供参考） chenyu$ make qemu -j 16 ((THU.CST) os is loading ... Special kernel symbols: entry 0xA00000A0 (phys) etext 0xA0020000 (phys) edata 0xA0153A20 (phys) end 0xA0156D00 (phys) Kernel executable memory footprint: 1244KB memory management: default_pmm_manager memory map: [A0000000, A2000000] freemem start at: A0197000 free pages: 00001E69 ## 00000020 check_alloc_page() succeeded! check_pgdir() succeeded! check_boot_pgdir() succeeded! check_slab() succeeded! kmalloc_init() succeeded! check_vma_struct() succeeded! check_pgfault() succeeded! check_vmm() succeeded. LAB3 Check Pass! "},"lab3/lab3_3_vmm.html":{"url":"lab3/lab3_3_vmm.html","title":"虚拟内存管理","keywords":"","body":"虚拟内存管理 "},"lab3/lab3_3_1_vmm_principles.html":{"url":"lab3/lab3_3_1_vmm_principles.html","title":"基本原理概述","keywords":"","body":"基本原理概述 什么是虚拟内存？简单地说是指程序员或CPU“看到”的内存。但有几点需要注意： 虚拟内存单元不一定有实际的物理内存单元对应，即实际的物理内存单元可能不存在； 如果虚拟内存单元对应有实际的物理内存单元，那二者的地址一般是不相等的； 通过操作系统实现的某种内存映射可建立虚拟内存与物理内存的对应关系，使得程序员或CPU访问的虚拟内存地址会自动转换为一个物理内存地址。 那么这个“虚拟”的作用或意义在哪里体现呢？在操作系统中，虚拟内存其实包含多个虚拟层次，在不同的层次体现了不同的作用。首先，在有了分页机制后，程序员或CPU“看到”的地址已经不是实际的物理地址了，这已经有一层虚拟化，我们可简称为内存地址虚拟化。有了内存地址虚拟化，我们就可以通过设置页表项来限定软件运行时的访问空间，确保软件运行不越界，完成内存访问保护的功能。 通过内存地址虚拟化，可以使得软件在没有访问某虚拟内存地址时不分配具体的物理内存，而只有在实际访问某虚拟内存地址时，操作系统再动态地分配物理内存，建立虚拟内存到物理内存的页映射关系，这种技术称为按需分页（demand paging）。把不经常访问的数据所占的内存空间临时写到硬盘上，这样可以腾出更多的空闲内存空间给经常访问的数据；当CPU访问到不经常访问的数据时，再把这些数据从硬盘读入到内存中，这种技术称为页换入换出（page　swap in/out）。这种内存管理技术给了程序员更大的内存“空间”，从而可以让更多的程序在内存中并发运行。 "},"lab3/lab3_3_2_labs_steps.html":{"url":"lab3/lab3_3_2_labs_steps.html","title":"实验执行流程概述","keywords":"","body":"实验执行流程概述 本次实验主要完成ucore内核对虚拟内存的管理工作。其总体设计思路还是比较简单，即首先完成初始化虚拟内存管理机制，即需要设置好哪些页需要放在物理内存中，哪些页不需要放在物理内存中，而是可被换出到硬盘上，并涉及完善建立页表映射、页访问异常处理操作等函数实现。然后就执行一组访存测试，看看我们建立的页表项是否能够正确完成虚实地址映射，是否正确描述了虚拟内存页在物理内存中还是在硬盘上，是否能够正确把虚拟内存页在物理内存和硬盘之间进行传递，是否正确实现了页面替换算法等。lab3的总体执行流程如下。 首先是初始化过程。参考ucore总控函数init的代码，可以看到在调用完成虚拟内存初始化的vmm_init函数之前，需要首先调用setup_exception_vector函数和pic_init函数等，这些工作与lab1的中断异常初始化工作的内容是相同的。接着是调用pmm_init函数完成物理内存的管理，这也是我们lab2已经完成的内容。 在调用完pmm_init函数之后，将进一步调用三个lab3中才有的新函数vmm_init。这个函数涉及了本次实验中的练习2。但需要注意的是，我们需要先实现练习1中的TLB的填充逻辑才能进行vmm_init。TLB的填充逻辑位于kern/mm/la32_tlb.c文件中，这里需要同学们查阅LoongArch32的文档中的“存储管理”章节与“控制状态寄存器”章节，了解LoongArch32的软件填充TLB的基本原理以及CSR中TLB有关寄存器各个位的作用，以实现练习1。 回到ucore的init代码，第一个函数vmm_init是检查我们的练习1与练习2是否正确实现了。为了表述不在物理内存中的“合法”虚拟页，需要有数据结构来描述这样的页，为此ucore建立了mm_struct和vma_struct数据结构（接下来的小节中有进一步详细描述），假定我们已经描述好了这样的“合法”虚拟页，当ucore访问这些“合法”虚拟页时，会由于没有虚实地址映射而产生页访问异常。如果我们正确实现了练习2，则do_pgfault函数会申请一个空闲物理页，并建立好虚实映射关系，从而使得这样的“合法”虚拟页有实际的物理页帧对应。这样练习1就算完成了。 接下来将进一步分析完成lab3主要注意的关键问题和涉及的关键数据结构。 "},"lab3/lab3_3_3_data_structures.html":{"url":"lab3/lab3_3_3_data_structures.html","title":"关键数据结构和相关函数分析","keywords":"","body":"关键数据结构和相关函数分析 对于第一个问题的出现，在于实验二中有关内存的数据结构和相关操作都是直接针对实际存在的资源--物理内存空间的管理，没有从一般应用程序对内存的“需求”考虑，即需要有相关的数据结构和操作来体现一般应用程序对虚拟内存的“需求”。一般应用程序的对虚拟内存的“需求”与物理内存空间的“供给”没有直接的对应关系，ucore是通过page fault异常处理来间接完成这二者之间的衔接。 page_fault函数不知道哪些是“合法”的虚拟页，原因是ucore还缺少一定的数据结构来描述这种不在物理内存中的“合法”虚拟页。为此ucore通过建立mm_struct和vma_struct数据结构，描述了ucore模拟应用程序运行所需的合法内存空间。当访问内存产生page fault异常时，可获得访问的内存的方式（读或写）以及具体的虚拟内存地址，这样ucore就可以查询此地址，看是否属于vma_struct数据结构中描述的合法地址范围中，如果在，则可根据具体情况进行请求调页；如果不在，则报错。mm_struct和vma_struct数据结构结合页表表示虚拟地址空间和物理地址空间的示意图如下所示： 图 虚拟地址空间和物理地址空间的示意图 在ucore中描述应用程序对虚拟内存“需求”的数据结构是vma_struct（定义在vmm.h中），以及针对vma_struct的函数操作。这里把一个vma_struct结构的变量简称为vma变量。vma_struct的定义如下： struct vma_struct { // the set of vma using the same PDT struct mm_struct *vm_mm; uintptr_t vm_start; // start addr of vma uintptr_t vm_end; // end addr of vma uint32_t vm_flags; // flags of vma //linear list link which sorted by start addr of vma list_entry_t list_link; }; vm_start和vm_end描述了一个连续地址的虚拟内存空间的起始位置和结束位置，这两个值都应该是PGSIZE 对齐的，而且描述的是一个合理的地址空间范围（即严格确保 vm_start #define VM_READ 0x00000001 //只读 #define VM_WRITE 0x00000002 //可读写 #define VM_EXEC 0x00000004 //可执行 vm_mm是一个指针，指向一个比vma_struct更高的抽象层次的数据结构mm_struct，这里把一个mm_struct结构的变量简称为mm变量。这个数据结构表示了包含所有虚拟内存空间的共同属性，具体定义如下 struct mm_struct { // linear list link which sorted by start addr of vma list_entry_t mmap_list; // current accessed vma, used for speed purpose struct vma_struct *mmap_cache; pde_t *pgdir; // the PDT of these vma int map_count; // the count of these vma }; mmap_list是双向链表头，链接了所有属于同一页目录表的虚拟内存空间，mmap_cache是指向当前正在使用的虚拟内存空间，由于操作系统执行的“局部性”原理，当前正在用到的虚拟内存空间在接下来的操作中可能还会用到，这时就不需要查链表，而是直接使用此指针就可找到下一次要用到的虚拟内存空间。由于mmap_cache 的引入，可使得 mm_struct 数据结构的查询加速 30% 以上。pgdir 所指向的就是 mm_struct数据结构所维护的页表。通过访问pgdir可以查找某虚拟地址对应的页表项是否存在以及页表项的属性等。map_count记录mmap_list 里面链接的 vma_struct的个数。 涉及vma_struct的操作函数也比较简单，主要包括三个： vma_create--创建vma insert_vma_struct--插入一个vma find_vma--查询vma。 vma_create函数根据输入参数vm_start、vm_end、vm_flags来创建并初始化描述一个虚拟内存空间的vma_struct结构变量。insert_vma_struct函数完成把一个vma变量按照其空间位置[vma->vm_start,vma->vm_end]从小到大的顺序插入到所属的mm变量中的mmap_list双向链表中。find_vma根据输入参数addr和mm变量，查找在mm变量中的mmap_list双向链表中某个vma包含此addr，即vma->vm_startend。这三个函数与后续讲到的page fault异常处理有紧密联系。 涉及mm_struct的操作函数比较简单，只有mm_create和mm_destroy两个函数，从字面意思就可以看出是是完成mm_struct结构的变量创建和删除。在mm_create中用kmalloc分配了一块空间，所以在mm_destroy中也要对应进行释放。在ucore运行过程中，会产生描述虚拟内存空间的vma_struct结构，所以在mm_destroy中也要进对这些mmap_list中的vma进行释放。 "},"lab3/lab3_4_tlbmiss_handler.html":{"url":"lab3/lab3_4_tlbmiss_handler.html","title":"TLB Refill例外处理","keywords":"","body":"TLB Refill例外处理 对于LoongArch32架构而言，实现虚存管理的关键在于实现TLB相关的例外处理。其过程中主要涉及到函数 -- handle_tlbmiss的具体实现。在TLB中无法找到的页面或者该页面不为有效时，CPU会产生TLB Refill例外，从而设置CSR.PRMD并跳转到TLB Refill例外处理程序开始处理。这个TLB Refill用于完成软件定义页表的实现。在uCore的设计中，我们将TLB Refill例外入口的设置与通用例外入口设置相同，从而可以复用保存现场，设置CRMD、内核栈等代码，然后由trap_dispatch根据读取已经保存的trapframe中的CSR.ESTAT寄存器的值判断是否为TLB Refill相关的例外，若是，则调用handle_tlbmiss。 因此，大致的调用关系如下： loongarch_trap-->trap_dispatch-->handle_tlbmiss 而当handle_tlbmiss函数处理时，如果发现对应的PTE不存在，就需要对缺页（Page Fault）进行处理。这里首先调用了pgfault_handler函数，进行了相关检查后最终调用到了do_pgfault函数，下面需要具体分析一下do_pgfault函数。do_pgfault的调用关系如下所示： loongarch_trap-->trap_dispatch-->handle_tlbmiss-->pgfault_handler-->do_pgfault ucore中do_pgfault函数是完成页访问异常处理的主要函数，它根据在trapframe中保存的CPU的控制寄存器CSR.BADVA中获取的页访问异常的虚拟地址以及根据errorCode的错误类型来查找此地址是否在某个VMA的地址范围内以及是否满足正确的读写权限，如果在此范围内并且权限也正确，这认为这是一次合法访问，但没有建立虚实对应关系。所以需要分配一个空闲的内存页，并修改页表完成虚地址到物理地址的映射，刷新TLB，然后返回到产生页访问异常的指令处重新执行此指令。如果该虚地址不在某VMA范围内，则认为是一次非法访问。 "},"lab4.html":{"url":"lab4.html","title":"Lab 4","keywords":"","body":"实验四：内核线程管理 "},"lab4/lab4_1_goals.html":{"url":"lab4/lab4_1_goals.html","title":"实验目的","keywords":"","body":"实验目的 了解内核线程创建/执行的管理过程 了解内核线程的切换和基本调度过程 "},"lab4/lab4_2_labs.html":{"url":"lab4/lab4_2_labs.html","title":"实验内容","keywords":"","body":"实验内容 实验2/3完成了物理和虚拟内存管理，这给创建内核线程（内核线程是一种特殊的进程）打下了提供内存管理的基础。当一个程序加载到内存中运行时，首先通过ucore OS的内存管理子系统分配合适的空间，然后就需要考虑如何分时使用CPU来“并发”执行多个程序，让每个运行的程序（这里用线程或进程表示）“感到”它们各自拥有“自己”的CPU。 本次实验将首先接触的是内核线程的管理。内核线程是一种特殊的进程，内核线程与用户进程的区别有两个： 内核线程只运行在内核态 用户进程会在在用户态和内核态交替运行 所有内核线程共用ucore内核内存空间，不需为每个内核线程维护单独的内存空间 而用户进程需要维护各自的用户内存空间 相关原理介绍可看附录A：【原理】进程/线程的属性与特征解析。 "},"lab4/lab4_2_1_exercises.html":{"url":"lab4/lab4_2_1_exercises.html","title":"练习","keywords":"","body":"练习 为了实现lab4的目标，lab2提供了3个基本练习和1个扩展练习，要求完成实验报告。 注意有“LAB4”的注释，代码中所有需要完成的地方（challenge除外）都有“LAB4”和“YOUR CODE”的注释 对实验报告的要求： 基于markdown格式来完成，以文本方式为主 填写各个基本练习中要求完成的报告内容 完成实验后，请分析sys_code中提供的参考答案，并请在实验报告中说明你的实现与参考答案的区别 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点） 列出你认为OS原理中很重要，但在实验中没有对应上的知识点 练习1：分配并初始化一个进程控制块（需要编码） alloc_proc函数（位于kern/process/proc.c中）负责分配并返回一个新的struct proc_struct结构，用于存储新建立的内核线程的管理信息。ucore需要对这个结构进行最基本的初始化，你需要完成这个初始化过程。 【提示】在alloc_proc函数的实现中，需要初始化的proc_struct结构中的成员变量至少包括：state/pid/runs/kstack/need_resched/parent/mm/context/tf/cr3/flags/name。 请在实验报告中简要说明你的设计实现过程。请回答如下问题： 请说明proc_struct中struct context context和struct trapframe *tf成员变量含义和在本实验中的作用是啥？（提示通过看代码和编程调试可以判断出来） 练习2：为新创建的内核线程分配资源（需要编码） 创建一个内核线程需要分配和设置好很多资源。kernel_thread函数通过调用do_fork函数完成具体内核线程的创建工作。do_kernel函数会调用alloc_proc函数来分配并初始化一个进程控制块，但alloc_proc只是找到了一小块内存用以记录进程的必要信息，并没有实际分配这些资源。ucore一般通过do_fork实际创建新的内核线程。do_fork的作用是，创建当前内核线程的一个副本，它们的执行上下文、代码、数据都一样，但是存储位置不同。在这个过程中，需要给新内核线程分配资源，并且复制原进程的状态。你需要完成在kern/process/proc.c中的do_fork函数中的处理过程。它的大致执行步骤包括： 调用alloc_proc，首先获得一块用户信息块。 为进程分配一个内核栈。 复制原进程的内存管理信息到新进程（但内核线程不必做此事） 复制原进程上下文到新进程 将新进程添加到进程列表 唤醒新进程 返回新进程号 请在实验报告中简要说明你的设计实现过程。请回答如下问题： 请说明ucore是否做到给每个新fork的线程一个唯一的id？请说明你的分析和理由。 练习3：阅读代码，理解 proc_run 函数和它调用的函数如何完成进程切换的。（无编码工作） 请在实验报告中简要说明你对proc_run函数的分析。并回答如下问题： 在本实验的执行过程中，创建且运行了几个内核线程？ 语句local_intr_save(intr_flag);....local_intr_restore(intr_flag);在这里有何作用?请说明理由 完成代码编写后，编译并运行代码：make qemu -j 16 如果可以得到如 编译方法所示的显示内容（仅供参考，不是标准答案输出），则基本正确。 扩展练习Challenge：实现支持任意大小的内存分配算法 这不是本实验的内容，其实是上一次实验内存的扩展，但考虑到现在的slab算法比较复杂，有必要实现一个比较简单的任意大小内存分配算法。可参考本实验中的slab如何调用基于页的内存分配算法（注意，不是要你关注slab的具体实现）来实现first-fit/best-fit/worst-fit/buddy等支持任意大小的内存分配算法。。 【注意】下面是相关的Linux实现文档，供参考 SLOB http://en.wikipedia.org/wiki/SLOB http://lwn.net/Articles/157944/ SLAB https://www.ibm.com/developerworks/cn/linux/l-linux-slab-allocator/ "},"lab4/lab4_2_2_compile.html":{"url":"lab4/lab4_2_2_compile.html","title":"编译方法","keywords":"","body":"编译方法 Makefile修改 在Makefile中取消LAB4 := -DLAB4_EX1 -DLAB4_EX2(第9行)的注释 LAB1 := -DLAB1_EX4 # -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3 LAB3 := -DLAB3_EX1 -DLAB3_EX2 LAB4 := -DLAB4_EX1 -DLAB4_EX2 # LAB5 := -DLAB5_EX1 -DLAB5_EX2 # LAB6 := -DLAB6_EX2 # LAB7 := -DLAB7_EX1 #-D_SHOW_PHI # LAB8 := -DLAB8_EX1 -DLAB8_EX2 编译并运行代码的命令如下： make make qemu -j 16 补全代码后可以得到如下显示界面（仅供参考） chenyu$ make qemu -j 16 (THU.CST) os is loading ... Special kernel symbols: entry 0xA00000A0 (phys) etext 0xA0020000 (phys) edata 0xA0153CF0 (phys) end 0xA0156FD0 (phys) Kernel executable memory footprint: 1244KB memory management: default_pmm_manager memory map: [A0000000, A2000000] freemem start at: A0197000 free pages: 00001E69 ## 00000020 check_alloc_page() succeeded! check_pgdir() succeeded! check_boot_pgdir() succeeded! check_slab() succeeded! kmalloc_init() succeeded! check_vma_struct() succeeded! check_pgfault() succeeded! check_vmm() succeeded. sched class: RR_scheduler proc_init succeeded this initproc, pid = 1, name = \"init\" To U: \"(null)\". To U: \"en.., Bye, Bye. :)\" kernel panic at kern/process/proc.c:1274: LAB4 Check Passed! Welcome to the kernel debug monitor!! Type 'help' for a list of commands. K> "},"lab4/lab4_3_kernel_thread_management.html":{"url":"lab4/lab4_3_kernel_thread_management.html","title":"内核线程管理","keywords":"","body":"内核线程管理 "},"lab4/lab4_3_1_lab_steps.html":{"url":"lab4/lab4_3_1_lab_steps.html","title":"实验执行流程概述","keywords":"","body":"实验执行流程概述 为了实现内核线程，需要设计管理线程的数据结构，即进程控制块（在这里也可叫做线程控制块）。如果要让内核线程运行，我们首先要创建内核线程对应的进程控制块，还需把这些进程控制块通过链表连在一起，便于随时进行插入，删除和查找操作等进程管理事务。这个链表就是进程控制块链表。然后在通过调度器（scheduler）来让不同的内核线程在不同的时间段占用CPU执行，实现对CPU的分时共享。那lab4中是如何一步一步实现这个过程的呢？ 我们还是从kern/init/init.c中的kern_init函数入手分析。在kern_init函数中，当完成虚拟内存的初始化工作后，就调用了proc_init函数，这个函数完成了idleproc内核线程和initproc内核线程的创建或复制工作，这也是本次实验要完成的练习。idleproc内核线程的工作就是不停地查询，看是否有其他内核线程可以执行了，如果有，马上让调度器选择那个内核线程执行（请参考cpu_idle函数的实现）。所以idleproc内核线程是在ucore操作系统没有其他内核线程可执行的情况下才会被调用。接着就是调用kernel_thread函数来创建initproc内核线程。initproc内核线程的工作就是显示“Hello World”，表明自己存在且能正常工作了。 调度器会在特定的调度点上执行调度，完成进程切换。在lab4中，这个调度点就一处，即在cpu_idle函数中，此函数如果发现当前进程（也就是idleproc）的need_resched置为1（在初始化idleproc的进程控制块时就置为1了），则调用schedule函数，完成进程调度和进程切换。进程调度的过程其实比较简单，就是在进程控制块链表中查找到一个“合适”的内核线程，所谓“合适”就是指内核线程处于“PROC_RUNNABLE”状态。在接下来的switch_to函数(在后续有详细分析，有一定难度，需深入了解一下)完成具体的进程切换过程。一旦切换成功，那么initproc内核线程就可以通过显示字符串来表明本次实验成功。 接下来将主要介绍了进程创建所需的重要数据结构--进程控制块proc_struct，以及ucore创建并执行内核线程idleproc和initproc的两种不同方式，特别是创建initproc的方式将被延续到实验五中，扩展为创建用户进程的主要方式。另外，还初步涉及了进程调度（实验六涉及并会扩展）和进程切换内容。 "},"lab4/lab4_3_2_pcb.html":{"url":"lab4/lab4_3_2_pcb.html","title":"设计关键数据结构 -- 进程控制块","keywords":"","body":"设计关键数据结构 -- 进程控制块 在实验四中，进程管理信息用struct proc_struct表示，在kern/process/proc.h中定义如下： struct proc_struct { enum proc_state state; // Process state int pid; // Process ID int runs; // the running times of Proces uintptr_t kstack; // Process kernel stack volatile bool need_resched; // bool value: need to be rescheduled to release CPU? struct proc_struct *parent; // the parent process struct mm_struct *mm; // Process's memory management field struct context context; // Switch here to run process struct trapframe *tf; // Trap frame for current interrupt uintptr_t cr3; // CR3 register: the base addr of Page Directroy Table(PDT) uint32_t flags; // Process flag char name[PROC_NAME_LEN + 1]; // Process name list_entry_t list_link; // Process link list list_entry_t hash_link; // Process hash list int exit_code; // exit code (be sent to parent proc) uint32_t wait_state; // waiting state struct proc_struct *cptr, *yptr, *optr; // relations between processes struct run_queue *rq; // running queue contains Process list_entry_t run_link; // the entry linked in run queue int time_slice; // time slice for occupying the CPU struct fs_struct *fs_struct; // the file related info(pwd, files_count, files_array, fs_semaphore) of process }; 下面重点解释一下几个比较重要的成员变量： ● mm：内存管理的信息，包括内存映射列表、页表指针等。mm成员变量在lab3中用于虚存管理。但在实际OS中，内核线程常驻内存，不需要考虑swap page问题，在lab5中涉及到了用户进程，才考虑进程用户内存空间的swap page问题，mm才会发挥作用。所以在lab4中mm对于内核线程就没有用了，这样内核线程的proc_struct的成员变量*mm=0是合理的。mm里有个很重要的项pgdir，记录的是该进程使用的一级页表的物理地址。由于*mm=NULL，所以在proc_struct数据结构中需要有一个代替pgdir项来记录页表起始地址，这就是proc_struct数据结构中的cr3成员变量。 ● state：进程所处的状态。 ● parent：用户进程的父进程（创建它的进程）。在所有进程中，只有一个进程没有父进程，就是内核创建的第一个内核线程idleproc。内核根据这个父子关系建立一个树形结构，用于维护一些特殊的操作，例如确定某个进程是否可以对另外一个进程进行某种操作等等。 ● context：进程的上下文，用于进程切换（参见switch.S）。在 uCore中，所有的进程在内核中也是相对独立的（例如独立的内核堆栈以及上下文等等）。使用 context 保存寄存器的目的就在于在内核态中能够进行上下文之间的切换。实际利用context进行上下文切换的函数是在kern/process/switch.S中定义switch_to。 ● tf：中断帧的指针，总是指向内核栈的某个位置：当进程从用户空间跳到内核空间时，中断帧记录了进程在被中断前的状态。当内核需要跳回用户空间时，需要调整中断帧以恢复让进程继续执行的各寄存器值。除此之外，uCore内核允许嵌套中断。因此为了保证嵌套中断发生时tf 总是能够指向当前的trapframe，uCore 在内核栈上维护了 tf 的链，可以参考trap.c::trap函数做进一步的了解。 ● cr3: cr3 保存页表的物理地址，目的就是进程切换的时候方便直接使用 lcr3实现页表切换，避免每次都根据 mm 来计算 cr3。mm数据结构是用来实现用户空间的虚存管理的，但是内核线程没有用户空间，它执行的只是内核中的一小段代码（通常是一小段函数），所以它没有mm 结构，也就是NULL。当某个进程是一个普通用户态进程的时候，PCB 中的 cr3 就是 mm 中页表（pgdir）的物理地址；而当它是内核线程的时候，cr3 等于boot_cr3。而boot_cr3指向了uCore启动时建立好的饿内核虚拟空间的页目录表首地址。 ● kstack: 每个线程都有一个内核栈，并且位于内核地址空间的不同位置。对于内核线程，该栈就是运行时的程序使用的栈；而对于普通进程，该栈是发生特权级改变的时候使保存被打断的硬件信息用的栈。uCore在创建进程时分配了 2 个连续的物理页（参见memlayout.h中KSTACKSIZE的定义）作为内核栈的空间。这个栈很小，所以内核中的代码应该尽可能的紧凑，并且避免在栈上分配大的数据结构，以免栈溢出，导致系统崩溃。kstack记录了分配给该进程/线程的内核栈的位置。主要作用有以下几点。首先，当内核准备从一个进程切换到另一个的时候，需要根据kstack 的值正确的设置好 tss （可以回顾一下在实验一中讲述的 tss 在中断处理过程中的作用），以便在进程切换以后再发生中断时能够使用正确的栈。其次，内核栈位于内核地址空间，并且是不共享的（每个线程都拥有自己的内核栈），因此不受到 mm 的管理，当进程退出的时候，内核能够根据 kstack 的值快速定位栈的位置并进行回收。uCore 的这种内核栈的设计借鉴的是 linux 的方法（但由于内存管理实现的差异，它实现的远不如 linux 的灵活），它使得每个线程的内核栈在不同的位置，这样从某种程度上方便调试，但同时也使得内核对栈溢出变得十分不敏感，因为一旦发生溢出，它极可能污染内核中其它的数据使得内核崩溃。如果能够通过页表，将所有进程的内核栈映射到固定的地址上去，能够避免这种问题，但又会使得进程切换过程中对栈的修改变得相当繁琐。感兴趣的同学可以参考 linux kernel 的代码对此进行尝试。 为了管理系统中所有的进程控制块，uCore维护了如下全局变量（位于kern/process/proc.c）： ● static struct proc *current：当前占用CPU且处于“运行”状态进程控制块指针。通常这个变量是只读的，只有在进程切换的时候才进行修改，并且整个切换和修改过程需要保证操作的原子性，目前至少需要屏蔽中断。可以参考 switch_to 的实现。 ● static struct proc *initproc：本实验中，指向一个内核线程。本实验以后，此指针将指向第一个用户态进程。 ● static list_entry_t hash_list[HASH_LIST_SIZE]：所有进程控制块的哈希表，proc_struct中的成员变量hash_link将基于pid链接入这个哈希表中。 ● list_entry_t proc_list：所有进程控制块的双向线性列表，proc_struct中的成员变量list_link将链接入这个链表中。 "},"lab4/lab4_3_3_create_exec_kernel_thread.html":{"url":"lab4/lab4_3_3_create_exec_kernel_thread.html","title":"创建并执行内核线程","keywords":"","body":"创建并执行内核线程 建立进程控制块（proc.c中的alloc_proc函数）后，现在就可以通过进程控制块来创建具体的进程/线程了。首先，考虑最简单的内核线程，它通常只是内核中的一小段代码或者函数，没有自己的“专属”空间。这是由于在uCore OS启动后，已经对整个内核内存空间进行了管理，通过设置页表建立了内核虚拟空间（即boot_cr3指向的二级页表描述的空间）。所以uCore OS内核中的所有线程都不需要再建立各自的页表，只需共享这个内核虚拟空间就可以访问整个物理内存了。从这个角度看，内核线程被uCore OS内核这个大“内核进程”所管理。 "},"lab4/lab4_3_3_1_create_kthread_idleproc.html":{"url":"lab4/lab4_3_3_1_create_kthread_idleproc.html","title":"创建第0个内核线程idleproc","keywords":"","body":"创建第 0 个内核线程 idleproc 在init.c::kern_init函数调用了proc.c::proc_init函数。proc_init函数启动了创建内核线程的步骤。首先当前的执行上下文（从kern_init 启动至今）就可以看成是uCore内核（也可看做是内核进程）中的一个内核线程的上下文。为此，uCore通过给当前执行的上下文分配一个进程控制块以及对它进行相应初始化，将其打造成第0个内核线程 -- idleproc。具体步骤如下： 首先调用alloc_proc函数来通过kmalloc函数获得proc_struct结构的一块内存块-，作为第0个进程控制块。并把proc进行初步初始化（即把proc_struct中的各个成员变量清零）。但有些成员变量设置了特殊的值，比如： proc->state = PROC_UNINIT; 设置进程为“初始”态 proc->pid = -1; 设置进程pid的未初始化值 proc->cr3 = boot_cr3; 使用内核页目录表的基址 ... 上述三条语句中,第一条设置了进程的状态为“初始”态，这表示进程已经“出生”了，正在获取资源茁壮成长中；第二条语句设置了进程的pid为-1，这表示进程的“身份证号”还没有办好；第三条语句表明由于该内核线程在内核中运行，故采用为uCore内核已经建立的页表，即设置为在uCore内核页表的起始地址boot_cr3。后续实验中可进一步看出所有内核线程的内核虚地址空间（也包括物理地址空间）是相同的。既然内核线程共用一个映射内核空间的页表，这表示内核空间对所有内核线程都是“可见”的，所以更精确地说，这些内核线程都应该是从属于同一个唯一的“大内核进程”—uCore内核。 接下来，proc_init函数对idleproc内核线程进行进一步初始化： idleproc->pid = 0; idleproc->state = PROC_RUNNABLE; idleproc->kstack = (uintptr_t)bootstack; idleproc->need_resched = 1; set_proc_name(idleproc, \"idle\"); 需要注意前4条语句。第一条语句给了idleproc合法的身份证号--0，这名正言顺地表明了idleproc是第0个内核线程。通常可以通过pid的赋值来表示线程的创建和身份确定。“0”是第一个的表示方法是计算机领域所特有的，比如C语言定义的第一个数组元素的小标也是“0”。第二条语句改变了idleproc的状态，使得它从“出生”转到了“准备工作”，就差uCore调度它执行了。第三条语句设置了idleproc所使用的内核栈的起始地址。需要注意以后的其他线程的内核栈都需要通过分配获得，因为uCore启动时设置的内核栈直接分配给idleproc使用了。第四条很重要，因为uCore希望当前CPU应该做更有用的工作，而不是运行idleproc这个“无所事事”的内核线程，所以把idleproc->need_resched设置为“1”，结合idleproc的执行主体--cpu_idle函数的实现，可以清楚看出如果当前idleproc在执行，则只要此标志为1，马上就调用schedule函数要求调度器切换其他进程执行。 "},"lab4/lab4_3_3_2_create_kthread_initproc.html":{"url":"lab4/lab4_3_3_2_create_kthread_initproc.html","title":"创建第1个内核线程initproc","keywords":"","body":"创建第 1 个内核线程 initproc 第0个内核线程主要工作是完成内核中各个子系统的初始化，然后就通过执行cpu_idle函数开始过退休生活了。所以uCore接下来还需创建其他进程来完成各种工作，但idleproc内核子线程自己不想做，于是就通过调用kernel_thread函数创建了一个内核线程init_main。在实验四中，这个子内核线程的工作就是输出一些字符串，然后就返回了（参看init_main函数）。但在后续的实验中，init_main的工作就是创建特定的其他内核线程或用户进程（实验五涉及）。下面我们来分析一下创建内核线程的函数kernel_thread： kernel_thread(int (*fn)(void *), void *arg, uint32_t clone_flags) { struct trapframe tf; memset(&tf, 0, sizeof(struct trapframe)); tf.tf_regs.reg_r[LOONGARCH_REG_A0] = (uint32_t)arg; tf.tf_regs.reg_r[LOONGARCH_REG_A1] = (uint32_t)fn; tf.tf_regs.reg_r[LOONGARCH_REG_A7] = 0; tf.tf_estat = read_csr_exst(); tf.tf_era = (uint32_t)kernel_thread_entry; return do_fork(clone_flags | CLONE_VM, 0, &tf); } 注意，kernel_thread函数采用了局部变量tf来放置保存内核线程的临时中断帧，并把中断帧的指针传递给do_fork函数，而do_fork函数会调用copy_thread函数来在新创建的进程内核栈上专门给进程的中断帧分配一块空间。给中断帧分配完空间后，就需要构造新进程的中断帧，具体过程是：首先给tf进行清零初始化，并设置tf.tf_regs.reg_r[LOONGARCH_REG_A0],tf.tf_regs.reg_r[LOONGARCH_REG_A1],tf.tf_regs.reg_r[LOONGARCH_REG_V7]，tf.tf_estat寄存器的值。tf.tf_era的指出了是kernel_thread_entry（位于kern/process/entry.S中），kernel_thread_entry是entry.S中实现的汇编函数，它做的事情很简单： kernel_thread_entry: addi.w sp, sp, -16 //goto kernel_thread addi.w t0, a1, 0 // la.abs t0, a1 jirl ra, t0, 0 // bl a1 move v0, a0 //goto do_exit():see proc.c la.abs t0, do_exit jirl ra, t0, 0 从上可以看出，kernel_thread_entry函数主要为内核线程的主体fn函数做了一个准备开始和结束运行的“壳”，并把函数fn的参数arg（保存在a0寄存器中）压栈，然后调用fn函数，把函数返回值a0寄存器内容压栈，调用do_exit函数退出线程执行。 do_fork是创建线程的主要函数。kernel_thread函数通过调用do_fork函数最终完成了内核线程的创建工作。下面我们来分析一下do_fork函数的实现（练习2）。do_fork函数主要做了以下6件事情： 分配并初始化进程控制块（alloc_proc函数）； 分配并初始化内核栈（setup_stack函数）； 根据clone_flag标志复制或共享进程内存管理结构（copy_mm函数）； 设置进程在内核（将来也包括用户态）正常运行和调度所需的中断帧和执行上下文（copy_thread函数）； 把设置好的进程控制块放入hash_list和proc_list两个全局进程链表中； 自此，进程已经准备好执行了，把进程状态设置为“就绪”态； 设置返回码为子进程的id号。 这里需要注意的是，如果上述前3步执行没有成功，则需要做对应的出错处理，把相关已经占有的内存释放掉。copy_mm函数目前只是把current->mm设置为NULL，这是由于目前在实验四中只能创建内核线程，proc->mm描述的是进程用户态空间的情况，所以目前mm还用不上。copy_thread函数做的事情比较多，代码如下： static void copy_thread(struct proc_struct *proc, uintptr_t esp, struct trapframe *tf) { proc->tf = (struct trapframe *)(proc->kstack + KSTACKSIZE) - 1; *(proc->tf) = *tf; proc->tf->tf_regs.reg_r[LOONGARCH_REG_A7] = 0; // use A7 as syscall result register if(esp == 0) //a kernel thread esp = (uintptr_t)proc->tf - 32; proc->tf->tf_regs.reg_r[LOONGARCH_REG_SP] = esp; proc->context.sf_ra = (uintptr_t)forkret; proc->context.sf_sp = (uintptr_t)(proc->tf) - 32; } ///////////////////////// 此函数首先在内核堆栈的顶部设置中断帧大小的一块栈空间，并在此空间中拷贝在kernel_thread函数建立的临时中断帧的初始值，并进一步设置中断帧中的栈指针sp，这表示此内核线程在执行过程中，能响应中断，打断当前的执行。执行到这步后，此进程的中断帧就建立好了，对于initproc而言，它的中断帧如下所示： ////////////////////////// //所在地址位置 initproc->tf= (proc->kstack+KSTACKSIZE) – sizeof (struct trapframe); //具体内容 initproc->tf.tf_regs.reg_r[LOONGARCH_REG_A0] = (uint32_t)init_main; initproc->tf.tf_regs.reg_r[LOONGARCH_REG_A1] = (uint32_t)fn; initproc->tf.tf_regs.reg_r[LOONGARCH_REG_A7] = 0; initproc->tf.tf_era = (uint32_t)kernel_thread_entry; initproc->tf->tf_regs.reg_r[LOONGARCH_REG_SP] = esp; initproc->context.sf_ra = (uintptr_t)forkret; initproc->context.sf_sp = (uintptr_t)(initproc->tf) - 32; 设置好中断帧后，最后就是设置initproc的进程上下文，（process context，也称执行现场）了。只有设置好执行现场后，一旦uCore调度器选择了initproc执行，就需要根据initproc->context中保存的执行现场来恢复initproc的执行。这里设置了initproc的执行现场中主要的两个信息：上次停止执行时的下一条指令地址context.sf_ra和上次停止执行时的堆栈地址context.sf_sp。其实initproc还没有执行过，所以这其实就是initproc实际执行的第一条指令地址和堆栈指针。可以看出，由于initproc的中断帧占用了实际给initproc分配的栈空间的顶部，所以initproc就只能把栈顶指针context.sf_sp设置在initproc的中断帧的起始位置。根据context.sf_ra的赋值，可以知道initproc实际开始执行的地方在forkret函数（主要完成do_fork函数返回的处理工作）处。至此，initproc内核线程已经做好准备执行了。 "},"lab4/lab4_3_3_3_sched_run_kthread.html":{"url":"lab4/lab4_3_3_3_sched_run_kthread.html","title":"调度并执行内核线程initproc","keywords":"","body":"调度并执行内核线程 initproc 在uCore执行完proc_init函数后，就创建好了两个内核线程：idleproc和initproc，这时uCore当前的执行现场就是idleproc，等到执行到init函数的最后一个函数cpu_idle之前，uCore的所有初始化工作就结束了，idleproc将通过执行cpu_idle函数让出CPU，给其它内核线程执行，具体过程如下： void cpu_idle(void) { while (1) { if (current->need_resched) { schedule(); …… 首先，判断当前内核线程idleproc的need_resched是否不为0，回顾前面“创建第一个内核线程idleproc”中的描述，proc_init函数在初始化idleproc中，就把idleproc->need_resched置为1了，所以会马上调用schedule函数找其他处于“就绪”态的进程执行。 uCore在实验四中只实现了一个最简单的FIFO调度器，其核心就是schedule函数。它的执行逻辑很简单： 1．设置当前内核线程current->need_resched为0； 2．在proc_list队列中查找下一个处于“就绪”态的线程或进程next； 3．找到这样的进程后，就调用proc_run函数，保存当前进程current的执行现场（进程上下文），恢复新进程的执行现场，完成进程切换。 至此，新的进程next就开始执行了。由于在proc10中只有两个内核线程，且idleproc要让出CPU给initproc执行，我们可以看到schedule函数通过查找proc_list进程队列，只能找到一个处于“就绪”态的initproc内核线程。并通过proc_run和进一步的switch_to函数完成两个执行现场的切换，具体流程如下： 让current指向next内核线程initproc； 设置current_pgdir的值为next内核线程initproc的页目录表起始地址next->cr3，这实际上是完成进程间的页表切换； 由switch_to函数完成具体的两个线程的执行现场切换，即切换各个寄存器，当switch_to函数执行最后一条指令时，就跳转到initproc执行了。 由于idleproc和initproc都是共用一个内核页表boot_cr3，所以此时第二步其实没用，但考虑到以后的进程有各自的页表，其起始地址各不相同，只有完成页表切换，才能确保新的进程能够正常执行。 第三步proc_run函数调用switch_to函数，参数是前一个进程和后一个进程的执行现场：process context。在上一节“设计进程控制块”中，描述了context结构包含的要保存和恢复的寄存器。我们再看看switch.S中的switch_to函数的执行流程： .text .globl switch_to switch_to: //save the registers st.w sp, a0, 48 st.w fp, a0, 44 st.w ra, a0, 40 st.w tp, a0, 36 st.w s8, a0, 32 st.w s7, a0, 28 st.w s6, a0, 24 st.w s5, a0, 20 st.w s4, a0, 16 st.w s3, a0, 12 st.w s2, a0, 8 st.w s1, a0, 4 st.w s0, a0, 0 //use as nop dbar 0 首先，保存前一个进程的执行现场，第三条汇编指令（如下所示）保存了进程在返回switch_to函数后的指令地址到context.sf_ra中 st.w ra, a0, 40 在接下来的汇编指令完成了保存前一个进程的其他10个寄存器到context中的相应成员变量中。至此前一个进程的执行现场保存完毕。再往后是恢复向一个进程的执行现场，这其实就是上述保存过程的逆执行过程，逐一把context中相关成员变量的值赋值给对应的寄存器，最后两条汇编指令就是将程序跳转到context.sf_ra保存的地址处执行。即当前进程已经是下一个进程了。uCore会执行进程切换，让initproc执行。在对initproc进行初始化时，设置了initproc->context.sf_era = (uintptr_t)forkret，这样，当执行switch_to函数并返回后，initproc将执行其实际上的执行入口地址forkret。而forkret会调用位于kern/trap/trapentry.S中的forkrets函数执行，具体代码如下： .global forkrets .type forkrets, @function forkrets: addi.w a0, sp, -16 b exception_return .end forkrets 可以看出，forkrets函数首先把sp指向当前进程的中断帧，再跳转到中断返回函数中开始执行中断返回相关操作。中断返回执行完之后开始执行initproc的主体了。Initprocde的主体函数很简单就是输出一段字符串，然后就返回到kernel_tread_entry函数，并进一步调用do_exit执行退出操作了。本来do_exit应该完成一些资源回收工作等，但这些不是实验四涉及的，而是由后续的实验来完成。至此，实验四中的主要工作描述完毕。 "},"lab4/lab4_5_appendix_a.html":{"url":"lab4/lab4_5_appendix_a.html","title":"附录A：【原理】进程的属性与特征解析","keywords":"","body":"附录A：实验四的参考输出如下： make qemu (THU.CST) os is loading ... Special kernel symbols: entry 0xc010002c (phys) etext 0xc010d0f7 (phys) edata 0xc012dad0 (phys) end 0xc0130e78 (phys) Kernel executable memory footprint: 196KB memory management: default_pmm_manager e820map: memory: 0009f400, [00000000, 0009f3ff], type = 1. memory: 00000c00, [0009f400, 0009ffff], type = 2. memory: 00010000, [000f0000, 000fffff], type = 2. memory: 07efd000, [00100000, 07ffcfff], type = 1. memory: 00003000, [07ffd000, 07ffffff], type = 2. memory: 00040000, [fffc0000, ffffffff], type = 2. check_alloc_page() succeeded! check_pgdir() succeeded! check_boot_pgdir() succeeded! -------------------- BEGIN -------------------- PDE(0e0) c0000000-f8000000 38000000 urw |-- PTE(38000) c0000000-f8000000 38000000 -rw PDE(001) fac00000-fb000000 00400000 -rw |-- PTE(000e0) faf00000-fafe0000 000e0000 urw |-- PTE(00001) fafeb000-fafec000 00001000 -rw --------------------- END --------------------- check_slab() succeeded! kmalloc_init() succeeded! check_vma_struct() succeeded! page fault at 0x00000100: K/W [no page found]. check_pgfault() succeeded! check_vmm() succeeded. ide 0: 10000(sectors), 'QEMU HARDDISK'. ide 1: 262144(sectors), 'QEMU HARDDISK'. SWAP: manager = fifo swap manager BEGIN check_swap: count 1, total 31944 mm->sm_priv c0130e64 in fifo_init_mm setup Page Table for vaddr 0X1000, so alloc a page setup Page Table vaddr 0~4MB OVER! set up init env for check_swap begin! page fault at 0x00001000: K/W [no page found]. page fault at 0x00002000: K/W [no page found]. page fault at 0x00003000: K/W [no page found]. page fault at 0x00004000: K/W [no page found]. set up init env for check_swap over! write Virt Page c in fifo_check_swap write Virt Page a in fifo_check_swap write Virt Page d in fifo_check_swap write Virt Page b in fifo_check_swap write Virt Page e in fifo_check_swap page fault at 0x00005000: K/W [no page found]. swap_out: i 0, store page in vaddr 0x1000 to disk swap entry 2 write Virt Page b in fifo_check_swap write Virt Page a in fifo_check_swap page fault at 0x00001000: K/W [no page found]. swap_out: i 0, store page in vaddr 0x2000 to disk swap entry 3 swap_in: load disk swap entry 2 with swap_page in vadr 0x1000 write Virt Page b in fifo_check_swap page fault at 0x00002000: K/W [no page found]. swap_out: i 0, store page in vaddr 0x3000 to disk swap entry 4 swap_in: load disk swap entry 3 with swap_page in vadr 0x2000 write Virt Page c in fifo_check_swap page fault at 0x00003000: K/W [no page found]. swap_out: i 0, store page in vaddr 0x4000 to disk swap entry 5 swap_in: load disk swap entry 4 with swap_page in vadr 0x3000 write Virt Page d in fifo_check_swap page fault at 0x00004000: K/W [no page found]. swap_out: i 0, store page in vaddr 0x5000 to disk swap entry 6 swap_in: load disk swap entry 5 with swap_page in vadr 0x4000 check_swap() succeeded! ++ setup timer interrupts this initproc, pid = 1, name = \"init\" To U: \"Hello world!!\". To U: \"en.., Bye, Bye. :)\" kernel panic at kern/process/proc.c:316: process exit!!. Welcome to the kernel debug monitor!! Type 'help' for a list of commands. K> "},"lab5.html":{"url":"lab5.html","title":"Lab 5","keywords":"","body":"实验五：用户进程管理 "},"lab5/lab5_1_goals.html":{"url":"lab5/lab5_1_goals.html","title":"实验目的","keywords":"","body":"实验目的 了解第一个用户进程创建过程 了解系统调用框架的实现机制 了解ucore如何实现系统调用sys_fork/sys_exec/sys_exit/sys_wait来进行进程管理 "},"lab5/lab5_2_lab2.html":{"url":"lab5/lab5_2_lab2.html","title":"实验内容","keywords":"","body":"实验内容 实验4完成了内核线程，但到目前为止，所有的运行都在内核态执行。实验5将创建用户进程，让用户进程在用户态执行，且在需要ucore支持时，可通过系统调用来让ucore提供服务。为此需要构造出第一个用户进程，并通过系统调用sys_fork/sys_exec/sys_exit/sys_wait来支持运行不同的应用程序，完成对用户进程的执行过程的基本管理。相关原理介绍可看附录。 "},"lab5/lab5_2_1_exercises.html":{"url":"lab5/lab5_2_1_exercises.html","title":"练习","keywords":"","body":"练习 为了实现lab5的目标，lab2提供了3个基本练习和1个扩展练习，要求完成实验报告。 注意有“LAB5”的注释，代码中所有需要完成的地方（challenge除外）都有“LAB5”和“YOUR CODE”的注释 对实验报告的要求： 基于markdown格式来完成，以文本方式为主 填写各个基本练习中要求完成的报告内容 完成实验后，请分析ucore_lab中提供的参考答案，并请在实验报告中说明你的实现与参考答案的区别 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点） 列出你认为OS原理中很重要，但在实验中没有对应上的知识点 练习1: 加载应用程序并执行（需要编码） do_execv函数调用load_icode（位于kern/process/proc.c中）来加载并解析一个处于内存中的ELF执行文件格式的应用程序，建立相应的用户内存空间来放置应用程序的代码段、数据段等，且要设置好proc_struct结构中的成员变量trapframe中的内容，确保在执行此进程后，能够从应用程序设定的起始执行地址开始执行。需设置正确的trapframe内容。 请在实验报告中简要说明你的设计实现过程。 请在实验报告中描述当创建一个用户态进程并加载了应用程序后，CPU是如何让这个应用程序最终在用户态执行起来的。即这个用户态进程被ucore选择占用CPU执行（RUNNING态）到具体执行应用程序第一条指令的整个经过。 练习2: 父进程复制自己的内存空间给子进程（需要编码） 创建子进程的函数do_fork在执行中将拷贝当前进程（即父进程）的用户内存地址空间中的合法内容到新进程中（子进程），完成内存资源的复制。具体是通过copy_range函数（位于kern/mm/pmm.c中）实现的，请补充copy_range的实现，确保能够正确执行。 请在实验报告中简要说明如何设计实现”Copy on Write 机制“，给出概要设计，鼓励给出详细设计。 Copy-on-write（简称COW）的基本概念是指如果有多个使用者对一个资源A（比如内存块）进行读操作，则每个使用者只需获得一个指向同一个资源A的指针，就可以该资源了。若某使用者需要对这个资源A进行写操作，系统会对该资源进行拷贝操作，从而使得该“写操作”使用者获得一个该资源A的“私有”拷贝—资源B，可对资源B进行写操作。该“写操作”使用者对资源B的改变对于其他的使用者而言是不可见的，因为其他使用者看到的还是资源A。 练习3: 阅读分析源代码，理解进程执行 fork/exec/wait/exit 的实现，以及系统调用的实现（不需要编码） 请在实验报告中简要说明你对 fork/exec/wait/exit函数的分析。并回答如下问题： 请分析fork/exec/wait/exit在实现中是如何影响进程的执行状态的？ 请给出ucore中一个用户态进程的执行状态生命周期图（包执行状态，执行状态之间的变换关系，以及产生变换的事件或函数调用）。（字符方式画即可） 执行：make qemu -j 16。如果输出结果和编译方法中的结果一致，则基本正确。 扩展练习 Challenge ：实现 Copy on Write （COW）机制 给出实现源码,测试用例和设计报告（包括在cow情况下的各种状态转换（类似有限状态自动机）的说明）。 这个扩展练习涉及到本实验和上一个实验“虚拟内存管理”。在ucore操作系统中，当一个用户父进程创建自己的子进程时，父进程会把其申请的用户空间设置为只读，子进程可共享父进程占用的用户内存空间中的页面（这就是一个共享的资源）。当其中任何一个进程修改此用户内存空间中的某页面时，ucore会通过page fault异常获知该操作，并完成拷贝内存页面，使得两个进程都有各自的内存页面。这样一个进程所做的修改不会被另外一个进程可见了。请在ucore中实现这样的COW机制。 由于COW实现比较复杂，容易引入bug，请参考 https://dirtycow.ninja/ 看看能否在ucore的COW实现中模拟这个错误和解决方案。需要有解释。 这是一个big challenge. "},"lab5/lab5_2_2_compile.html":{"url":"lab5/lab5_2_2_compile.html","title":"编译方法","keywords":"","body":"编译方法 Makefile修改 在Makefile中取消LAB5 := -DLAB5_EX1 -DLAB5_EX2(第10行)的注释 LAB1 := -DLAB1_EX4 # -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3 LAB3 := -DLAB3_EX1 -DLAB3_EX2 LAB4 := -DLAB4_EX1 -DLAB4_EX2 LAB5 := -DLAB5_EX1 -DLAB5_EX2 # LAB6 := -DLAB6_EX2 # LAB7 := -DLAB7_EX1 #-D_SHOW_PHI # LAB8 := -DLAB8_EX1 -DLAB8_EX2 编译并运行代码的命令如下： make make qemu -j 16 补全代码后可以得到如下显示界面（仅供参考） chenyu$ make qemu -j 16 (THU.CST) os is loading ... Special kernel symbols: entry 0xA00000A0 (phys) etext 0xA0020000 (phys) edata 0xA0153EC0 (phys) end 0xA01571A0 (phys) Kernel executable memory footprint: 1245KB memory management: default_pmm_manager memory map: [A0000000, A2000000] freemem start at: A0198000 free pages: 00001E68 ## 00000020 check_alloc_page() succeeded! check_pgdir() succeeded! check_boot_pgdir() succeeded! check_slab() succeeded! kmalloc_init() succeeded! check_vma_struct() succeeded! check_pgfault() succeeded! check_vmm() succeeded. sched class: RR_scheduler proc_init succeeded kernel_execve: pid = 2, name = \"exit\". I am the parent. Forking the child... I am parent, fork a child pid 3 I am the parent, waiting now.. I am the child. waitpid 3 ok. exit pass. all user-mode processes have quit. init check memory pass. kernel panic at kern/process/proc.c:554: initproc exit. Welcome to the kernel debug monitor!! Type 'help' for a list of commands. K> "},"lab5/lab5_3_user_process.html":{"url":"lab5/lab5_3_user_process.html","title":"用户进程管理","keywords":"","body":"用户进程管理 "},"lab5/lab5_3_1_lab_steps.html":{"url":"lab5/lab5_3_1_lab_steps.html","title":"实验执行流程概述","keywords":"","body":"实验执行流程概述 到实验四为止，ucore还一直在核心态“打转”，没有到用户态执行。提供各种操作系统功能的内核线程只能在CPU核心态运行是操作系统自身的要求，操作系统就要呆在核心态，才能管理整个计算机系统。但应用程序员也需要编写各种应用软件，且要在计算机系统上运行。如果把这些应用软件都作为内核线程来执行，那系统的安全性就无法得到保证了。所以，ucore要提供用户态进程的创建和执行机制，给应用程序执行提供一个用户态运行环境。接下来我们就简要分析本实验的执行过程，以及分析用户进程的整个生命周期来阐述用户进程管理的设计与实现。 显然，由于进程的执行空间扩展到了用户态空间，且出现了创建子进程执行应用程序等与lab4有较大不同的地方，所以具体实现的不同主要集中在进程管理和内存管理部分。首先，我们从ucore的初始化部分来看，会发现初始化的总控函数kern_init没有任何变化。但这并不意味着lab4与lab5差别不大。其实kern_init调用的物理内存初始化，进程管理初始化等都有一定的变化。 在内存管理部分，与lab4最大的区别就是增加用户态虚拟内存的管理。为了管理用户态的虚拟内存，需要对页表的内容进行扩展，能够把部分物理内存映射为用户态虚拟内存。如果某进程执行过程中，CPU在用户态下执行（在CSR.CRMD中的PLV域，如果为0，表示CPU运行在特权态；如果为3，表示CPU运行在用户态。），则可以访问本进程页表描述的用户态虚拟内存，但由于权限不够，不能访问内核态虚拟内存。另一方面，不同的进程有各自的页表，所以即使不同进程的用户态虚拟地址相同，但由于页表把虚拟页映射到了不同的物理页帧，所以不同进程的虚拟内存空间是被隔离开的，相互之间无法直接访问。在用户态内存空间和内核态内核空间之间需要拷贝数据，让CPU处在内核态才能完成对用户空间的读或写，为此需要设计专门的拷贝函数（copy_from_user和copy_to_user）完成。但反之则会导致违反CPU的权限管理，导致内存访问异常。 在进程管理方面，主要涉及到的是进程控制块中与内存管理相关的部分，包括建立进程的页表和维护进程可访问空间（可能还没有建立虚实映射关系）的信息；加载一个ELF格式的程序到进程控制块管理的内存中的方法；在进程复制（fork）过程中，把父进程的内存空间拷贝到子进程内存空间的技术。另外一部分与用户态进程生命周期管理相关，包括让进程放弃CPU而睡眠等待某事件；让父进程等待子进程结束；一个进程杀死另一个进程；给进程发消息；建立进程的血缘关系链表。 当实现了上述内存管理和进程管理的需求后，接下来ucore的用户进程管理工作就比较简单了。首先，“硬”构造出第一个进程（lab4中已有描述），它是后续所有进程的祖先；然后，在proc_init函数中，通过alloc把当前ucore的执行环境转变成idle内核线程的执行现场；然后调用kernl_thread来创建第二个内核线程init_main，而init_main内核线程有创建了user_main内核线程。到此，内核线程创建完毕，应该开始用户进程的创建过程，这第一步实际上是通过user_main函数调用kernel_tread创建子进程，通过kernel_execve调用来把某一具体程序的执行内容放入内存。具体的放置方式是根据ld在此文件上的地址分配为基本原则，把程序的不同部分放到某进程的用户空间中，从而通过此进程来完成程序描述的任务。一旦执行了这一程序对应的进程，就会从内核态切换到用户态继续执行。以此类推，CPU在用户空间执行的用户进程，其地址空间不会被其他用户的进程影响，但由于系统调用（用户进程直接获得操作系统服务的唯一通道）、外设中断和异常中断的会随时产生，从而间接推动了用户进程实现用户态到到内核态的切换工作。ucore对CPU内核态与用户态的切换过程需要比较仔细地分析（这其实是实验一的扩展练习）。当进程执行结束后，需回收进程占用和没消耗完毕的设备整个过程，且为新的创建进程请求提供服务。在本实验中，当系统中存在多个进程或内核线程时，ucore采用了一种FIFO的很简单的调度方法来管理每个进程占用CPU的时间和频度等。在ucore运行过程中，由于调度、时间中断、系统调用等原因，使得进程会进行切换、创建、睡眠、等待、发消息等各种不同的操作，周而复始，生生不息。 "},"lab5/lab5_3_2_create_user_process.html":{"url":"lab5/lab5_3_2_create_user_process.html","title":"创建用户进程","keywords":"","body":"创建用户进程 在实验四中，我们已经完成了对内核线程的创建，但与用户进程的创建过程相比，创建内核线程的过程还远远不够。而这两个创建过程的差异本质上就是用户进程和内核线程的差异决定的。 1. 应用程序的组成和编译 我们首先来看一个应用程序，这里我们假定是hello应用程序，在user/hello.c中实现，代码如下： #include #include int main(void) { cprintf(\"Hello world!!.\\n\"); cprintf(\"I am process %d.\\n\", getpid()); cprintf(\"hello pass.\\n\"); return 0; } hello应用程序只是输出一些字符串，并通过系统调用sys_getpid（在getpid函数中调用）输出代表hello应用程序执行的用户进程的进程标识--pid。 首先，我们需要了解ucore操作系统如何能够找到hello应用程序。这需要分析ucore和hello是如何编译的。在本实验源码目录下执行make，可得到如下输出： …… + cc user/hello.c loongarch32-linux-gnu-gcc -c -Iuser/libs -Ikern/include -fno-builtin-fprintf -fno-builtin -nostdlib -nostdinc -g -G0 -Wa,-O0 -fno-pic -mno-shared -msoft-float -ggdb -gstabs -mlcsr user/hello.c -o obj/user/hello.o loongarch32-linux-gnu-ld -T user/libs/user.ld obj/user/hello.o --whole-archive obj/user/libuser.a -o obj/user/hello …… sed 's/$FILE/hello/g' tools/piggy.S.in > obj/user/hello.S …… # 注意，可以观察obj/user/hello.S文件，这里有使用.incbin去引入先前编译好的obj/user/hello loongarch32-linux-gnu-gcc -c obj/user/hello.S -o obj/user/hello.piggy.o loongarch32-linux-gnu-ld -nostdlib -n -G 0 -static -T tools/kernel.ld obj/init/init.o …… obj/user/hello.piggy.o …… -o obj/ucore-kernel-piggy 从中可以看出，hello应用程序不仅仅是hello.c，还包含了支持hello应用程序的用户态库： user/libs/initcode.S：所有应用程序的起始用户态执行地址“_start”，调整了gp和sp后，调用umain函数。 user/libs/umain.c：实现了umain函数，这是所有应用程序执行的第一个C函数，它将调用应用程序的main函数，并在main函数结束后调用exit函数，而exit函数最终将调用sys_exit系统调用，让操作系统回收进程资源。 user/libs/ulib.[ch]：实现了最小的C函数库，除了一些与系统调用无关的函数，其他函数是对访问系统调用的包装。 user/libs/syscall.[ch]：用户层发出系统调用的具体实现。 user/libs/stdio.c：实现cprintf函数，通过系统调用sys_putc来完成字符输出。 user/libs/panic.c：实现__panic/__warn函数，通过系统调用sys_exit完成用户进程退出。 除了这些用户态库函数实现外，还有一些libs/*.[ch]是操作系统内核和应用程序共用的函数实现。这些用户库函数其实在本质上与UNIX系统中的标准libc没有区别，只是实现得很简单，但hello应用程序的正确执行离不开这些库函数。 【注意】libs/*.[ch]、user/libs/*.[ch]、user/*.[ch]的源码中没有任何特权指令。 a00c3160 _binary_obj_user_hello_end a00125ac file_open a00018d0 strcpy a0000240 ide_device_valid a01455c0 _binary_obj_user_forktree_start a000cb30 wakeup_queue a00156f0 vfs_set_bootfs a000d12c cond_signal a0011850 sysfile_fsync a001a57c dev_init_stdout a00b4ac0 _binary_obj_user_hello_start 在make的最后一步执行了一个ld命令，把hello应用程序的执行码obj/user/hello.piggy.o连接在了ucore kernel的末尾。并观察tools/piggy.S.in文件可以发现，我们定义了.global _binary_obj_user_$FILE_start和.global _binary_obj_user_$FILE_end。这样这两个符号就会保留在最终ld生成的文件中，这样这个hello用户程序就能够和ucore内核一起被 bootloader 加载到内存里中，并且通过这两个全局变量定位hello用户程序执行码的起始位置和大小。而到了与文件系统相关的实验后，ucore会提供一个简单的文件系统，那时所有的用户程序就都不再用这种方法进行加载了，而可以用大家熟悉的文件方式进行加载了。 （注，后续的文件系统采用initrd的方式，并非位于磁盘。） 2. 用户进程的虚拟地址空间 在user/libs/user.ld描述了用户程序的用户虚拟空间的执行入口虚拟地址： SECTIONS { /* Load programs at this address: \".\" means the current address */ . = 0x10000000; 在tools/kernel.ld描述了操作系统的内核虚拟空间的起始入口虚拟地址： SECTIONS { /* Load the kernel at this address: \".\" means the current address */ . = 0xa0000000; 这样ucore把用户进程的虚拟地址空间分了两块，一块与内核线程一样，是所有用户进程都共享的内核虚拟地址空间，映射到同样的物理内存空间中，这样在物理内存中只需放置一份内核代码，使得用户进程从用户态进入核心态时，内核代码可以统一应对不同的内核程序；另外一块是用户虚拟地址空间，虽然虚拟地址范围一样，但映射到不同且没有交集的物理内存空间中。这样当ucore把用户进程的执行代码（即应用程序的执行代码）和数据（即应用程序的全局变量等）放到用户虚拟地址空间中时，确保了各个进程不会“非法”访问到其他进程的物理内存空间。 这样ucore给一个用户进程具体设定的虚拟内存空间（kern/mm/memlayout.h）如下所示： 3. 创建并执行用户进程 在确定了用户进程的执行代码和数据，以及用户进程的虚拟空间布局后，我们可以来创建用户进程了。在本实验中第一个用户进程是由第二个内核线程initproc通过把hello应用程序执行码覆盖到initproc的用户虚拟内存空间来创建的，相关代码如下所示： // kernel_execve - do SYS_exec syscall to exec a user program called by user_main kernel_thread static int kernel_execve(const char *name, unsigned char *binary, size_t size) { int ret, len = strlen(name); asm volatile( \"addi.w $a7, $zero,%1;\\n\" // syscall no. \"move $a0, %2;\\n\" \"move $a1, %3;\\n\" \"move $a2, %4;\\n\" \"move $a3, %5;\\n\" \"syscall 0;\\n\" \"move %0, $a7;\\n\" : \"=r\"(ret) : \"i\"(SYSCALL_BASE+SYS_exec), \"r\"(name), \"r\"(len), \"r\"(binary), \"r\"(size) : \"a0\", \"a1\", \"a2\", \"a3\", \"a7\" ); return ret; } #define __KERNEL_EXECVE(name, path, ...) ({ \\ const char *argv[] = {path, ##__VA_ARGS__, NULL}; \\ kprintf(\"kernel_execve: pid = %d, name = \\\"%s\\\".\\n\", \\ current->pid, name); \\ kernel_execve(name, argv); \\ }) #define KERNEL_EXECVE(x, ...) __KERNEL_EXECVE(#x, #x, ##__VA_ARGS__) …… // init_main - the second kernel thread used to create kswapd_main & user_main kernel threads static int init_main(void *arg) { #ifdef TEST KERNEL_EXECVE2(TEST, TESTSTART, TESTSIZE); #else KERNEL_EXECVE(hello); #endif panic(\"kernel_execve failed.\\n\"); return 0; } 对于上述代码，我们需要从后向前按照函数/宏的实现一个一个来分析。Initproc的执行主体是init_main函数，这个函数在缺省情况下是执行宏KERNEL_EXECVE(hello)，而这个宏最终是调用kernel_execve函数来调用SYS_exec系统调用，由于ld在链接hello应用程序执行码时定义了两全局变量： _binary_obj___user_hello_out_start：hello执行码的起始位置 _binary_obj___user_hello_out_size中：hello执行码的大小 kernel_execve把这两个变量作为SYS_exec系统调用的参数，让ucore来创建此用户进程。当ucore收到此系统调用后，将依次调用如下函数 vector128(vectors.S)--\\> \\_\\_alltraps(trapentry.S)--\\>trap(trap.c)--\\>trap\\_dispatch(trap.c)-- --\\>syscall(syscall.c)--\\>sys\\_exec（syscall.c）--\\>do\\_execve(proc.c) 最终通过do_execve函数来完成用户进程的创建工作。此函数的主要工作流程如下： 首先为加载新的执行码做好用户态内存空间清空准备。如果mm不为NULL，则设置页表为内核空间页表，且进一步判断mm的引用计数减1后是否为0，如果为0，则表明没有进程再需要此进程所占用的内存空间，为此将根据mm中的记录，释放进程所占用户空间内存和进程页表本身所占空间。最后把当前进程的mm内存管理指针为空。由于此处的initproc是内核线程，所以mm为NULL，整个处理都不会做。 接下来的一步是加载应用程序执行码到当前进程的新创建的用户态虚拟空间中。这里涉及到读ELF格式的文件，申请内存空间，建立用户态虚存空间，加载应用程序执行码等。load_icode函数完成了整个复杂的工作。 load_icode函数的主要工作就是给用户进程建立一个能够让用户进程正常运行的用户环境。此函数有一百多行，完成了如下重要工作： 调用mm_create函数来申请进程的内存管理数据结构mm所需内存空间，并对mm进行初始化； 调用setup_pgdir来申请一个页目录表所需的一个页大小的内存空间，并把描述ucore内核虚空间映射的内核页表（boot_pgdir所指）的内容拷贝到此新目录表中，最后让mm->pgdir指向此页目录表，这就是进程新的页目录表了，且能够正确映射内核虚空间； 根据应用程序执行码的起始位置来解析此ELF格式的执行程序，并调用mm_map函数根据ELF格式的执行程序说明的各个段（代码段、数据段、BSS段等）的起始位置和大小建立对应的vma结构，并把vma插入到mm结构中，从而表明了用户进程的合法用户态虚拟地址空间； 调用根据执行程序各个段的大小分配物理内存空间，并根据执行程序各个段的起始位置确定虚拟地址，并在页表中建立好物理地址和虚拟地址的映射关系，然后把执行程序各个段的内容拷贝到相应的内核虚拟地址中，至此应用程序执行码和数据已经根据编译时设定地址放置到虚拟内存中了； 需要给用户进程设置用户栈，为此调用mm_mmap函数建立用户栈的vma结构，明确用户栈的位置在用户虚空间的顶端，大小为256个页，即1MB，并分配一定数量的物理内存且建立好栈的虚地址物理地址映射关系； 至此,进程内的内存管理vma和mm数据结构已经建立完成，于是把mm->pgdir赋值到变量current_pgdir中，即更新了用户进程的虚拟内存空间，此时的initproc已经被hello的代码和数据覆盖，成为了第一个用户进程，但此时这个用户进程的执行现场还没建立好； 先清空进程的中断帧，再重新设置进程的中断帧，使得在执行中断返回指令后，能够让CPU转到用户态特权级，并回到用户态内存空间，且能够跳转到用户进程的第一条指令执行，并确保在用户态能够响应中断； 至此，用户进程的用户环境已经搭建完毕。此时initproc将按产生系统调用的函数调用路径原路返回，执行中断返回指令“ertn”（位于exception.S的最后一句）后，将切换到用户进程hello的第一条语句位置_start处（位于user/libs/initcode.S的第三句）开始执行。 "},"lab5/lab5_3_3_process_exit_wait.html":{"url":"lab5/lab5_3_3_process_exit_wait.html","title":"进程退出和等待进程","keywords":"","body":"进程退出和等待进程 当进程执行完它的工作后，就需要执行退出操作，释放进程占用的资源。ucore分了两步来完成这个工作，首先由进程本身完成大部分资源的占用内存回收工作，然后由此进程的父进程完成剩余资源占用内存的回收工作。为何不让进程本身完成所有的资源回收工作呢？这是因为进程要执行回收操作，就表明此进程还存在，还在执行指令，这就需要内核栈的空间不能释放，且表示进程存在的进程控制块不能释放。所以需要父进程来帮忙释放子进程无法完成的这两个资源回收工作。 为此在用户态的函数库中提供了exit函数，此函数最终访问sys_exit系统调用接口让操作系统来帮助当前进程执行退出过程中的部分资源回收。我们来看看ucore是如何做进程退出工作的。 首先，exit函数会把一个退出码error_code传递给ucore，ucore通过执行内核函数do_exit来完成对当前进程的退出处理，主要工作简单地说就是回收当前进程所占的大部分内存资源，并通知父进程完成最后的回收工作，具体流程如下： 1. 如果current->mm != NULL，表示是用户进程，则开始回收此用户进程所占用的用户态虚拟内存空间； a) 首先执行“lcr3(boot_cr3)”，切换到内核态的页表上，这样当前用户进程目前只能在内核虚拟地址空间执行了，这是为了确保后续释放用户态内存和进程页表的工作能够正常执行； b) 如果当前进程控制块的成员变量mm的成员变量mm_count减1后为0（表明这个mm没有再被其他进程共享，可以彻底释放进程所占的用户虚拟空间了。），则开始回收用户进程所占的内存资源： i. 调用exit_mmap函数释放current->mm->vma链表中每个vma描述的进程合法空间中实际分配的内存，然后把对应的页表项内容清空，最后还把页表所占用的空间释放并把对应的页目录表项清空； ii. 调用put_pgdir函数释放当前进程的页目录所占的内存； iii. 调用mm_destroy函数释放mm中的vma所占内存，最后释放mm所占内存； c) 此时设置current->mm为NULL，表示与当前进程相关的用户虚拟内存空间和对应的内存管理成员变量所占的内核虚拟内存空间已经回收完毕； 2. 这时，设置当前进程的执行状态current->state=PROC_ZOMBIE，当前进程的退出码current->exit_code=error_code。此时当前进程已经不能被调度了，需要此进程的父进程来做最后的回收工作（即回收描述此进程的内核栈和进程控制块）； 3. 如果当前进程的父进程current->parent处于等待子进程状态： current->parent->wait_state==WT_CHILD， 则唤醒父进程（即执行“wakup_proc(current->parent)”），让父进程帮助自己完成最后的资源回收； 4. 如果当前进程还有子进程，则需要把这些子进程的父进程指针设置为内核线程initproc，且各个子进程指针需要插入到initproc的子进程链表中。如果某个子进程的执行状态是PROC_ZOMBIE，则需要唤醒initproc来完成对此子进程的最后回收工作。 5. 执行schedule()函数，选择新的进程执行。 那么父进程如何完成对子进程的最后回收工作呢？这要求父进程要执行wait用户函数或wait_pid用户函数，这两个函数的区别是，wait函数等待任意子进程的结束通知，而wait_pid函数等待进程id号为pid的子进程结束通知。这两个函数最终访问sys_wait系统调用接口让ucore来完成对子进程的最后回收工作，即回收子进程的内核栈和进程控制块所占内存空间，具体流程如下： 1. 如果pid!=0，表示只找一个进程id号为pid的退出状态的子进程，否则找任意一个处于退出状态的子进程； 2. 如果此子进程的执行状态不为PROC_ZOMBIE，表明此子进程还没有退出，则当前进程只好设置自己的执行状态为PROC_SLEEPING，睡眠原因为WT_CHILD（即等待子进程退出），调用schedule()函数选择新的进程执行，自己睡眠等待，如果被唤醒，则重复跳回步骤1处执行； 3. 如果此子进程的执行状态为PROC_ZOMBIE，表明此子进程处于退出状态，需要当前进程（即子进程的父进程）完成对子进程的最终回收工作，即首先把子进程控制块从两个进程队列proc_list和hash_list中删除，并释放子进程的内核堆栈和进程控制块。自此，子进程才彻底地结束了它的执行过程，消除了它所占用的所有资源。 "},"lab5/lab5_3_4_syscall.html":{"url":"lab5/lab5_3_4_syscall.html","title":"系统调用实现","keywords":"","body":"系统调用实现 系统调用的英文名字是System Call。操作系统为什么需要实现系统调用呢？其实这是实现了用户进程后，自然引申出来需要实现的操作系统功能。用户进程只能在操作系统给它圈定好的“用户环境”中执行，但“用户环境”限制了用户进程能够执行的指令，即用户进程只能执行一般的指令，无法执行特权指令。如果用户进程想执行一些需要特权指令的任务，比如通过网卡发网络包等，只能让操作系统来代劳了。于是就需要一种机制来确保用户进程不能执行特权指令，但能够请操作系统“帮忙”完成需要特权指令的任务，这种机制就是系统调用。 采用系统调用机制为用户进程提供一个获得操作系统服务的统一接口层，这样一来可简化用户进程的实现，把一些共性的、繁琐的、与硬件相关、与特权指令相关的任务放到操作系统层来实现，但提供一个简洁的接口给用户进程调用；二来这层接口事先可规定好，且严格检查用户进程传递进来的参数和操作系统要返回的数据，使得让操作系统给用户进程服务的同时，保护操作系统不会被用户进程破坏。 从硬件层面上看，需要硬件能够支持在用户态的用户进程通过某种机制切换到内核态。试验一讲述中断硬件支持和软件处理过程其实就可以用来完成系统调用所需的软硬件支持。下面我们来看看如何在ucore中实现系统调用。 1. 建立系统调用的用户库准备 在操作系统中初始化好系统调用相关的中断描述符、中断处理起始地址等后，还需在用户态的应用程序中初始化好相关工作，简化应用程序访问系统调用的复杂性。为此在用户态建立了一个中间层，即简化的libc实现，在user/libs/ulib.[ch]和user/libs/syscall.[ch]中完成了对访问系统调用的封装。用户态最终的访问系统调用函数是syscall，实现如下： static inline int syscall(int num, ...) { va_list ap; va_start(ap, num); uint32_t arg[MAX_ARGS]; int i, ret; for (i = 0; i 2. 与用户进程相关的系统调用 在本实验中，与进程相关的各个系统调用属性如下所示： 系统调用名含义具体完成服务的函数 SYS_exitprocess exitdo_exit SYS_forkcreate child process, dup mm do_fork-->wakeup_proc SYS_waitwait child processdo_wait SYS_execafter fork, process execute a new programload a program and refresh the mm SYS_yieldprocess flag itself need resechedulingproc->need_sched=1, then scheduler will rescheule this process SYS_killkill processdo_kill-->proc->flags |= PF_EXITING, -->wakeup_proc-->do_wait-->do_exit SYS_getpidget the process's pid 通过这些系统调用，可方便地完成从进程/线程创建到退出的整个运行过程。 3. 系统调用的执行过程 与用户态的函数库调用执行过程相比，系统调用执行过程的有四点主要的不同： 通过“syscall”指令发起调用； 通过“ertn”指令完成调用返回； 当到达内核态后，操作系统需要严格检查系统调用传递的参数，确保不破坏整个系统的安全性； 执行系统调用可导致进程等待某事件发生，从而可引起进程切换； 下面我们以getpid系统调用的执行过程大致看看操作系统是如何完成整个执行过程的。当用户进程调用getpid函数，最终执行到“syscall”指令后，CPU根据操作系统建立的系统调用中断描述符，转入内核态，并跳转到exception13处（kern/trap/exception.S），开始了操作系统的系统调用执行过程，函数调用和返回操作的关系如下所示： exception13(exception.S)--\\> \\_\\loongarch_trap(trap.c)--\\>trap\\_dispatch(trap.c)-- --\\>syscall(syscall.c)--\\>sys\\_getpid(syscall.c)--\\>……--\\>\\_\\exception_return(exception.S) 在执行trap函数前，软件还需进一步保存执行系统调用前的执行现场，即把与用户进程继续执行所需的相关寄存器等当前内容保存到当前进程的中断帧trapframe中（注意，在创建进程是，把进程的trapframe放在给进程的内核栈分配的空间的顶部）。软件做的工作在exception.S的exception_handler函数部分： exception_handler: // Save t0 and t1 csrwr t0, LISA_CSR_KS0 csrwr t1, LISA_CSR_KS1 // Save previous stack pointer in t1 move t1, sp csrwr t1, LISA_CSR_KS2 //t1 saved the vaual of KS2,KS2 saved sp /* Warning: csrwr will bring the old csr register value into rd, not only just write rd to csr register, so you may see the rd changed. It's documented in the manual from loongarch. */ // check if user mode csrrd t0, LISA_CSR_PRMD andi t0, t0, 3 beq t0, zero, 1f /* Coming from user mode - load kernel stack into sp */ la t0, current // current pointer ld.w t0, t0, 0 // proc struct ld.w t0, t0, 12 // kstack pointer addi.w t1, zero, 1 slli.w t1, t1, 13 // KSTACKSIZE=8192=pow(2,13) add.w sp, t0, t1 csrrd t1, LISA_CSR_KS2 1: //saved EXST to t0 for save EXST to sp later(line 114) csrrd t0, LISA_CSR_EXST //return KS2 csrrd t1, LISA_CSR_KS2 b common_exception common_exception: /* * At this point: * Interrupts are off. (The processor did this for us.) * t0 contains the exception status(like exception cause on MIPS). * t1 contains the old stack pointer. * sp points into the kernel stack. * All other registers are untouched. */ /* * Allocate stack space for 35 words to hold the trap frame, * plus four more words for a minimal argument block. */ addi.w sp, sp, -156 st.w s8, sp, 148 st.w s7, sp, 144 st.w s6, sp, 140 st.w s5, sp, 136 st.w s4, sp, 132 st.w s3, sp, 128 st.w s2, sp, 124 st.w s1, sp, 120 st.w s0, sp, 116 st.w fp, sp, 112 st.w reserved_reg, sp, 108 st.w t8, sp, 104 st.w t7, sp, 100 st.w t6, sp, 96 st.w t5, sp, 92 st.w t4, sp, 88 st.w t3, sp, 84 st.w t2, sp, 80 //st.w t1, sp, 76 //st.w t0, sp, 72 st.w a7, sp, 68 st.w a6, sp, 64 st.w a5, sp, 60 st.w a4, sp, 56 st.w a3, sp, 52 st.w a2, sp, 48 st.w a1, sp, 44 st.w a0, sp, 40 st.w t1, sp, 36 // replace sp with real sp, now use t1 for free st.w tp, sp, 32 // save real t0 and t1 after real sp (stored in t1 previously) stored csrrd t1, LISA_CSR_KS1 st.w t1, sp, 76 csrrd t1, LISA_CSR_KS0 st.w t1, sp, 72 // replace with real value // save tf_era after t0 and t1 saved csrrd t1, LISA_CSR_EPC st.w t1, sp, 152 /* * Save remaining exception context information. */ // save ra (note: not in pushregs, it's tf_ra) st.w ra, sp, 28 // save prmd csrrd t1, LISA_CSR_PRMD st.w t1, sp, 24 // save estat st.w t0, sp, 20 // now use t0 for free // store badv csrrd t0, LISA_CSR_BADV st.w t0, sp, 16 st.w zero, sp, 12 // support nested interrupt // IE and PLV will automatically set to 0 when trap occur // set trapframe as function argument addi.w a0, sp, 16 li t0, 0xb0 # PLV=0, IE=0, PG=1 csrwr t0, LISA_CSR_CRMD la.abs t0, loongarch_trap jirl ra, t0, 0 //bl loongarch_trap …… 自此，用于保存用户态的用户进程执行现场的trapframe的内容填写完毕，操作系统可开始完成具体的系统调用服务。在sys_getpid函数中，简单地把当前进程的pid成员变量做为函数返回值就是一个具体的系统调用服务。完成服务后，操作系统按调用关系的路径原路返回到__exception.S中。然后操作系统开始根据当前进程的中断帧内容做恢复执行现场操作。其实就是把trapframe的一部分内容保存到寄存器内容。这是内核栈的结构如下： exception_return: // restore prmd ld.w t0, sp, 24 li t1, 7 csrxchg t0, t1, LISA_CSR_PRMD // restore era no k0 and k1 for la32, so must do first ld.w t0, sp, 152 csrwr t0, LISA_CSR_EPC // restore general registers ld.w ra, sp, 28 ld.w tp, sp, 32 //ld.w sp, sp, 36 (do it finally) ld.w a0, sp, 40 ld.w a1, sp, 44 ld.w a2, sp, 48 ld.w a3, sp, 52 ld.w a4, sp, 56 ld.w a5, sp, 60 ld.w a6, sp, 64 ld.w a7, sp, 68 ld.w t0, sp, 72 ld.w t1, sp, 76 ld.w t2, sp, 80 ld.w t3, sp, 84 ld.w t4, sp, 88 ld.w t5, sp, 92 ld.w t6, sp, 96 ld.w t7, sp, 100 ld.w t8, sp, 104 ld.w reserved_reg, sp, 108 ld.w fp, sp, 112 ld.w s0, sp, 116 ld.w s1, sp, 120 ld.w s2, sp, 124 ld.w s3, sp, 128 ld.w s4, sp, 132 ld.w s5, sp, 136 ld.w s6, sp, 140 ld.w s7, sp, 144 ld.w s8, sp, 148 // restore sp ld.w sp, sp, 36 ertn .end exception_return .end common_exception 这时执行“ertn”指令后，CPU根据内核栈的情况回复到用户态，即“syscall”后的那条指令。这样整个系统调用就执行完毕了。 至此，实验五中的主要工作描述完毕。 "},"lab5/lab5_5_appendix.html":{"url":"lab5/lab5_5_appendix.html","title":"附录 A：【原理】用户进程的特征","keywords":"","body":"附录 A：【原理】用户进程的特征 从内核线程到用户进程 在实验四中设计实现了进程控制块，并实现了内核线程的创建和简单的调度执行。但实验四中没有在用户态执行用户进程的管理机制，既无法体现用户进程的地址空间，以及用户进程间地址空间隔离的保护机制，不支持进程执行过程的用户态和核心态之间的切换，且没有用户进程的完整状态变化的生命周期。其实没有实现的原因是内核线程不需要这些功能。那内核线程相对于用户态线程有何特点呢？ 但其实我们已经在实验四中看到了内核线程，内核线程的管理实现相对是简单的，其特点是直接使用操作系统（比如ucore）在初始化中建立的内核虚拟内存地址空间，不同的内核线程之间可以通过调度器实现线程间的切换，达到分时使用CPU的目的。由于内核虚拟内存空间是一一映射计算机系统的物理空间的，这使得可用空间的大小不会超过物理空间大小，所以操作系统程序员编写内核线程时，需要考虑到有限的地址空间，需要保证各个内核线程在执行过程中不会破坏操作系统的正常运行。这样在实现内核线程管理时，不必考虑涉及与进程相关的虚拟内存管理中的缺页处理、按需分页、写时复制、页换入换出等功能。如果在内核线程执行过程中出现了访存错误异常或内存不够的情况，就认为操作系统出现错误了，操作系统将直接宕机。在ucore中，就是调用panic函数，进入内核调试监控器kernel_debug_monitor。 内核线程管理思想相对简单，但编写内核线程对程序员的要求很高。从理论上讲（理想情况），如果程序员都是能够编写操作系统级别的“高手”，能够勤俭和高效地使用计算机系统中的资源，且这些“高手”都为他人着想，具有奉献精神，在别的应用需要计算机资源的时候，能够从大局出发，从整个系统的执行效率出发，让出自己占用的资源，那这些“高手”编写出来的程序直接作为内核线程运行即可，也就没有用户进程存在的必要了。 但现实与理论的差距是巨大的，能编写操作系统的程序员是极少数的，与当前的应用程序员相比，估计大约差了3~4个数量级。如果还要求编写操作系统的程序员考虑其他未知程序员的未知需求，那这样的程序员估计可以成为是编程界的“上帝”了。 从应用程序编写和运行的角度看，既然程序员都不是“上帝”，操作系统程序员就需要给应用程序员编写的程序提供一个既“宽松”又“严格”的执行环境，让对内存大小和CPU使用时间等资源的限制没有仔细考虑的应用程序都能在操作系统中正常运行，且即使程序太可靠，也只能破坏自己，而不能破坏其他运行程序和整个系统。“严格”就是安全性保证，即应用程序执行不会破坏在内存中存在的其他应用程序和操作系统的内存空间等独占的资源；“宽松”就算是方便性支持，即提供给应用程序尽量丰富的服务功能和一个远大于物理内存空间的虚拟地址空间，使得应用程序在执行过程中不必考虑很多繁琐的细节（比如如何初始化PCI总线和外设等，如何管理物理内存等）。 让用户进程正常运行的用户环境 在操作系统原理的介绍中，一般提到进程的概念其实主要是指用户进程。从操作系统的设计和实现的角度看，其实用户进程是指一个应用程序在操作系统提供的一个用户环境中的一次执行过程。这里的重点是用户环境。用户环境有啥功能？用户环境指的是什么？ 从功能上看，操作系统提供的这个用户环境有两方面的特点。一方面与存储空间相关，即限制用户进程可以访问的物理地址空间，且让各个用户进程之间的物理内存空间访问不重叠，这样可以保证不同用户进程之间不能相互破坏各自的内存空间，利用虚拟内存的功能（页换入换出）。给用户进程提供了远大于实际物理内存空间的虚拟内存空间。 另一方面与执行指令相关，即限制用户进程可执行的指令，不能让用户进程执行特权指令（比如修改页表起始地址），从而保证用户进程无法破坏系统。但如果不能执行特权指令，则很多功能（比如访问磁盘等）无法实现，所以需要提供某种机制，让操作系统完成需要特权指令才能做的各种服务功能，给用户进程一个“服务窗口”,用户进程可以通过这个“窗口”向操作系统提出服务请求，由操作系统来帮助用户进程完成需要特权指令才能做的各种服务。另外，还要有一个“中断窗口”，让用户进程不主动放弃使用CPU时，操作系统能够通过这个“中断窗口”强制让用户进程放弃使用CPU，从而让其他用户进程有机会执行。 基于功能分析，我们就可以把这个用户环境定义为如下组成部分： 建立用户虚拟空间的页表和支持页换入换出机制的用户内存访存错误异常服务例程：提供地址隔离和超过物理空间大小的虚存空间。 应用程序执行的用户态CPU特权级：在用户态CPU特权级，应用程序只能执行一般指令，如果特权指令，结果不是无效就是产生“执行非法指令”异常； 系统调用机制：给用户进程提供“服务窗口”； 中断响应机制：给用户进程设置“中断窗口”，这样产生中断后，当前执行的用户进程将被强制打断，CPU控制权将被操作系统的中断服务例程使用。 用户态进程的执行过程分析 在这个环境下运行的进程就是用户进程。那如果用户进程由于某种原因下面进入内核态后，那在内核态执行的是什么呢？还是用户进程吗？首先分析一下用户进程这样会进入内核态呢？回顾一下lab1，就可以知道当产生外设中断、CPU执行异常（比如访存错误）、陷入（系统调用），用户进程就会切换到内核中的操作系统中来。表面上看，到内核态后，操作系统取得了CPU控制权，所以现在执行的应该是操作系统代码，由于此时CPU处于核心态特权级，所以操作系统的执行过程就就应该是内核进程了。这样理解忽略了操作系统的具体实现。如果考虑操作系统的具体实现，应该如果来理解进程呢？ 从进程控制块的角度看，如果执行了进程执行现场（上下文）的切换，就认为到另外一个进程执行了，及进程的分界点设定在执行进程切换的前后。到底切换了什么呢？其实只是切换了进程的页表和相关硬件寄存器，这些信息都保存在进程控制块中的相关域中。所以，我们可以把执行应用程序的代码一直到执行操作系统中的进程切换处为止都认为是一个应用程序的执行过程（其中有操作系统的部分代码执行过过程）即进程。因为在这个过程中，没有更换到另外一个进程控制块的进程的页表和相关硬件寄存器。 从指令执行的角度看，如果再仔细分析一下操作系统这个软件的特点并细化一下进入内核原因，就可以看出进一步进行划分。操作系统的主要功能是给上层应用提供服务，管理整个计算机系统中的资源。所以操作系统虽然是一个软件，但其实是一个基于事件的软件，这里操作系统需要响应的事件包括三类：外设中断、CPU执行异常（比如访存错误）、陷入（系统调用）。如果用户进程通过系统调用要求操作系统提供服务，那么用户进程的角度看，操作系统就是一个特殊的软件库（比如相对于用户态的libc库，操作系统可看作是内核态的libc库），完成用户进程的需求，从执行逻辑上看，是用户进程“主观”执行的一部分，即用户进程“知道”操作系统要做的事情。那么在这种情况下，进程的代码空间包括用户态的执行程序和内核态响应用户进程通过系统调用而在核心特权态执行服务请求的操作系统代码，为此这种情况下的进程的内存虚拟空间也包括两部分：用户态的虚地址空间和核心态的虚地址空间。但如果此时发生的事件是外设中断和CPU执行异常，虽然CPU控制权也转入到操作系统中的中断服务例程，但这些内核执行代码执行过程是用户进程“不知道”的，是另外一段执行逻辑。那么在这种情况下，实际上是执行了两段目标不同的执行程序，一个是代表应用程序的用户进程，一个是代表中断服务例程处理外设中断和CPU执行异常的内核线程。这个用户进程和内核线程在产生中断或异常的时候，CPU硬件就完成了它们之间的指令流切换。 用户进程的运行状态分析 用户进程在其执行过程中会存在很多种不同的执行状态，根据操作系统原理，一个用户进程一般的运行状态有五种：创建（new）态、就绪（ready）态、运行（running）态、等待（blocked）态、退出（exit）态。各个状态之间会由于发生了某事件而进行状态转换。 但在用户进程的执行过程中，具体在哪个时间段处于上述状态的呢？上述状态是如何转变的呢？首先，我们看创建（new）态，操作系统完成进程的创建工作，而体现进程存在的就是进程控制块，所以一旦操作系统创建了进程控制块，则可以认为此时进程就已经存在了，但由于进程能够运行的各种资源还没准备好，所以此时的进程处于创建（new）态。创建了进程控制块后，进程并不能就执行了，还需准备好各种资源，如果把进程执行所需要的虚拟内存空间，执行代码，要处理的数据等都准备好了，则此时进程已经可以执行了，但还没有被操作系统调度，需要等待操作系统选择这个进程执行，于是把这个做好“执行准备”的进程放入到一个队列中，并可以认为此时进程处于就绪（ready）态。当操作系统的调度器从就绪进程队列中选择了一个就绪进程后，通过执行进程切换，就让这个被选上的就绪进程执行了，此时进程就处于运行（running）态了。到了运行态后，会出现三种事件。如果进程需要等待某个事件（比如主动睡眠10秒钟，或进程访问某个内存空间，但此内存空间被换出到硬盘swap分区中了，进程不得不等待操作系统把缓慢的硬盘上的数据重新读回到内存中），那么操作系统会把CPU给其他进程执行，并把进程状态从运行（running）态转换为等待（blocked）态。如果用户进程的应用程序逻辑流程执行结束了，那么操作系统会把CPU给其他进程执行，并把进程状态从运行（running）态转换为退出（exit）态，并准备回收用户进程占用的各种资源，当把表示整个进程存在的进程控制块也回收了，这进程就不存在了。在这整个回收过程中，进程都处于退出（exit）态。2考虑到在内存中存在多个处于就绪态的用户进程，但只有一个CPU，所以为了公平起见，每个就绪态进程都只有有限的时间片段，当一个运行态的进程用完了它的时间片段后，操作系统会剥夺此进程的CPU使用权，并把此进程状态从运行（running）态转换为就绪（ready）态，最后把CPU给其他进程执行。如果某个处于等待（blocked）态的进程所等待的事件产生了（比如睡眠时间到，或需要访问的数据已经从硬盘换入到内存中），则操作系统会通过把等待此事件的进程状态从等待（blocked）态转到就绪（ready）态。这样进程的整个状态转换形成了一个有限状态自动机。 "},"lab6.html":{"url":"lab6.html","title":"Lab 6","keywords":"","body":"实验六: 调度器 "},"lab6/lab6_1_goals.html":{"url":"lab6/lab6_1_goals.html","title":"实验目的","keywords":"","body":"实验目的 理解操作系统的调度管理机制 熟悉 ucore 的系统调度器框架，以及缺省的Round-Robin 调度算法 基于调度器框架实现一个(Stride Scheduling)调度算法来替换缺省的调度算法 "},"lab6/lab6_2_labs.html":{"url":"lab6/lab6_2_labs.html","title":"实验内容","keywords":"","body":"实验内容 实验五完成了用户进程的管理，可在用户态运行多个进程。但到目前为止，采用的调度策略是很简单的FIFO调度策略。本次实验，主要是熟悉ucore的系统调度器框架，以及基于此框架的Round-Robin（RR） 调度算法。然后参考RR调度算法的实现，完成Stride Scheduling调度算法。 "},"lab6/lab6_2_1_exercises.html":{"url":"lab6/lab6_2_1_exercises.html","title":"练习","keywords":"","body":"练习 为了实现lab6的目标，lab2提供了2个基本练习和2个扩展练习，要求完成实验报告。 注意有“LAB6”的注释，代码中所有需要完成的地方（challenge除外）都有“LAB6”和“YOUR CODE”的注释 对实验报告的要求： 基于markdown格式来完成，以文本方式为主 填写各个基本练习中要求完成的报告内容 完成实验后，请分析ucore_lab中提供的参考答案，并请在实验报告中说明你的实现与参考答案的区别 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点） 列出你认为OS原理中很重要，但在实验中没有对应上的知识点 练习1: 使用 Round Robin 调度算法（不需要编码） 请在实验报告中完成： 请理解并分析sched_class中各个函数指针的用法，并结合Round Robin 调度算法描ucore的调度执行过程 请在实验报告中简要说明如何设计实现”多级反馈队列调度算法“，给出概要设计，鼓励给出详细设计 练习2: 实现 Stride Scheduling 调度算法（需要编码） 首先需要换掉RR调度器的实现，即用default_sched_stride_c覆盖default_sched.c。然后根据此文件和后续文档对Stride度器的相关描述，完成Stride调度算法的实现。 后面的实验文档部分给出了Stride调度算法的大体描述。这里给出Stride调度算法的一些相关的资料（目前网上中文的资料比较欠缺）。 strid-shed paper location1 strid-shed paper location2 也可GOOGLE “Stride Scheduling” 来查找相关资料 完成代码编写后，编译并运行代码：make qemu -j 16 如果可以得到如 编译方法所示的显示内容（仅供参考，不是标准答案输出），则基本正确。 请在实验报告中简要说明你的设计实现过程。 扩展练习 Challenge 1 ：实现 Linux 的 CFS 调度算法 在ucore的调度器框架下实现下Linux的CFS调度算法。可阅读相关Linux内核书籍或查询网上资料，可了解CFS的细节，然后大致实现在ucore中。 扩展练习 Challenge 2 ：在ucore上实现尽可能多的各种基本调度算法(FIFO, SJF,...)，并设计各种测试用例，能够定量地分析出各种调度算法在各种指标上的差异，说明调度算法的适用范围。 "},"lab6/lab6_2_2_compile.html":{"url":"lab6/lab6_2_2_compile.html","title":"编译方法","keywords":"","body":"编译方法 Makefile修改 在Makefile中取消LAB6 := -DLAB6_EX2(第11行)的注释 LAB1 := -DLAB1_EX4 # -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3 LAB3 := -DLAB3_EX1 -DLAB3_EX2 LAB4 := -DLAB4_EX1 -DLAB4_EX2 LAB5 := -DLAB5_EX1 -DLAB5_EX2 LAB6 := -DLAB6_EX2 # LAB7 := -DLAB7_EX1 #-D_SHOW_PHI # LAB8 := -DLAB8_EX1 -DLAB8_EX2 编译并运行代码的命令如下： make make qemu -j 16 补全代码后可以得到如下显示界面（仅供参考） chenyu$ make qemu -j 16 (THU.CST) os is loading ... Special kernel symbols: entry 0xA00000A0 (phys) etext 0xA0020000 (phys) edata 0xA0153EC0 (phys) end 0xA01571A0 (phys) Kernel executable memory footprint: 1245KB memory management: default_pmm_manager memory map: [A0000000, A2000000] freemem start at: A0198000 free pages: 00001E68 ## 00000020 check_alloc_page() succeeded! check_pgdir() succeeded! check_boot_pgdir() succeeded! check_slab() succeeded! kmalloc_init() succeeded! check_vma_struct() succeeded! check_pgfault() succeeded! check_vmm() succeeded. sched class: RR_scheduler proc_init succeeded kernel_execve: pid = 2, name = \"exit\". I am the parent. Forking the child... I am parent, fork a child pid 3 I am the parent, waiting now.. I am the child. waitpid 3 ok. exit pass. all user-mode processes have quit. init check memory pass. kernel panic at kern/process/proc.c:554: initproc exit. Welcome to the kernel debug monitor!! Type 'help' for a list of commands. K> 实验六输出与实验五相同，如实验六补全的代码存在问题，则不能得到与实验五相同的输出。 "},"lab6/lab6_3_scheduler_design.html":{"url":"lab6/lab6_3_scheduler_design.html","title":"调度框架和调度算法设计与实现","keywords":"","body":"调度框架和调度算法设计与实现 "},"lab6/lab6_3_1_exercises.html":{"url":"lab6/lab6_3_1_exercises.html","title":"实验执行流程概述","keywords":"","body":"实验执行流程概述 在实验五，创建了用户进程，并让它们正确运行。这中间也实现了FIFO调度策略。可通过阅读实验五下的 kern/schedule/sched.c 的 schedule 函数的实现来了解其FIFO调度策略。与实验五相比，实验六专门需要针对处理器调度框架和各种算法进行设计与实现，为此对ucore的调度部分进行了适当的修改，使得kern/schedule/sched.c 只实现调度器框架，而不再涉及具体的调度算法实现。而调度算法在单独的文件（default_sched.[ch]）中实现。 除此之外，实验中还涉及了idle进程的概念。当cpu没有进程可以执行的时候，系统应该如何工作？在实验五的scheduler实现中，ucore内核不断的遍历进程池，直到找到第一个runnable状态的 process，调用并执行它。也就是说，当系统没有进程可以执行的时候，它会把所有 cpu 时间用在搜索进程池，以实现 idle的目的。但是这样的设计不被大多数操作系统所采用，原因在于它将进程调度和 idle 进程两种不同的概念混在了一起，而且，当调度器比较复杂时，schedule 函数本身也会比较复杂，这样的设计结构很不清晰而且难免会出现错误。所以在此次实验中，ucore建立了一个单独的进程(kern/process/proc.c 中的 idleproc)作为 cpu 空闲时的 idle 进程，这个程序是通常一个死循环。你需要了解这个程序的实现。 接下来可看看实验六的大致执行过程，在init.c中的kern_init函数增加了对sched_init函数的调用。sched_init函数主要完成了对实现特定调度算法的调度类（sched_class）的绑定，使得ucore在后续的执行中，能够通过调度框架找到实现特定调度算法的调度类并完成进程调度相关工作。为了更好地理解实验六整个运行过程，这里需要关注的重点问题包括： 何时或何事件发生后需要调度？ 何时或何事件发生后需要调整实现调度算法所涉及的参数？ 如果基于调度框架设计具体的调度算法？ 如果灵活应用链表等数据结构管理进程调度？ 大家可带着这些问题进一步阅读后续的内容。 "},"lab6/lab6_3_3_process_state.html":{"url":"lab6/lab6_3_3_process_state.html","title":"进程状态","keywords":"","body":"进程状态 在此次实验中，进程的状态之间的转换需要有一个更为清晰的表述，在 ucore中，runnable的进程会被放在运行队列中。值得注意的是，在具体实现中，ucore定义的进程控制块struct proc_struct包含了成员变量state,用于描述进程的运行状态，而running和runnable共享同一个状态(state)值(PROC_RUNNABLE。不同之处在于处于running态的进程不会放在运行队列中。进程的正常生命周期如下： 进程首先在 cpu 初始化或者 sys_fork 的时候被创建，当为该进程分配了一个进程控制块之后，该进程进入 uninit态(在proc.c 中 alloc_proc)。 当进程完全完成初始化之后，该进程转为runnable态。 当到达调度点时，由调度器 sched_class 根据运行队列rq的内容来判断一个进程是否应该被运行，即把处于runnable态的进程转换成running状态，从而占用CPU执行。 running态的进程通过wait等系统调用被阻塞，进入sleeping态。 sleeping态的进程被wakeup变成runnable态的进程。 running态的进程主动 exit 变成zombie态，然后由其父进程完成对其资源的最后释放，子进程的进程控制块成为unused。 所有从runnable态变成其他状态的进程都要出运行队列，反之，被放入某个运行队列中。 "},"lab6/lab6_3_4_process_implement.html":{"url":"lab6/lab6_3_4_process_implement.html","title":"进程调度实现","keywords":"","body":"进程调度实现 "},"lab6/lab6_3_4_1_kernel_preempt_point.html":{"url":"lab6/lab6_3_4_1_kernel_preempt_point.html","title":"内核抢占点","keywords":"","body":"内核抢占点 调度本质上体现了对CPU资源的抢占。对于用户进程而言，由于有中断的产生，可以随时打断用户进程的执行，转到操作系统内部，从而给了操作系统以调度控制权，让操作系统可以根据具体情况（比如用户进程时间片已经用完了）选择其他用户进程执行。这体现了用户进程的可抢占性（preemptive）。但如果把ucore操作系统也看成是一个特殊的内核进程或多个内核线程的集合，那ucore是否也是可抢占的呢？其实ucore内核执行是不可抢占的（non-preemptive），即在执行“任意”内核代码时，CPU控制权可被强制剥夺。这里需要注意，不是在所有情况下ucore内核执行都是不可抢占的，有以下几种“固定”情况是例外： 进行同步互斥操作，比如争抢一个信号量、锁（lab7中会详细分析）； 进行磁盘读写等耗时的异步操作，由于等待完成的耗时太长，ucore会调用shcedule让其他就绪进程执行。 这几种情况其实都是由于当前进程所需的某个资源（也可称为事件）无法得到满足，无法继续执行下去，从而不得不主动放弃对CPU的控制权。如果参照用户进程任何位置都可被内核打断并放弃CPU控制权的情况，这些在内核中放弃CPU控制权的执行地点是“固定”而不是“任意”的，不能体现内核任意位置都可抢占性的特点。我们搜寻一下实验五的代码，可发现在如下几处地方调用了shedule函数： 表一：调用进程调度函数schedule的位置和原因 编号位置原因 1proc.c::do_exit用户线程执行结束，主动放弃CPU控制权。 2proc.c::do_wait用户线程等待子进程结束，主动放弃CPU控制权。 3proc.c::init_main1. initproc内核线程等待所有用户进程结束，如果没有结束，就主动放弃CPU控制权; 2. initproc内核线程在所有用户进程结束后，让kswapd内核线程执行10次，用于回收空闲内存资源 4proc.c::cpu_idleidleproc内核线程的工作就是等待有处于就绪态的进程或线程，如果有就调用schedule函数 5sync.h::lock在获取锁的过程中，如果无法得到锁，则主动放弃CPU控制权 6trap.c::trap如果在当前进程在用户态被打断去，且当前进程控制块的成员变量need_resched设置为1，则当前线程会放弃CPU控制权 仔细分析上述位置，第1、2、5处的执行位置体现了由于获取某种资源一时等不到满足、进程要退出、进程要睡眠等原因而不得不主动放弃CPU。第3、4处的执行位置比较特殊，initproc内核线程等待用户进程结束而执行schedule函数；idle内核线程在没有进程处于就绪态时才执行，一旦有了就绪态的进程，它将执行schedule函数完成进程调度。这里只有第6处的位置比较特殊： if (!in_kernel) { …… if (current->need_resched) { schedule(); } } 这里表明了只有当进程在用户态执行到“任意”某处用户代码位置时发生了中断，且当前进程控制块成员变量need_resched为1（表示需要调度了）时，才会执行shedule函数。这实际上体现了对用户进程的可抢占性。如果没有第一行的if语句，那么就可以体现对内核代码的可抢占性。但如果要把这一行if语句去掉，我们就不得不实现对ucore中的所有全局变量的互斥访问操作，以防止所谓的racecondition现象，这样ucore的实现复杂度会增加不少。 "},"lab6/lab6_3_4_2_process_switch.html":{"url":"lab6/lab6_3_4_2_process_switch.html","title":"进程切换过程","keywords":"","body":"进程切换过程 进程调度函数schedule选择了下一个将占用CPU执行的进程后，将调用进程切换，从而让新的进程得以执行。通过实验四和实验五的理解，应该已经对进程调度和上下文切换有了初步的认识。在实验五中，结合调度器框架的设计，可对ucore中的进程切换以及堆栈的维护和使用等有更加深刻的认识。假定有两个用户进程，在二者进行进程切换的过程中，具体的步骤如下： 首先在执行某进程A的用户代码时，出现了一个 trap (例如是一个外设产生的中断)，这个时候就会从进程A的用户态切换到内核态(过程(1))，并且保存好进程A的trapframe；当内核态处理中断时发现需要进行进程切换时，ucore要通过schedule函数选择下一个将占用CPU执行的进程（即进程B），然后会调用proc_run函数，proc_run函数进一步调用switch_to函数，切换到进程B的内核态(过程(2))，继续进程B上一次在内核态的操作，并通过ertn指令，最终将执行权转交给进程B的用户空间(过程(3))。 当进程B由于某种原因发生中断之后(过程(4))，会从进程B的用户态切换到内核态，并且保存好进程B的trapframe；当内核态处理中断时发现需要进行进程切换时，即需要切换到进程A，ucore再次切换到进程A(过程(5))，会执行进程A上一次在内核调用schedule (具体还要跟踪到 switch_to 函数)函数返回后的下一行代码，这行代码当然还是在进程A的上一次中断处理流程中。最后当进程A的中断处理完毕的时候，执行权又会反交给进程A的用户代码(过程(6))。这就是在只有两个进程的情况下，进程切换间的大体流程。 几点需要强调的是： a) 需要透彻理解在进程切换以后，程序是从哪里开始执行的？需要注意到虽然指令还是同一个cpu上执行，但是此时已经是另外一个进程在执行了，且使用的资源已经完全不同了。 b) 内核在第一个程序运行的时候，需要进行哪些操作？有了实验四和实验五的经验，可以确定，内核启动第一个用户进程的过程，实际上是从进程启动时的内核状态切换到该用户进程的内核状态的过程，而且该用户进程在用户态的起始入口应该是forkrets。 "},"lab6/lab6_3_5_scheduler_framework.html":{"url":"lab6/lab6_3_5_scheduler_framework.html","title":"调度框架和调度算法","keywords":"","body":"调度框架和调度算法 "},"lab6/lab6_3_5_1_designed.html":{"url":"lab6/lab6_3_5_1_designed.html","title":"设计思路","keywords":"","body":"设计思路 实行一个进程调度策略，到底需要实现哪些基本功能对应的数据结构？首先考虑到一个无论哪种调度算法都需要选择一个就绪进程来占用CPU运行。为此我们可把就绪进程组织起来，可用队列（双向链表）、二叉树、红黑树、数组…等不同的组织方式。 在操作方面，如果需要选择一个就绪进程，就可以从基于某种组织方式的就绪进程集合中选择出一个进程执行。需要注意，这里“选择”和“出”是两个操作，选择是在集合中挑选一个“合适”的进程，“出”意味着离开就绪进程集合。另外考虑到一个处于运行态的进程还会由于某种原因（比如时间片用完了）回到就绪态而不能继续占用CPU执行，这就会重新进入到就绪进程集合中。这两种情况就形成了调度器相关的三个基本操作：在就绪进程集合中选择、进入就绪进程集合和离开就绪进程集合。这三个操作属于调度器的基本操作。 在进程的执行过程中，就绪进程的等待时间和执行进程的执行时间是影响调度选择的重要因素，这两个因素随着时间的流逝和各种事件的发生在不停地变化，比如处于就绪态的进程等待调度的时间在增长，处于运行态的进程所消耗的时间片在减少等。这些进程状态变化的情况需要及时让进程调度器知道，便于选择更合适的进程执行。所以这种进程变化的情况就形成了调度器相关的一个变化感知操作：timer时间事件感知操作。这样在进程运行或等待的过程中，调度器可以调整进程控制块中与进程调度相关的属性值（比如消耗的时间片、进程优先级等），并可能导致对进程组织形式的调整（比如以时间片大小的顺序来重排双向链表等），并最终可能导致调选择新的进程占用CPU运行。这个操作属于调度器的进程调度属性调整操作。 "},"lab6/lab6_3_5_2_data_structure.html":{"url":"lab6/lab6_3_5_2_data_structure.html","title":"数据结构","keywords":"","body":"数据结构 在理解框架之前，需要先了解一下调度器框架所需要的数据结构。 通常的操作系统中，进程池是很大的（虽然在 ucore 中，MAX_PROCESS 很小）。在 ucore 中，调度器引入 run-queue（简称rq,即运行队列）的概念，通过链表结构管理进程。 由于目前 ucore 设计运行在单CPU上，其内部只有一个全局的运行队列，用来管理系统内全部的进程。 运行队列通过链表的形式进行组织。链表的每一个节点是一个list_entry_t,每个list_entry_t 又对应到了 struct proc_struct *,这其间的转换是通过宏 le2proc 来完成 的。具体来说，我们知道在 struct proc_struct 中有一个叫 run_link 的 list_entry_t，因此可以通过偏移量逆向找到对因某个 run_list的 struct proc_struct。即进程结构指针 proc = le2proc(链表节点指针, run_link)。 为了保证调度器接口的通用性，ucore调度框架定义了如下接口，该接口中，几乎全部成员变量均为函数指针。具体的功能会在后面的框架说明中介绍。 1 struct sched_class { 2 // 调度器的名字 3 const char *name; 4 // 初始化运行队列 5 void (*init) (struct run_queue *rq); 6 // 将进程 p 插入队列 rq 7 void (*enqueue) (struct run_queue *rq, struct proc_struct *p); 8 // 将进程 p 从队列 rq 中删除 9 void (*dequeue) (struct run_queue *rq, struct proc_struct *p); 10 // 返回 运行队列 中下一个可执行的进程 11 struct proc_struct* (*pick_next) (struct run_queue *rq); 12 // timetick 处理函数 13 void (*proc_tick)(struct run_queue* rq, struct proc_struct* p); 14 }; 此外，proc.h 中的 struct proc_struct 中也记录了一些调度相关的信息： 1 struct proc_struct { 2 // . . . 3 // 该进程是否需要调度，只对当前进程有效 4 volatile bool need_resched; 5 // 该进程的调度链表结构，该结构内部的连接组成了 运行队列 列表 6 list_entry_t run_link; 7 // 该进程剩余的时间片，只对当前进程有效 8 int time_slice; 9 // round-robin 调度器并不会用到以下成员 10 // 该进程在优先队列中的节点，仅在 LAB6 使用 11 skew_heap_entry_t lab6_run_pool; 12 // 该进程的调度优先级，仅在 LAB6 使用 13 uint32_t lab6_priority; 14 // 该进程的调度步进值，仅在 LAB6 使用 15 uint32_t lab6_stride; 16 }; 在此次实验中，你需要了解 default_sched.c中的实现RR调度算法的函数。在该文件中，你可以看到ucore 已经为 RR 调度算法创建好了一个名为 RR_sched_class 的调度策略类。 通过数据结构 struct run_queue 来描述完整的 run_queue（运行队列）。它的主要结构如下： 1 struct run_queue { 2 //其运行队列的哨兵结构，可以看作是队列头和尾 3 list_entry_t run_list; 4 //优先队列形式的进程容器，只在 LAB6 中使用 5 skew_heap_entry_t *lab6_run_pool; 6 //表示其内部的进程总数 7 unsigned int proc_num; 8 //每个进程一轮占用的最多时间片 9 int max_time_slice; 10 }; 在 ucore 框架中，运行队列存储的是当前可以调度的进程，所以，只有状态为runnable的进程才能够进入运行队列。当前正在运行的进程并不会在运行队列中，这一点需要注意。 "},"lab6/lab6_3_5_3_scheduler_point_functions.html":{"url":"lab6/lab6_3_5_3_scheduler_point_functions.html","title":"调度点的相关关键函数","keywords":"","body":"调度点的相关关键函数 虽然进程各种状态变化的原因和导致的调度处理各异，但其实仔细观察各个流程的共性部分，会发现其中只涉及了三个关键调度相关函数：wakup_proc、shedule、run_timer_list。如果我们能够让这三个调度相关函数的实现与具体调度算法无关，那么就可以认为ucore实现了一个与调度算法无关的调度框架。 wakeup_proc函数其实完成了把一个就绪进程放入到就绪进程队列中的工作，为此还调用了一个调度类接口函数sched_class_enqueue，这使得wakeup_proc的实现与具体调度算法无关。schedule函数完成了与调度框架和调度算法相关三件事情:把当前继续占用CPU执行的运行进程放放入到就绪进程队列中，从就绪进程队列中选择一个“合适”就绪进程，把这个“合适”的就绪进程从就绪进程队列中摘除。通过调用三个调度类接口函数sched_class_enqueue、sched_class_pick_next、sched_class_enqueue来使得完成这三件事情与具体的调度算法无关。run_timer_list函数在每次timer中断处理过程中被调用，从而可用来调用调度算法所需的timer时间事件感知操作，调整相关进程的进程调度相关的属性值。通过调用调度类接口函数sched_class_proc_tick使得此操作与具体调度算法无关。 这里涉及了一系列调度类接口函数： sched_class_enqueue sched_class_dequeue sched_class_pick_next sched_class_proc_tick 这4个函数的实现其实就是调用某基于sched_class数据结构的特定调度算法实现的4个指针函数。采用这样的调度类框架后，如果我们需要实现一个新的调度算法，则我们需要定义一个针对此算法的调度类的实例，一个就绪进程队列的组织结构描述就行了，其他的事情都可交给调度类框架来完成。 "},"lab6/lab6_3_5_4_RR.html":{"url":"lab6/lab6_3_5_4_RR.html","title":"RR 调度算法实现","keywords":"","body":"RR 调度算法实现 RR调度算法的调度思想 是让所有runnable态的进程分时轮流使用CPU时间。RR调度器维护当前runnable进程的有序运行队列。当前进程的时间片用完之后，调度器将当前进程放置到运行队列的尾部，再从其头部取出进程进行调度。RR调度算法的就绪队列在组织结构上也是一个双向链表，只是增加了一个成员变量，表明在此就绪进程队列中的最大执行时间片。而且在进程控制块proc_struct中增加了一个成员变量time_slice，用来记录进程当前的可运行时间片段。这是由于RR调度算法需要考虑执行进程的运行时间不能太长。在每个timer到时的时候，操作系统会递减当前执行进程的time_slice，当time_slice为0时，就意味着这个进程运行了一段时间（这个时间片段称为进程的时间片），需要把CPU让给其他进程执行，于是操作系统就需要让此进程重新回到rq的队列尾，且重置此进程的时间片为就绪队列的成员变量最大时间片max_time_slice值，然后再从rq的队列头取出一个新的进程执行。下面来分析一下其调度算法的实现。 RR_enqueue的函数实现如下表所示。即把某进程的进程控制块指针放入到rq队列末尾，且如果进程控制块的时间片为0，则需要把它重置为rq成员变量max_time_slice。这表示如果进程在当前的执行时间片已经用完，需要等到下一次有机会运行时，才能再执行一段时间。 static void RR_enqueue(struct run_queue *rq, struct proc_struct *proc) { assert(list_empty(&(proc->run_link))); list_add_before(&(rq->run_list), &(proc->run_link)); if (proc->time_slice == 0 || proc->time_slice > rq->max_time_slice) { proc->time_slice = rq->max_time_slice; } proc->rq = rq; rq->proc_num ++; } RR_pick_next的函数实现如下表所示。即选取就绪进程队列rq中的队头队列元素，并把队列元素转换成进程控制块指针。 static struct proc_struct * RR_pick_next(struct run_queue *rq) { list_entry_t *le = list_next(&(rq->run_list)); if (le != &(rq->run_list)) { return le2proc(le, run_link); } return NULL; } RR_dequeue的函数实现如下表所示。即把就绪进程队列rq的进程控制块指针的队列元素删除，并把表示就绪进程个数的proc_num减一。 static void RR_dequeue(struct run_queue *rq, struct proc_struct *proc) { assert(!list_empty(&(proc->run_link)) && proc->rq == rq); list_del_init(&(proc->run_link)); rq->proc_num --; } RR_proc_tick的函数实现如下表所示。即每次timer到时后，trap函数将会间接调用此函数来把当前执行进程的时间片time_slice减一。如果time_slice降到零，则设置此进程成员变量need_resched标识为1，这样在下一次中断来后执行trap函数时，会由于当前进程程成员变量need_resched标识为1而执行schedule函数，从而把当前执行进程放回就绪队列末尾，而从就绪队列头取出在就绪队列上等待时间最久的那个就绪进程执行。 static void RR_proc_tick(struct run_queue *rq, struct proc_struct *proc) { if (proc->time_slice > 0) { proc->time_slice --; } if (proc->time_slice == 0) { proc->need_resched = 1; } } "},"lab6/lab6_3_6_stride_scheduling.html":{"url":"lab6/lab6_3_6_stride_scheduling.html","title":"Stride Scheduling","keywords":"","body":"Stride Scheduling "},"lab6/lab6_3_6_1_basic_method.html":{"url":"lab6/lab6_3_6_1_basic_method.html","title":"基本思路","keywords":"","body":"基本思路 【提示】请先看练习2中提到的论文, 理解后在看下面的内容。 考察 round-robin 调度器，在假设所有进程都充分使用了其拥有的 CPU 时间资源的情况下，所有进程得到的 CPU 时间应该是相等的。但是有时候我们希望调度器能够更智能地为每个进程分配合理的 CPU 资源。假设我们为不同的进程分配不同的优先级，则我们有可能希望每个进程得到的时间资源与他们的优先级成正比关系。Stride调度是基于这种想法的一个较为典型和简单的算法。除了简单易于实现以外，它还有如下的特点： 可控性：如我们之前所希望的，可以证明 Stride Scheduling对进程的调度次数正比于其优先级。 确定性：在不考虑计时器事件的情况下，整个调度机制都是可预知和重现的。该算法的基本思想可以考虑如下： 为每个runnable的进程设置一个当前状态stride，表示该进程当前的调度权。另外定义其对应的pass值，表示对应进程在调度后，stride 需要进行的累加值。 每次需要调度时，从当前 runnable 态的进程中选择 stride最小的进程调度。 对于获得调度的进程P，将对应的stride加上其对应的步长pass（只与进程的优先权有关系）。 在一段固定的时间之后，回到 2.步骤，重新调度当前stride最小的进程。可以证明，如果令 P.pass =BigStride / P.priority 其中 P.priority 表示进程的优先权（大于 1），而 BigStride 表示一个预先定义的大常数，则该调度方案为每个进程分配的时间将与其优先级成正比。证明过程我们在这里略去，有兴趣的同学可以在网上查找相关资料。将该调度器应用到 ucore 的调度器框架中来，则需要将调度器接口实现如下： init:– 初始化调度器类的信息（如果有的话）。– 初始化当前的运行队列为一个空的容器结构。（比如和RR调度算法一样，初始化为一个有序列表） enqueue– 初始化刚进入运行队列的进程 proc的stride属性。– 将 proc插入放入运行队列中去（注意：这里并不要求放置在队列头部）。 dequeue– 从运行队列中删除相应的元素。 pick next– 扫描整个运行队列，返回其中stride值最小的对应进程。– 更新对应进程的stride值，即pass = BIG_STRIDE / P->priority; P->stride += pass。 proc tick:– 检测当前进程是否已用完分配的时间片。如果时间片用完，应该正确设置进程结构的相关标记来引起进程切换。– 一个 process 最多可以连续运行 rq.max_time_slice个时间片。 在具体实现时，有一个需要注意的地方：stride属性的溢出问题，在之前的实现里面我们并没有考虑 stride 的数值范围，而这个值在理论上是不断增加的，在 stride溢出以后，基于stride的比较可能会出现错误。比如假设当前存在两个进程A和B，stride属性采用16位无符号整数进行存储。当前队列中元素如下（假设当前运行的进程已经被重新放置进运行队列中）： 此时应该选择 A 作为调度的进程，而在一轮调度后，队列将如下： 可以看到由于溢出的出现，进程间stride的理论比较和实际比较结果出现了偏差。我们首先在理论上分析这个问题：令PASS_MAX为当前所有进程里最大的步进值。则我们可以证明如下结论：对每次Stride调度器的调度步骤中，有其最大的步进值STRIDE_MAX和最小的步进值STRIDE_MIN之差： STRIDE_MAX – STRIDE_MIN 提问 1：如何证明该结论？ 有了该结论，在加上之前对优先级有Priority > 1限制，我们有STRIDE_MAX – STRIDE_MIN 65535。基于这种特殊考虑的比较方法，即便Stride有可能溢出，我们仍能够得到理论上的当前最小Stride，并做出正确的调度决定。 提问 2：在 ucore 中，目前Stride是采用无符号的32位整数表示。则BigStride应该取多少，才能保证比较的正确性？ "},"lab6/lab6_3_6_2_priority_queue.html":{"url":"lab6/lab6_3_6_2_priority_queue.html","title":"使用优先队列实现 Stride Scheduling","keywords":"","body":"使用优先队列实现 Stride Scheduling 在上述的实现描述中，对于每一次pick_next函数，我们都需要完整地扫描来获得当前最小的stride及其进程。这在进程非常多的时候是非常耗时和低效的，有兴趣的同学可以在实现了基于列表扫描的Stride调度器之后比较一下priority程序在Round-Robin及Stride调度器下各自的运行时间。考虑到其调度选择于优先队列的抽象逻辑一致，我们考虑使用优化的优先队列数据结构实现该调度。 优先队列是这样一种数据结构：使用者可以快速的插入和删除队列中的元素，并且在预先指定的顺序下快速取得当前在队列中的最小（或者最大）值及其对应元素。可以看到，这样的数据结构非常符合 Stride 调度器的实现。 本次实验提供了libs/skew_heap.h 作为优先队列的一个实现，该实现定义相关的结构和接口，其中主要包括： 1 // 优先队列节点的结构 2 typedef struct skew_heap_entry skew_heap_entry_t; 3 // 初始化一个队列节点 4 void skew_heap_init(skew_heap_entry_t *a); 5 // 将节点 b 插入至以节点 a 为队列头的队列中去，返回插入后的队列 6 skew_heap_entry_t *skew_heap_insert(skew_heap_entry_t *a, 7 skew_heap_entry_t *b, 8 compare_f comp); 9 // 将节点 b 插入从以节点 a 为队列头的队列中去，返回删除后的队列 10 skew_heap_entry_t *skew_heap_remove(skew_heap_entry_t *a, 11 skew_heap_entry_t *b, 12 compare_f comp); 其中优先队列的顺序是由比较函数comp决定的，sched_stride.c中提供了proc_stride_comp_f比较器用来比较两个stride的大小，你可以直接使用它。当使用优先队列作为Stride调度器的实现方式之后，运行队列结构也需要作相关改变，其中包括： struct run_queue中的lab6_run_pool指针，在使用优先队列的实现中表示当前优先队列的头元素，如果优先队列为空，则其指向空指针（NULL）。 struct proc_struct中的lab6_run_pool结构，表示当前进程对应的优先队列节点。本次实验已经修改了系统相关部分的代码，使得其能够很好地适应LAB6新加入的数据结构和接口。而在实验中我们需要做的是用优先队列实现一个正确和高效的Stride调度器，如果用较简略的伪代码描述，则有： init(rq):– Initialize rq->run_list– Set rq->lab6_run_pool to NULL– Set rq->proc_num to 0 enqueue(rq, proc)– Initialize proc->time_slice– Insert proc->lab6_run_pool into rq->lab6_run_pool– rq->proc_num ++ dequeue(rq, proc)– Remove proc->lab6_run_pool from rq->lab6_run_pool– rq->proc_num -- pick_next(rq)– If rq->lab6_run_pool == NULL, return NULL– Find the proc corresponding to the pointer rq->lab6_run_pool– proc->lab6_stride += BIG_STRIDE / proc->lab6_priority– Return proc proc_tick(rq, proc):– If proc->time_slice > 0, proc->time_slice --– If proc->time_slice == 0, set the flag proc->need_resched "},"lab7.html":{"url":"lab7.html","title":"Lab 7","keywords":"","body":"实验七：同步互斥 "},"lab7/lab7_1_goals.html":{"url":"lab7/lab7_1_goals.html","title":"实验目的","keywords":"","body":"实验目的 理解操作系统的同步互斥的设计实现； 理解底层支撑技术：禁用中断、定时器、等待队列； 在ucore中理解信号量（semaphore）机制的具体实现； 理解管程机制，在ucore内核中增加基于管程（monitor）的条件变量（condition variable）的支持； 了解经典进程同步问题，并能使用同步机制解决进程同步问题。 "},"lab7/lab7_2_labs.html":{"url":"lab7/lab7_2_labs.html","title":"实验内容","keywords":"","body":"实验内容 实验六完成了用户进程的调度框架和具体的调度算法，可调度运行多个进程。如果多个进程需要协同操作或访问共享资源，则存在如何同步和有序竞争的问题。本次实验，主要是熟悉ucore的进程同步机制—信号量（semaphore）机制，以及基于信号量的哲学家就餐问题解决方案。然后掌握管程的概念和原理，并参考信号量机制，实现基于管程的条件变量机制和基于条件变量来解决哲学家就餐问题。 在本次实验中，在kern/sync/check_sync.c中提供了一个基于信号量的哲学家就餐问题解法。同时还需完成练习，即实现基于管程（主要是灵活运用条件变量和互斥信号量）的哲学家就餐问题解法。哲学家就餐问题描述如下：有五个哲学家，他们的生活方式是交替地进行思考和进餐。哲学家们公用一张圆桌，周围放有五把椅子，每人坐一把。在圆桌上有五个碗和五根筷子，当一个哲学家思考时，他不与其他人交谈，饥饿时便试图取用其左、右最靠近他的筷子，但他可能一根都拿不到。只有在他拿到两根筷子时，方能进餐，进餐完后，放下筷子又继续思考。 "},"lab7/lab7_2_1_exercises.html":{"url":"lab7/lab7_2_1_exercises.html","title":"练习","keywords":"","body":"练习 为了实现lab7的目标，lab2提供了2个基本练习和2个扩展练习，要求完成实验报告。 注意有“LAB7”的注释，代码中所有需要完成的地方（challenge除外）都有“LAB7”和“YOUR CODE”的注释 对实验报告的要求： 基于markdown格式来完成，以文本方式为主 填写各个基本练习中要求完成的报告内容 完成实验后，请分析ucore_lab中提供的参考答案，并请在实验报告中说明你的实现与参考答案的区别 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点） 列出你认为OS原理中很重要，但在实验中没有对应上的知识点 练习1: 理解内核级信号量的实现和基于内核级信号量的哲学家就餐问题（不需要编码） 请在实验报告中给出内核级信号量的设计描述，并说明其大致执行流程。 请在实验报告中给出给用户态进程/线程提供信号量机制的设计方案，并比较说明给内核级提供信号量机制的异同。 练习2: 完成内核级条件变量和基于内核级条件变量的哲学家就餐问题（需要编码） 首先掌握管程机制，然后基于信号量实现完成条件变量实现，然后用管程机制实现哲学家就餐问题的解决方案（基于条件变量）。 完成代码编写后，编译并运行代码：make qemu -j 16 如果可以得到如 编译方法所示的显示内容（仅供参考，不是标准答案输出），则基本正确。 请在实验报告中给出内核级条件变量的设计描述，并说明其大致执行流程。 请在实验报告中给出给用户态进程/线程提供条件变量机制的设计方案，并比较说明给内核级提供条件变量机制的异同。 请在实验报告中回答：能否不用基于信号量机制来完成条件变量？如果不能，请给出理由，如果能，请给出设计说明和具体实现。 扩展练习 Challenge ：　在ucore中实现简化的死锁和重入探测机制 在ucore下实现一种探测机制，能够在多进程/线程运行同步互斥问题时，动态判断当前系统是否出现了死锁产生的必要条件，是否产生了多个进程进入临界区的情况。 如果发现，让系统进入monitor状态，打印出你的探测信息。 扩展练习 Challenge ：　参考Linux的RCU机制，在ucore中实现简化的RCU机制 在ucore 下实现下Linux的RCU同步互斥机制。可阅读相关Linux内核书籍或查询网上资料，可了解RCU的设计实现细节，然后简化实现在ucore中。 要求有实验报告说明你的设计思路，并提供测试用例。下面是一些参考资料： http://www.ibm.com/developerworks/cn/linux/l-rcu/ http://www.diybl.com/course/6_system/linux/Linuxjs/20081117/151814.html "},"lab7/lab7_2_2_compile.html":{"url":"lab7/lab7_2_2_compile.html","title":"编译方法","keywords":"","body":"编译方法 Makefile修改 在Makefile中取消LAB7 := -DLAB7_EX1 -D_SHOW_PHI(第12行)的注释 LAB1 := -DLAB1_EX4 # -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3 LAB3 := -DLAB3_EX1 -DLAB3_EX2 LAB4 := -DLAB4_EX1 -DLAB4_EX2 LAB5 := -DLAB5_EX1 -DLAB5_EX2 LAB6 := -DLAB6_EX2 LAB7 := -DLAB7_EX1 -D_SHOW_PHI # LAB8 := -DLAB8_EX1 -DLAB8_EX2 编译并运行代码的命令如下： make make qemu -j 16 补全代码后可以得到如下显示界面（仅供参考） chenyu$ make qemu -j 16 (THU.CST) os is loading ... Special kernel symbols: entry 0xA00000A0 (phys) etext 0xA0021000 (phys) edata 0xA0155490 (phys) end 0xA0158770 (phys) Kernel executable memory footprint: 1246KB memory management: default_pmm_manager memory map: [A0000000, A2000000] freemem start at: A0199000 free pages: 00001E67 ## 00000020 check_alloc_page() succeeded! check_pgdir() succeeded! check_boot_pgdir() succeeded! check_slab() succeeded! kmalloc_init() succeeded! check_vma_struct() succeeded! check_pgfault() succeeded! check_vmm() succeeded. sched class: stride_scheduler proc_init succeeded kernel_execve: pid = 2, name = \"exit\". I am the parent. Forking the child... I am parent, fork a child pid 13 I am the parent, waiting now.. I am No.0 philosopher_sema Iter 1, No.0 philosopher_sema is thinking I am No.1 philosopher_sema Iter 1, No.1 philosopher_sema is thinking I am No.2 philosopher_sema Iter 1, No.2 philosopher_sema is thinking I am No.2 philosopher_condvar Iter 1, No.2 philosopher_condvar is thinking I am No.3 philosopher_sema Iter 1, No.3 philosopher_sema is thinking I am No.4 philosopher_sema Iter 1, No.4 philosopher_sema is thinking I am No.0 philosopher_condvar Iter 1, No.0 philosopher_condvar is thinking I am No.1 philosopher_condvar Iter 1, No.1 philosopher_condvar is thinking I am No.3 philosopher_condvar Iter 1, No.3 philosopher_condvar is thinking I am No.4 philosopher_condvar Iter 1, No.4 philosopher_condvar is thinking I am the child. waitpid 13 ok. exit pass. Iter 1, No.0 philosopher_sema is eating Iter 1, No.2 philosopher_sema is eating phi_test_condvar: state_condvar[2] will eating phi_test_condvar: signal self_cv[2] cond_signal begin: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 0 cond_signal end: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 0 Iter 1, No.2 philosopher_condvar is eating phi_test_condvar: state_condvar[0] will eating phi_test_condvar: signal self_cv[0] cond_signal begin: cvp a01b2090, cvp->count 0, cvp->owner->next_count 0 cond_signal end: cvp a01b2090, cvp->count 0, cvp->owner->next_count 0 Iter 1, No.0 philosopher_condvar is eating phi_take_forks_condvar: 1 didn't get fork and will wait cond_wait begin: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 0 phi_take_forks_condvar: 3 didn't get fork and will wait cond_wait begin: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 0 phi_take_forks_condvar: 4 didn't get fork and will wait cond_wait begin: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.0 philosopher_sema is thinking Iter 2, No.2 philosopher_sema is thinking Iter 1, No.1 philosopher_sema is eating phi_test_condvar: state_condvar[3] will eating phi_test_condvar: signal self_cv[3] cond_signal begin: cvp a01b20cc, cvp->count 1, cvp->owner->next_count 0 Iter 1, No.4 philosopher_sema is eating cond_wait end: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 1 Iter 1, No.3 philosopher_condvar is eating cond_signal end: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.2 philosopher_condvar is thinking phi_test_condvar: state_condvar[1] will eating phi_test_condvar: signal self_cv[1] cond_signal begin: cvp a01b20a4, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 1 Iter 1, No.1 philosopher_condvar is eating cond_signal end: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.0 philosopher_condvar is thinking phi_take_forks_condvar: 0 didn't get fork and will wait cond_wait begin: cvp a01b2090, cvp->count 0, cvp->owner->next_count 0 phi_test_condvar: state_condvar[0] will eating phi_test_condvar: signal self_cv[0] cond_signal begin: cvp a01b2090, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b2090, cvp->count 0, cvp->owner->next_count 1 Iter 2, No.0 philosopher_condvar is eating cond_signal end: cvp a01b2090, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.1 philosopher_condvar is thinking Iter 2, No.1 philosopher_sema is thinking phi_take_forks_condvar: 2 didn't get fork and will wait cond_wait begin: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.2 philosopher_sema is eating Iter 2, No.4 philosopher_sema is thinking phi_test_condvar: state_condvar[2] will eating phi_test_condvar: signal self_cv[2] cond_signal begin: cvp a01b20b8, cvp->count 1, cvp->owner->next_count 0 Iter 2, No.0 philosopher_sema is eating cond_wait end: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 1 Iter 2, No.2 philosopher_condvar is eating cond_signal end: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.3 philosopher_condvar is thinking Iter 3, No.0 philosopher_sema is thinking Iter 3, No.2 philosopher_condvar is thinking phi_test_condvar: state_condvar[4] will eating phi_test_condvar: signal self_cv[4] cond_signal begin: cvp a01b20e0, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 1 Iter 1, No.4 philosopher_condvar is eating Iter 3, No.2 philosopher_sema is thinking Iter 1, No.3 philosopher_sema is eating cond_signal end: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 0 Iter 3, No.0 philosopher_condvar is thinking phi_test_condvar: state_condvar[1] will eating phi_test_condvar: signal self_cv[1] cond_signal begin: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 0 cond_signal end: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.1 philosopher_condvar is eating phi_take_forks_condvar: 3 didn't get fork and will wait cond_wait begin: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.1 philosopher_sema is eating Iter 2, No.3 philosopher_sema is thinking Iter 2, No.4 philosopher_sema is eating phi_take_forks_condvar: 0 didn't get fork and will wait cond_wait begin: cvp a01b2090, cvp->count 0, cvp->owner->next_count 0 Iter 3, No.1 philosopher_condvar is thinking phi_test_condvar: state_condvar[3] will eating phi_test_condvar: signal self_cv[3] cond_signal begin: cvp a01b20cc, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 1 Iter 2, No.3 philosopher_condvar is eating Iter 3, No.1 philosopher_sema is thinking Iter 3, No.2 philosopher_sema is eating cond_signal end: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 0 phi_test_condvar: state_condvar[0] will eating phi_test_condvar: signal self_cv[0] cond_signal begin: cvp a01b2090, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b2090, cvp->count 0, cvp->owner->next_count 1 Iter 3, No.0 philosopher_condvar is eating cond_signal end: cvp a01b2090, cvp->count 0, cvp->owner->next_count 0 Iter 2, No.4 philosopher_condvar is thinking phi_take_forks_condvar: 2 didn't get fork and will wait cond_wait begin: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 0 phi_take_forks_condvar: 4 didn't get fork and will wait cond_wait begin: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 0 Iter 4, No.0 philosopher_condvar is thinking Iter 3, No.4 philosopher_sema is thinking phi_test_condvar: state_condvar[1] will eating phi_test_condvar: signal self_cv[1] cond_signal begin: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 0 cond_signal end: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 0 Iter 3, No.1 philosopher_condvar is eating phi_test_condvar: state_condvar[4] will eating phi_test_condvar: signal self_cv[4] cond_signal begin: cvp a01b20e0, cvp->count 1, cvp->owner->next_count 0 Iter 3, No.0 philosopher_sema is eating Iter 4, No.2 philosopher_sema is thinking Iter 2, No.3 philosopher_sema is eating cond_wait end: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 1 Iter 2, No.4 philosopher_condvar is eating cond_signal end: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 0 Iter 3, No.3 philosopher_condvar is thinking phi_take_forks_condvar: 3 didn't get fork and will wait cond_wait begin: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 0 phi_test_condvar: state_condvar[3] will eating phi_test_condvar: signal self_cv[3] cond_signal begin: cvp a01b20cc, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 1 Iter 3, No.3 philosopher_condvar is eating cond_signal end: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 0 Iter 3, No.4 philosopher_condvar is thinking Iter 4, No.0 philosopher_sema is thinking Iter 3, No.1 philosopher_sema is eating Iter 3, No.3 philosopher_sema is thinking Iter 4, No.1 philosopher_condvar is thinking Iter 3, No.4 philosopher_sema is eating phi_test_condvar: state_condvar[0] will eating phi_test_condvar: signal self_cv[0] cond_signal begin: cvp a01b2090, cvp->count 0, cvp->owner->next_count 0 cond_signal end: cvp a01b2090, cvp->count 0, cvp->owner->next_count 0 Iter 4, No.0 philosopher_condvar is eating Iter 4, No.4 philosopher_sema is thinking No.0 philosopher_condvar quit phi_test_condvar: state_condvar[2] will eating phi_test_condvar: signal self_cv[2] cond_signal begin: cvp a01b20b8, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 1 Iter 3, No.2 philosopher_condvar is eating Iter 4, No.1 philosopher_sema is thinking Iter 4, No.2 philosopher_sema is eating cond_signal end: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 0 Iter 4, No.3 philosopher_condvar is thinking phi_test_condvar: state_condvar[4] will eating phi_test_condvar: signal self_cv[4] cond_signal begin: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 0 cond_signal end: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 0 Iter 3, No.4 philosopher_condvar is eating Iter 4, No.0 philosopher_sema is eating phi_take_forks_condvar: 1 didn't get fork and will wait cond_wait begin: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 0 phi_take_forks_condvar: 3 didn't get fork and will wait cond_wait begin: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 0 Iter 4, No.4 philosopher_condvar is thinking No.0 philosopher_sema quit phi_test_condvar: state_condvar[1] will eating phi_test_condvar: signal self_cv[1] cond_signal begin: cvp a01b20a4, cvp->count 1, cvp->owner->next_count 0 Iter 4, No.4 philosopher_sema is eating cond_wait end: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 1 Iter 4, No.1 philosopher_condvar is eating cond_signal end: cvp a01b20a4, cvp->count 0, cvp->owner->next_count 0 phi_test_condvar: state_condvar[3] will eating phi_test_condvar: signal self_cv[3] cond_signal begin: cvp a01b20cc, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 1 Iter 4, No.3 philosopher_condvar is eating No.2 philosopher_sema quit Iter 4, No.1 philosopher_sema is eating cond_signal end: cvp a01b20cc, cvp->count 0, cvp->owner->next_count 0 Iter 4, No.2 philosopher_condvar is thinking phi_take_forks_condvar: 4 didn't get fork and will wait cond_wait begin: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 0 No.1 philosopher_sema quit phi_take_forks_condvar: 2 didn't get fork and will wait cond_wait begin: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 0 No.4 philosopher_sema quit Iter 3, No.3 philosopher_sema is eating No.1 philosopher_condvar quit phi_test_condvar: state_condvar[2] will eating phi_test_condvar: signal self_cv[2] cond_signal begin: cvp a01b20b8, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 1 Iter 4, No.2 philosopher_condvar is eating cond_signal end: cvp a01b20b8, cvp->count 0, cvp->owner->next_count 0 phi_test_condvar: state_condvar[4] will eating phi_test_condvar: signal self_cv[4] cond_signal begin: cvp a01b20e0, cvp->count 1, cvp->owner->next_count 0 cond_wait end: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 1 Iter 4, No.4 philosopher_condvar is eating cond_signal end: cvp a01b20e0, cvp->count 0, cvp->owner->next_count 0 No.3 philosopher_condvar quit Iter 4, No.3 philosopher_sema is thinking No.4 philosopher_condvar quit No.2 philosopher_condvar quit Iter 4, No.3 philosopher_sema is eating No.3 philosopher_sema quit all user-mode processes have quit. init check memory pass. kernel panic at kern/process/proc.c:554: initproc exit. Welcome to the kernel debug monitor!! Type 'help' for a list of commands. K> "},"lab7/lab7_3_synchronization_implement.html":{"url":"lab7/lab7_3_synchronization_implement.html","title":"同步互斥机制的设计与实现","keywords":"","body":"同步互斥的设计与实现 "},"lab7/lab7_3_1_experiment.html":{"url":"lab7/lab7_3_1_experiment.html","title":"实验执行流程概述","keywords":"","body":"实验执行流程概述 互斥是指某一资源同时只允许一个进程对其进行访问，具有唯一性和排它性，但互斥不用限制进程对资源的访问顺序，即访问可以是无序的。同步是指在进程间的执行必须严格按照规定的某种先后次序来运行，即访问是有序的，这种先后次序取决于要系统完成的任务需求。在进程写资源情况下，进程间要求满足互斥条件。在进程读资源情况下，可允许多个进程同时访问资源。 实验七设计实现了多种同步互斥手段，包括时钟中断管理、等待队列、信号量、管程机制（包含条件变量设计）等，并基于信号量实现了哲学家问题的执行过程。而本次实验的练习是要求用管程机制实现哲学家问题的执行过程。在实现信号量机制和管程机制时，需要让无法进入临界区的进程睡眠，为此在ucore中设计了等待队列wait_queue。当进程无法进入临界区（即无法获得信号量）时，可让进程进入等待队列，这时的进程处于等待状态（也可称为阻塞状态），从而会让实验六中的调度器选择一个处于就绪状态（即RUNNABLE STATE）的进程，进行进程切换，让新进程有机会占用CPU执行，从而让整个系统的运行更加高效。 在实验七中的ucore初始化过程，开始的执行流程都与实验六相同，直到执行到创建第二个内核线程init_main时，修改了init_main的具体执行内容，即增加了check_sync函数的调用，而位于kern/sync/check_sync.c中的check_sync函数可以理解为是实验七的起始执行点，是实验七的总控函数。进一步分析此函数，可以看到这个函数主要分为了两个部分，第一部分是实现基于信号量的哲学家问题，第二部分是实现基于管程的哲学家问题。 对于check_sync函数的第一部分，首先实现初始化了一个互斥信号量，然后创建了对应5个哲学家行为的5个信号量，并创建5个内核线程代表5个哲学家，每个内核线程完成了基于信号量的哲学家吃饭睡觉思考行为实现。这部分是给学生作为练习参考用的。学生可以看看信号量是如何实现的，以及如何利用信号量完成哲学家问题。 对于check_sync函数的第二部分，首先初始化了管程，然后又创建了5个内核线程代表5个哲学家，每个内核线程要完成基于管程的哲学家吃饭、睡觉、思考的行为实现。这部分需要学生来具体完成。学生需要掌握如何用信号量来实现条件变量，以及包含条件变量的管程如何能够确保哲学家能够正常思考和吃饭。 "},"lab7/lab7_3_2_synchronization_basic_support.html":{"url":"lab7/lab7_3_2_synchronization_basic_support.html","title":"同步互斥机制的底层支撑","keywords":"","body":"同步互斥的底层支撑 由于有处理器调度的存在，且进程在访问某类资源暂时无法满足的情况下，进程会进入等待状态。这导致了多进程执行时序的不确定性和潜在执行结果的不确定性。为了确保执行结果的正确性，本试验需要设计更加完善的进程等待和互斥的底层支撑机制，确保能正确提供基于信号量和条件变量的同步互斥机制。 根据操作系统原理的知识，我们知道如果没有在硬件级保证读内存-修改值-写回内存的原子性，我们只能通过复杂的软件来实现同步互斥操作。但由于有定时器、屏蔽/使能中断、等待队列wait_queue支持test_and_set_bit等原子操作机器指令（在本次实验中没有用到）的存在，使得我们在实现进程等待、同步互斥上得到了极大的简化。下面将对定时器、屏蔽/使能中断和等待队列进行进一步讲解。 "},"lab7/lab7_3_2_1_timer.html":{"url":"lab7/lab7_3_2_1_timer.html","title":"计时器","keywords":"","body":"定时器 在传统的操作系统中，定时器是其中一个基础而重要的功能.它提供了基于时间事件的调度机制。在ucore 中，时钟（timer）中断给操作系统提供了有一定间隔的时间事件，操作系统将其作为基本的调度和计时单位（我们记两次时间中断之间的时间间隔为一个时间片，timer splice）。 基于此时间单位，操作系统得以向上提供基于时间点的事件，并实现基于时间长度的睡眠等待和唤醒机制。在每个时钟中断发生时，操作系统产生对应的时间事件。应用程序或者操作系统的其他组件可以以此来构建更复杂和高级的进程管理和调度算法。 sched.h, sched.c 定义了有关timer的各种相关接口来使用 timer 服务，其中主要包括: typedef struct {……} timer_t: 定义了 timer_t 的基本结构，其可以用 sched.h 中的timer_init函数对其进行初始化。 void timer_init(timer t *timer, struct proc_struct *proc, int expires): 对某定时器 进行初始化，让它在 expires 时间片之后唤醒 proc 进程。 void add_timer(timer t *timer): 向系统添加某个初始化过的timer_t，该定时器在 指定时间后被激活，并将对应的进程唤醒至runnable（如果当前进程处在等待状态）。 void del_timer(timer_t *time): 向系统删除（或者说取消）某一个定时器。该定时器在取消后不会被系统激活并唤醒进程。 void run_timer_list(void): 更新当前系统时间点，遍历当前所有处在系统管理内的定时器，找出所有应该激活的计数器，并激活它们。该过程在且只在每次定时器中断时被调用。在ucore 中，其还会调用调度器事件处理程序。 一个 timer_t 在系统中的存活周期可以被描述如下： timer_t 在某个位置被创建和初始化，并通过 add_timer加入系统管理列表中 系统时间被不断累加，直到 run_timer_list 发现该 timer_t到期。 run_timer_list更改对应的进程状态，并从系统管理列表中移除该timer_t。 尽管本次实验并不需要填充定时器相关的代码，但是作为系统重要的组件（同时定时器也是调度器的一个部分），你应该了解其相关机制和在ucore中的实现方法和使用方法。且在trap_dispatch函数中修改之前对时钟中断的处理，使得ucore能够利用定时器提供的功能完成调度和睡眠唤醒等操作。 "},"lab7/lab7_3_2_2_interrupt.html":{"url":"lab7/lab7_3_2_2_interrupt.html","title":"屏蔽与使能中断","keywords":"","body":"屏蔽与使能中断 根据操作系统原理的知识，我们知道如果没有在硬件级保证读内存-修改值-写回内存的原子性，我们只能通过复杂的软件来实现同步互斥操作。但由于有开关中断和test_and_set_bit等原子操作机器指令的存在，使得我们在实现同步互斥原语上可以大大简化。 在ucore中提供的底层机制包括中断屏蔽/使能控制等。kern/sync.c中实现的开关中断的控制函数local_intr_save(x)和local_intr_restore(x)，它们是基于kern/driver文件下的intr_enable()、intr_disable()函数实现的。具体调用关系为： 关中断：local_intr_save --> __intr_save --> intr_disable --> __lcsr_csrxchg(LISA_CSR_CRMD_IE, LISA_CSR_CRMD_IE, LISA_CSR_CRMD) 开中断：local_intr_restore--> __intr_restore --> intr_enable --> __lcsr_csrxchg(0, LISA_CSR_CRMD_IE, LISA_CSR_CRMD) 在LoongArch32架构中，中断的开关是通过修改CSR.CRMD寄存器中的IE位来实现的，最终实现了关（屏蔽）中断和开（使能）中断。通过关闭中断，可以防止对当前执行的控制流被其他中断事件处理所打断。既然不能中断，那也就意味着在内核运行的当前进程无法被打断或被重新调度，即实现了对临界区的互斥操作。所以在单处理器情况下，可以通过开关中断实现对临界区的互斥保护，需要互斥的临界区代码的一般写法为： local_intr_save(intr_flag); { 临界区代码 } local_intr_restore(intr_flag); …… 由于目前ucore只实现了对单处理器的支持，所以通过这种方式，就可简单地支撑互斥操作了。在多处理器情况下，这种方法是无法实现互斥的，因为屏蔽了一个CPU的中断，只能阻止本地CPU上的进程不会被中断或调度，并不意味着其他CPU上执行的进程不能执行临界区的代码。所以，开关中断只对单处理器下的互斥操作起作用。在本实验中，开关中断机制是实现信号量等高层同步互斥原语的底层支撑基础之一。 "},"lab7/lab7_3_2_3_waitqueue.html":{"url":"lab7/lab7_3_2_3_waitqueue.html","title":"等待队列","keywords":"","body":"等待队列 到目前为止，我们的实验中，用户进程或内核线程还没有睡眠的支持机制。在课程中提到用户进程或内核线程可以转入等待状态以等待某个特定事件（比如睡眠,等待子进程结束,等待信号量等），当该事件发生时这些进程能够被再次唤醒。内核实现这一功能的一个底层支撑机制就是等待队列wait_queue，等待队列和每一个事件（睡眠结束、时钟到达、任务完成、资源可用等）联系起来。需要等待事件的进程在转入休眠状态后插入到等待队列中。当事件发生之后，内核遍历相应等待队列，唤醒休眠的用户进程或内核线程，并设置其状态为就绪状态（PROC_RUNNABLE），并将该进程从等待队列中清除。ucore在kern/sync/{ wait.h, wait.c }中实现了等待项wait结构和等待队列wait queue结构以及相关函数），这是实现ucore中的信号量机制和条件变量机制的基础，进入wait queue的进程会被设为等待状态（PROC_SLEEPING），直到他们被唤醒。 　数据结构定义 typedef struct { struct proc_struct *proc; //等待进程的指针 uint32_t wakeup_flags; //进程被放入等待队列的原因标记 wait_queue_t *wait_queue; //指向此wait结构所属于的wait_queue list_entry_t wait_link; //用来组织wait_queue中wait节点的连接 } wait_t; typedef struct { list_entry_t wait_head; //wait_queue的队头 } wait_queue_t; le2wait(le, member) //实现wait_t中成员的指针向wait_t 指针的转化 　相关函数说明 与wait和wait queue相关的函数主要分为两层，底层函数是对wait queue的初始化、插入、删除和查找操作，相关函数如下： void wait_init(wait_t *wait, struct proc_struct *proc); //初始化wait结构 bool wait_in_queue(wait_t *wait); //wait是否在wait queue中 void wait_queue_init(wait_queue_t *queue); //初始化wait_queue结构 void wait_queue_add(wait_queue_t *queue, wait_t *wait); //把wait前插到wait queue中 void wait_queue_del(wait_queue_t *queue, wait_t *wait); //从wait queue中删除wait wait_t *wait_queue_next(wait_queue_t *queue, wait_t *wait);//取得wait的后一个链接指针 wait_t *wait_queue_prev(wait_queue_t *queue, wait_t *wait);//取得wait的前一个链接指针 wait_t *wait_queue_first(wait_queue_t *queue); //取得wait queue的第一个wait wait_t *wait_queue_last(wait_queue_t *queue); //取得wait queue的最后一个wait bool wait_queue_empty(wait_queue_t *queue); //wait queue是否为空 高层函数基于底层函数实现了让进程进入等待队列--wait_current_set，以及从等待队列中唤醒进程--wakeup_wait，相关函数如下： //让wait与进程关联，且让当前进程关联的wait进入等待队列queue，当前进程睡眠 void wait_current_set(wait_queue_t *queue, wait_t *wait, uint32_t wait_state); //把与当前进程关联的wait从等待队列queue中删除 wait_current_del(queue, wait); //唤醒与wait关联的进程 void wakeup_wait(wait_queue_t *queue, wait_t *wait, uint32_t wakeup_flags, bool del); //唤醒等待队列上挂着的第一个wait所关联的进程 void wakeup_first(wait_queue_t *queue, uint32_t wakeup_flags, bool del); //唤醒等待队列上所有的等待的进程 void wakeup_queue(wait_queue_t *queue, uint32_t wakeup_flags, bool del); 调用关系举例 如下图所示，对于唤醒进程的函数wakeup_wait，可以看到它会被各种信号量的V操作函数up调用，并且它会调用wait_queue_del函数和wakup_proc函数来完成唤醒进程的操作。 如下图所示，而对于让进程进入等待状态的函数wait_current_set，可以看到它会被各种信号量的P操作函数｀down调用，并且它会调用wait_init完成对等待项的初始化，并进一步调用wait_queue_add`来把与要处于等待状态的进程所关联的等待项挂到与信号量绑定的等待队列中。 "},"lab7/lab7_3_3_semaphore.html":{"url":"lab7/lab7_3_3_semaphore.html","title":"信号量","keywords":"","body":"信号量 信号量是一种同步互斥机制的实现，普遍存在于现在的各种操作系统内核里。相对于spinlock 的应用对象，信号量的应用对象是在临界区中运行的时间较长的进程。等待信号量的进程需要睡眠来减少占用 CPU 的开销。参考教科书“Operating Systems Internals and Design Principles”第五章“同步互斥”中对信号量实现的原理性描述： struct semaphore { int count; queueType queue; }; void semWait(semaphore s) { s.count--; if (s.count 基于上诉信号量实现可以认为，当多个（>1）进程可以进行互斥或同步合作时，一个进程会由于无法满足信号量设置的某条件而在某一位置停止，直到它接收到一个特定的信号（表明条件满足了）。为了发信号，需要使用一个称作信号量的特殊变量。为通过信号量s传送信号，信号量的V操作采用进程可执行原语semSignal(s)；为通过信号量s接收信号，信号量的P操作采用进程可执行原语semWait(s)；如果相应的信号仍然没有发送，则进程被阻塞或睡眠，直到发送完为止。 ucore中信号量参照上述原理描述，建立在开关中断机制和wait_queue的基础上进行了具体实现。信号量的数据结构定义如下： typedef struct { int value; //信号量的当前值 wait_queue_t wait_queue; //信号量对应的等待队列 } semaphore_t; semaphore_t是最基本的记录型信号量（record semaphore)结构，包含了用于计数的整数值value，和一个进程等待队列wait_queue，一个等待的进程会挂在此等待队列上。 在ucore中最重要的信号量操作是P操作函数down(semaphore_t *sem)和V操作函数 up(semaphore_t *sem)。但这两个函数的具体实现是__down(semaphore_t *sem, uint32_t wait_state) 函数和__up(semaphore_t *sem, uint32_t wait_state)函数，二者的具体实现描述如下： ● __down(semaphore_t *sem, uint32_t wait_state, timer_t *timer)：具体实现信号量的P操作，首先关掉中断，然后判断当前信号量的value是否大于0。如果是>0，则表明可以获得信号量，故让value减一，并打开中断返回即可；如果不是>0，则表明无法获得信号量，故需要将当前的进程加入到等待队列中，并打开中断，然后运行调度器选择另外一个进程执行。如果被V操作唤醒，则把自身关联的wait从等待队列中删除（此过程需要先关中断，完成后开中断）。具体实现如下所示： static __noinline uint32_t __down(semaphore_t *sem, uint32_t wait_state) { bool intr_flag; local_intr_save(intr_flag); if (sem->value > 0) { sem->value --; local_intr_restore(intr_flag); return 0; } wait_t __wait, *wait = &__wait; wait_current_set(&(sem->wait_queue), wait, wait_state); local_intr_restore(intr_flag); schedule(); local_intr_save(intr_flag); wait_current_del(&(sem->wait_queue), wait); local_intr_restore(intr_flag); if (wait->wakeup_flags != wait_state) { return wait->wakeup_flags; } return 0; } 与__down相关的调用和被调用函数关系图如下所示： ● __up(semaphore_t *sem, uint32_t wait_state)：具体实现信号量的V操作，首先关中断，如果信号量对应的wait queue中没有进程在等待，直接把信号量的value加一，然后开中断返回；如果有进程在等待且进程等待的原因是semophore设置的，则调用wakeup_wait函数将waitqueue中等待的第一个wait删除，且把此wait关联的进程唤醒，最后开中断返回。具体实现如下所示： static __noinline void __up(semaphore_t *sem, uint32_t wait_state) { bool intr_flag; local_intr_save(intr_flag); { wait_t *wait; if ((wait = wait_queue_first(&(sem->wait_queue))) == NULL) { sem->value ++; } else { wakeup_wait(&(sem->wait_queue), wait, wait_state, 1); } } local_intr_restore(intr_flag); } 与__up相关的调用和被调用函数关系图如下所示： 对照信号量的原理性描述和具体实现，可以发现二者在流程上基本一致，只是具体实现采用了关中断的方式保证了对共享资源的互斥访问，通过等待队列让无法获得信号量的进程睡眠等待。另外，我们可以看出信号量的计数器value具有有如下性质： value>0，表示共享资源的空闲数 vlaue value=0，表示等待队列为空 "},"lab7/lab7_3_4_monitors.html":{"url":"lab7/lab7_3_4_monitors.html","title":"管程和条件变量","keywords":"","body":"管程和条件变量 原理回顾 引入了管程是为了将对共享资源的所有访问及其所需要的同步操作集中并封装起来。Hansan为管程所下的定义：“一个管程定义了一个数据结构和能为并发进程所执行（在该数据结构上）的一组操作，这组操作能同步进程和改变管程中的数据”。有上述定义可知，管程由四部分组成： 管程内部的共享变量； 管程内部的条件变量； 管程内部并发执行的进程； 对局部于管程内部的共享数据设置初始值的语句。 局限在管程中的数据结构，只能被局限在管程的操作过程所访问，任何管程之外的操作过程都不能访问它；另一方面，局限在管程中的操作过程也主要访问管程内的数据结构。由此可见，管程相当于一个隔离区，它把共享变量和对它进行操作的若干个过程围了起来，所有进程要访问临界资源时，都必须经过管程才能进入，而管程每次只允许一个进程进入管程，从而需要确保进程之间互斥。 但在管程中仅仅有互斥操作是不够用的。进程可能需要等待某个条件Cond为真才能继续执行。如果采用忙等(busy waiting)方式： while not( Cond ) do {} 在单处理器情况下，将会导致所有其它进程都无法进入临界区使得该条件Cond为真，该管程的执行将会发生死锁。为此，可引入条件变量（Condition Variables，简称CV）。一个条件变量CV可理解为一个进程的等待队列，队列中的进程正等待某个条件Cond变为真。每个条件变量关联着一个条件，如果条件Cond不为真，则进程需要等待，如果条件Cond为真，则进程可以进一步在管程中执行。需要注意当一个进程等待一个条件变量CV（即等待Cond为真），该进程需要退出管程，这样才能让其它进程可以进入该管程执行，并进行相关操作，比如设置条件Cond为真，改变条件变量的状态，并唤醒等待在此条件变量CV上的进程。因此对条件变量CV有两种主要操作： wait_cv： 被一个进程调用，以等待断言Pc被满足后该进程可恢复执行. 进程挂在该条件变量上等待时，不被认为是占用了管程。 signal_cv：被一个进程调用，以指出断言Pc现在为真，从而可以唤醒等待断言Pc被满足的进程继续执行。 \"哲学家就餐\"实例 有了互斥和信号量支持的管程就可用用了解决各种同步互斥问题。比如参考《OS Concept》一书中的6.7.2小节“用管程解决哲学家就餐问题”就给出了这样的事例： monitor dp { enum {THINKING, HUNGRY, EATING} state[5]; condition self[5]; void pickup(int i) { state[i] = HUNGRY; test(i); if (state[i] != EATING) self[i].wait_cv(); } void putdown(int i) { state[i] = THINKING; test((i + 4) % 5); test((i + 1) % 5); } void test(int i) { if ((state[(i + 4) % 5] != EATING) && (state[i] == HUNGRY) && (state[(i + 1) % 5] != EATING)) { state[i] = EATING; self[i].signal_cv(); } } initialization code() { for (int i = 0; i 关键数据结构 虽然大部分教科书上说明管程适合在语言级实现比如java等高级语言，没有提及在采用C语言的OS中如何实现。下面我们将要尝试在ucore中用C语言实现采用基于互斥和条件变量机制的管程基本原理。 ucore中的管程机制是基于信号量和条件变量来实现的。ucore中的管程的数据结构monitor_t定义如下： typedef struct monitor{ semaphore_t mutex; // the mutex lock for going into the routines in monitor, should be initialized to 1 // the next semaphore is used to // (1) procs which call cond_signal funciton should DOWN next sema after UP cv.sema // OR (2) procs which call cond_wait funciton should UP next sema before DOWN cv.sema semaphore_t next; int next_count; // the number of of sleeped procs which cond_signal funciton condvar_t *cv; // the condvars in monitor } monitor_t; 管程中的成员变量mutex是一个二值信号量，是实现每次只允许一个进程进入管程的关键元素，确保了互斥访问性质。管程中的条件变量cv通过执行wait_cv，会使得等待某个条件Cond为真的进程能够离开管程并睡眠，且让其他进程进入管程继续执行；而进入管程的某进程设置条件Cond为真并执行signal_cv时，能够让等待某个条件Cond为真的睡眠进程被唤醒，从而继续进入管程中执行。 注意：管程中的成员变量信号量next和整型变量next_count是配合进程对条件变量cv的操作而设置的，这是由于发出signal_cv的进程A会唤醒由于wait_cv而睡眠的进程B，由于管程中只允许一个进程运行，所以进程B执行会导致唤醒进程B的进程A睡眠，直到进程B离开管程，进程A才能继续执行，这个同步过程是通过信号量next完成的；而next_count表示了由于发出singal_cv而睡眠的进程个数。 管程中的条件变量的数据结构condvar_t定义如下： typedef struct condvar{ semaphore_t sem; // the sem semaphore is used to down the waiting proc, and the signaling proc should up the waiting proc int count; 　 // the number of waiters on condvar monitor_t * owner; // the owner(monitor) of this condvar } condvar_t; 条件变量的定义中也包含了一系列的成员变量，信号量sem用于让发出wait_cv操作的等待某个条件Cond为真的进程睡眠，而让发出signal_cv操作的进程通过这个sem来唤醒睡眠的进程。count表示等在这个条件变量上的睡眠进程的个数。owner表示此条件变量的宿主是哪个管程。 条件变量的signal和wait的设计 理解了数据结构的含义后，我们就可以开始管程的设计实现了。ucore设计实现了条件变量wait_cv操作和signal_cv操作对应的具体函数，即cond_wait函数和cond_signal函数，此外还有cond_init初始化函数（可直接看源码）。函数cond_wait(condvar_t *cvp, semaphore_t *mp)和cond_signal (condvar_t *cvp)的实现原理参考了《OS Concept》一书中的6.7.3小节“用信号量实现管程”的内容。首先来看wait_cv的原理实现： wait_cv的原理描述 cv.count++; if(monitor.next_count > 0) sem_signal(monitor.next); else sem_signal(monitor.mutex); sem_wait(cv.sem); cv.count -- ; 对照着可分析出cond_wait函数的具体执行过程。可以看出如果进程A执行了cond_wait函数，表示此进程等待某个条件Cond不为真，需要睡眠。因此表示等待此条件的睡眠进程个数cv.count要加一。接下来会出现两种情况。 情况一：如果monitor.next_count如果大于0，表示有大于等于1个进程执行cond_signal函数且睡了，就睡在了monitor.next信号量上（假定这些进程挂在monitor.next信号量相关的等待队列Ｓ上），因此需要唤醒等待队列Ｓ中的一个进程B；然后进程A睡在cv.sem上。如果进程A醒了，则让cv.count减一，表示等待此条件变量的睡眠进程个数少了一个，可继续执行了！ 这里隐含这一个现象，即某进程A在时间顺序上先执行了cond_signal，而另一个进程B后执行了cond_wait，这会导致进程A没有起到唤醒进程B的作用。 问题: 在cond_wait有sem_signal(mutex)，但没有看到哪里有sem_wait(mutex)，这好像没有成对出现，是否是错误的？ 答案：其实在管程中的每一个函数的入口处会有wait(mutex)，这样二者就配好对了。 情况二：如果monitor.next_count如果小于等于0，表示目前没有进程执行cond_signal函数且睡着了，那需要唤醒的是由于互斥条件限制而无法进入管程的进程，所以要唤醒睡在monitor.mutex上的进程。然后进程A睡在cv.sem上，如果睡醒了，则让cv.count减一，表示等待此条件的睡眠进程个数少了一个，可继续执行了！ 然后来看signal_cv的原理实现： signal_cv的原理描述 if( cv.count > 0) { monitor.next_count ++; sem_signal(cv.sem); sem_wait(monitor.next); monitor.next_count -- ; } 对照着可分析出cond_signal函数的具体执行过程。首先进程B判断cv.count，如果不大于0，则表示当前没有执行cond_wait而睡眠的进程，因此就没有被唤醒的对象了，直接函数返回即可；如果大于0，这表示当前有执行cond_wait而睡眠的进程A，因此需要唤醒等待在cv.sem上睡眠的进程A。由于只允许一个进程在管程中执行，所以一旦进程B唤醒了别人（进程A），那么自己就需要睡眠。故让monitor.next_count加一，且让自己（进程B）睡在信号量monitor.next上。如果睡醒了，这让monitor.next_count减一。 管程中函数的入口出口设计 为了让整个管程正常运行，还需在管程中的每个函数的入口和出口增加相关操作，即： function_in_monitor （…） { sem.wait(monitor.mutex); //----------------------------- the real body of function; //----------------------------- if(monitor.next_count > 0) sem_signal(monitor.next); else sem_signal(monitor.mutex); } 这样带来的作用有两个，（1）只有一个进程在执行管程中的函数。（2）避免由于执行了cond_signal函数而睡眠的进程无法被唤醒。对于第二点，如果进程A由于执行了cond_signal函数而睡眠（这会让monitor.next_count大于0，且执行sem_wait(monitor.next)），则其他进程在执行管程中的函数的出口，会判断monitor.next_count是否大于0，如果大于0，则执行sem_signal(monitor.next)，从而执行了cond_signal函数而睡眠的进程被唤醒。上诉措施将使得管程正常执行。 需要注意的是，上述只是原理描述，与具体描述相比，还有一定的差距。需要大家在完成练习时仔细设计和实现。 "},"lab8.html":{"url":"lab8.html","title":"Lab 8","keywords":"","body":"实验八：文件系统 "},"lab8/lab8_1_goals.html":{"url":"lab8/lab8_1_goals.html","title":"实验目的","keywords":"","body":"实验目的 通过完成本次实验，希望能达到以下目标 了解基本的文件系统系统调用的实现方法； 了解一个基于索引节点组织方式的Simple FS文件系统的设计与实现； 了解文件系统抽象层-VFS的设计与实现； "},"lab8/lab8_2_labs.html":{"url":"lab8/lab8_2_labs.html","title":"实验内容","keywords":"","body":"实验内容 实验七完成了在内核中的同步互斥实验。本次实验涉及的是文件系统，通过分析了解ucore文件系统的总体架构设计，完善读写文件操作，从新实现基于文件系统的执行程序机制（即改写do_execve），从而可以完成执行存储在磁盘上的文件和实现文件读写等功能。 "},"lab8/lab8_2_1_exercises.html":{"url":"lab8/lab8_2_1_exercises.html","title":"练习","keywords":"","body":"练习 为了实现lab8的目标，lab2提供了2个基本练习，要求完成实验报告。 注意有“LAB8”的注释，代码中所有需要完成的地方（challenge除外）都有“LAB8”和“YOUR CODE”的注释 对实验报告的要求： 基于markdown格式来完成，以文本方式为主 填写各个基本练习中要求完成的报告内容 完成实验后，请分析ucore_lab中提供的参考答案，并请在实验报告中说明你的实现与参考答案的区别 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点） 列出你认为OS原理中很重要，但在实验中没有对应上的知识点 练习1: 完成读文件操作的实现（需要编码） 首先了解打开文件的处理流程，然后参考本实验后续的文件读写操作的过程分析，编写在sfs_inode.c中sfs_io_nolock读文件中数据的实现代码。 请在实验报告中给出设计实现”UNIX的PIPE机制“的概要设方案，鼓励给出详细设计方案 练习2: 完成基于文件系统的执行程序机制的实现（需要编码） 改写proc.c中的load_icode函数和其他相关函数，实现基于文件系统的执行程序机制。执行：make qemu -j 16。如果能看看到sh用户程序的执行界面，则基本成功了。如果在sh用户界面上可以执行”ls”,”hello”等其他放置在sfs文件系统中的其他执行程序，则可以认为本实验基本成功。 请在实验报告中给出设计实现基于”UNIX的硬链接和软链接机制“的概要设方案，鼓励给出详细设计方案 祝贺你通过自己的努力，完成了ucore OS lab1-lab8! "},"lab8/lab8_2_2_compile.html":{"url":"lab8/lab8_2_2_compile.html","title":"编译方法","keywords":"","body":"编译方法 Makefile修改 在Makefile中取消LAB8 := -DLAB8_EX1 -DLAB8_EX2(第13行)的注释 LAB1 := -DLAB1_EX4 # -D_SHOW_100_TICKS -D_SHOW_SERIAL_INPUT LAB2 := -DLAB2_EX1 -DLAB2_EX2 -DLAB2_EX3 LAB3 := -DLAB3_EX1 -DLAB3_EX2 LAB4 := -DLAB4_EX1 -DLAB4_EX2 LAB5 := -DLAB5_EX1 -DLAB5_EX2 LAB6 := -DLAB6_EX2 LAB7 := -DLAB7_EX1 # -D_SHOW_PHI LAB8 := -DLAB8_EX1 -DLAB8_EX2 编译并运行代码的命令如下： make make qemu -j 16 补全代码后可以得到如下显示界面（仅供参考） chenyu$ make qemu -j 16 (THU.CST) os is loading ... Special kernel symbols: entry 0xA00000A0 (phys) etext 0xA0021000 (phys) edata 0xA0251470 (phys) end 0xA0254750 (phys) Kernel executable memory footprint: 2254KB memory management: default_pmm_manager memory map: [A0000000, A2000000] freemem start at: A0295000 free pages: 00001D6B ## 00000020 check_alloc_page() succeeded! check_pgdir() succeeded! check_boot_pgdir() succeeded! check_slab() succeeded! kmalloc_init() succeeded! check_vma_struct() succeeded! check_pgfault() succeeded! check_vmm() succeeded. sched class: stride_scheduler proc_init succeeded Initrd: 0xa005d3d0 - 0xa02513cf, size: 0x001f4000, magic: 0x2f8dbe2a ramdisk_init(): initrd found, magic: 0x2f8dbe2a, 0x00000fa0 secs sfs: mount: 'simple file system' (318/182/500) vfs: mount disk0. kernel_execve: pid = 2, name = \"sh\". user sh is running!!! $ "},"lab8/lab8_3_fs_design_implement.html":{"url":"lab8/lab8_3_fs_design_implement.html","title":"文件系统设计与实现","keywords":"","body":"文件系统设计与实现 "},"lab8/lab8_3_1_ucore_fs_introduction.html":{"url":"lab8/lab8_3_1_ucore_fs_introduction.html","title":"ucore 文件系统总体介绍","keywords":"","body":"ucore 文件系统总体介绍 操作系统中负责管理和存储可长期保存数据的软件功能模块称为文件系统。在本次试验中，主要侧重文件系统的设计实现和对文件系统执行流程的分析与理解。 ucore的文件系统模型源于Havard的OS161的文件系统和Linux文件系统。但其实这二者都是源于传统的UNIX文件系统设计。UNIX提出了四个文件系统抽象概念：文件(file)、目录项(dentry)、索引节点(inode)和安装点(mount point)。 文件：UNIX文件中的内容可理解为是一有序字节buffer，文件都有一个方便应用程序识别的文件名称（也称文件路径名）。典型的文件操作有读、写、创建和删除等。 目录项：目录项不是目录（又称文件路径），而是目录的组成部分。在UNIX中目录被看作一种特定的文件，而目录项是文件路径中的一部分。如一个文件路径名是“/test/testfile”，则包含的目录项为：根目录“/”，目录“test”和文件“testfile”，这三个都是目录项。一般而言，目录项包含目录项的名字（文件名或目录名）和目录项的索引节点（见下面的描述）位置。 索引节点：UNIX将文件的相关元数据信息（如访问控制权限、大小、拥有者、创建时间、数据内容等等信息）存储在一个单独的数据结构中，该结构被称为索引节点。 安装点：在UNIX中，文件系统被安装在一个特定的文件路径位置，这个位置就是安装点。所有的已安装文件系统都作为根文件系统树中的叶子出现在系统中。 上述抽象概念形成了UNIX文件系统的逻辑数据结构，并需要通过一个具体文件系统的架构设计与实现把上述信息映射并储存到磁盘介质上，从而在具体文件系统的磁盘布局（即数据在磁盘上的物理组织）上具体体现出上述抽象概念。比如文件元数据信息存储在磁盘块中的索引节点上。当文件被载入内存时，内核需要使用磁盘块中的索引点来构造内存中的索引节点。 ucore模仿了UNIX的文件系统设计，ucore的文件系统架构主要由四部分组成： 通用文件系统访问接口层：该层提供了一个从用户空间到文件系统的标准访问接口。这一层访问接口让应用程序能够通过一个简单的接口获得ucore内核的文件系统服务。 文件系统抽象层：向上提供一个一致的接口给内核其他部分（文件系统相关的系统调用实现模块和其他内核功能模块）访问。向下提供一个同样的抽象函数指针列表和数据结构屏蔽不同文件系统的实现细节。 Simple FS文件系统层：一个基于索引方式的简单文件系统实例。向上通过各种具体函数实现以对应文件系统抽象层提出的抽象函数。向下访问外设接口 外设接口层：向上提供device访问接口屏蔽不同硬件细节。向下实现访问各种具体设备驱动的接口，比如disk设备接口/串口设备接口/键盘设备接口等。 对照上面的层次我们再大致介绍一下文件系统的访问处理过程，加深对文件系统的总体理解。假如应用程序操作文件（打开/创建/删除/读写），首先需要通过文件系统的通用文件系统访问接口层给用户空间提供的访问接口进入文件系统内部，接着由文件系统抽象层把访问请求转发给某一具体文件系统（比如SFS文件系统），具体文件系统（Simple FS文件系统层）把应用程序的访问请求转化为对磁盘上的block的处理请求，并通过外设接口层交给磁盘驱动例程来完成具体的磁盘操作。结合用户态写文件函数write的整个执行过程，我们可以比较清楚地看出ucore文件系统架构的层次和依赖关系。 ucore文件系统总体结构 从ucore操作系统不同的角度来看，ucore中的文件系统架构包含四类主要的数据结构, 它们分别是： 超级块（SuperBlock），它主要从文件系统的全局角度描述特定文件系统的全局信息。它的作用范围是整个OS空间。 索引节点（inode）：它主要从文件系统的单个文件的角度它描述了文件的各种属性和数据所在位置。它的作用范围是整个OS空间。 目录项（dentry）：它主要从文件系统的文件路径的角度描述了文件路径中的一个特定的目录项（注：一系列目录项形成目录/文件路径）。它的作用范围是整个OS空间。对于SFS而言，inode(具体为struct sfs_disk_inode)对应于物理磁盘上的具体对象，dentry（具体为struct sfs_disk_entry）是一个内存实体，其中的ino成员指向对应的inode number，另外一个成员是file name(文件名). 文件（file），它主要从进程的角度描述了一个进程在访问文件时需要了解的文件标识，文件读写的位置，文件引用情况等信息。它的作用范围是某一具体进程。 如果一个用户进程打开了一个文件，那么在ucore中涉及的相关数据结构（其中相关数据结构将在下面各个小节中展开叙述）和关系如下图所示： ucore中文件相关关键数据结构及其关系 "},"lab8/lab8_3_2_fs_interface.html":{"url":"lab8/lab8_3_2_fs_interface.html","title":"通用文件系统访问接口","keywords":"","body":"通用文件系统访问接口 文件和目录相关用户库函数 Lab8中部分用户库函数与文件系统有关，我们先讨论对单个文件进行操作的系统调用，然后讨论对目录和文件系统进行操作的系统调用。 在文件操作方面，最基本的相关函数是open、close、read、write。在读写一个文件之前，首先要用open系统调用将其打开。open的第一个参数指定文件的路径名，可使用绝对路径名；第二个参数指定打开的方式，可设置为O_RDONLY、O_WRONLY、O_RDWR，分别表示只读、只写、可读可写。在打开一个文件后，就可以使用它返回的文件描述符fd对文件进行相关操作。在使用完一个文件后，还要用close系统调用把它关闭，其参数就是文件描述符fd。这样它的文件描述符就可以空出来，给别的文件使用。 读写文件内容的系统调用是read和write。read系统调用有三个参数：一个指定所操作的文件描述符，一个指定读取数据的存放地址，最后一个指定读多少个字节。在C程序中调用该系统调用的方法如下： count = read(filehandle, buffer, nbytes); 该系统调用会把实际读到的字节数返回给count变量。在正常情形下这个值与nbytes相等，但有时可能会小一些。例如，在读文件时碰上了文件结束符，从而提前结束此次读操作。 如果由于参数无效或磁盘访问错误等原因，使得此次系统调用无法完成，则count被置为-1。而write函数的参数与之完全相同。 对于目录而言，最常用的操作是跳转到某个目录，这里对应的用户库函数是chdir。然后就需要读目录的内容了，即列出目录中的文件或目录名，这在处理上与读文件类似，即需要通过opendir函数打开目录，通过readdir来获取目录中的文件信息，读完后还需通过closedir函数来关闭目录。由于在ucore中把目录看成是一个特殊的文件，所以opendir和closedir实际上就是调用与文件相关的open和close函数。只有readdir需要调用获取目录内容的特殊系统调用sys_getdirentry。而且这里没有写目录这一操作。在目录中增加内容其实就是在此目录中创建文件，需要用到创建文件的函数。 文件和目录访问相关系统调用 与文件相关的open、close、read、write用户库函数对应的是sys_open、sys_close、sys_read、sys_write四个系统调用接口。与目录相关的readdir用户库函数对应的是sys_getdirentry系统调用。这些系统调用函数接口将通过syscall函数来获得ucore的内核服务。当到了ucore内核后，在调用文件系统抽象层的file接口和dir接口。 "},"lab8/lab8_3_4_fs_abstract.html":{"url":"lab8/lab8_3_4_fs_abstract.html","title":"文件系统抽象层 - VFS","keywords":"","body":"文件系统抽象层 - VFS 文件系统抽象层是把不同文件系统的对外共性接口提取出来，形成一个函数指针数组，这样，通用文件系统访问接口层只需访问文件系统抽象层，而不需关心具体文件系统的实现细节和接口。 "},"lab8/lab8_3_4_1_file_dir_interface.html":{"url":"lab8/lab8_3_4_1_file_dir_interface.html","title":"file & dir接口","keywords":"","body":"file & dir接口 file&dir接口层定义了进程在内核中直接访问的文件相关信息，这定义在file数据结构中，具体描述如下： struct file { enum { FD_NONE, FD_INIT, FD_OPENED, FD_CLOSED, } status; //访问文件的执行状态 bool readable; //文件是否可读 bool writable; //文件是否可写 int fd; //文件在filemap中的索引值 off_t pos; //访问文件的当前位置 struct inode *node; //该文件对应的内存inode指针 int open_count; //打开此文件的次数 }; 而在kern/process/proc.h中的proc_struct结构中描述了进程访问文件的数据接口files_struct，其数据结构定义如下： struct files_struct { struct inode *pwd; //进程当前执行目录的内存inode指针 struct file *fd_array; //进程打开文件的数组 atomic_t files_count; //访问此文件的线程个数 semaphore_t files_sem; //确保对进程控制块中fs_struct的互斥访问 }; 当创建一个进程后，该进程的files_struct将会被初始化或复制父进程的files_struct。当用户进程打开一个文件时，将从fd_array数组中取得一个空闲file项，然后会把此file的成员变量node指针指向一个代表此文件的inode的起始地址。 "},"lab8/lab8_3_4_2_inode_interface.html":{"url":"lab8/lab8_3_4_2_inode_interface.html","title":"inode 接口 ","keywords":"","body":"inode 接口 index node是位于内存的索引节点，它是VFS结构中的重要数据结构，因为它实际负责把不同文件系统的特定索引节点信息（甚至不能算是一个索引节点）统一封装起来，避免了进程直接访问具体文件系统。其定义如下： struct inode { union { //包含不同文件系统特定inode信息的union成员变量 struct device __device_info; //设备文件系统内存inode信息 struct sfs_inode __sfs_inode_info; //SFS文件系统内存inode信息 } in_info; enum { inode_type_device_info = 0x1234, inode_type_sfs_inode_info, } in_type; //此inode所属文件系统类型 atomic_t ref_count; //此inode的引用计数 atomic_t open_count; //打开此inode对应文件的个数 struct fs *in_fs; //抽象的文件系统，包含访问文件系统的函数指针 const struct inode_ops *in_ops; //抽象的inode操作，包含访问inode的函数指针 }; 在inode中，有一成员变量为in_ops，这是对此inode的操作函数指针列表，其数据结构定义如下： struct inode_ops { unsigned long vop_magic; int (*vop_open)(struct inode *node, uint32_t open_flags); int (*vop_close)(struct inode *node); int (*vop_read)(struct inode *node, struct iobuf *iob); int (*vop_write)(struct inode *node, struct iobuf *iob); int (*vop_getdirentry)(struct inode *node, struct iobuf *iob); int (*vop_create)(struct inode *node, const char *name, bool excl, struct inode **node_store); int (*vop_lookup)(struct inode *node, char *path, struct inode **node_store); …… }; 参照上面对SFS中的索引节点操作函数的说明，可以看出inode_ops是对常规文件、目录、设备文件所有操作的一个抽象函数表示。对于某一具体的文件系统中的文件或目录，只需实现相关的函数，就可以被用户进程访问具体的文件了，且用户进程无需了解具体文件系统的实现细节。 "},"lab8/lab8_3_3_sfs.html":{"url":"lab8/lab8_3_3_sfs.html","title":"Simple FS 文件系统","keywords":"","body":"Simple FS 文件系统 这里我们没有按照从上到下先讲文件系统抽象层，再讲具体的文件系统。这是由于如果能够理解Simple FS（简称SFS）文件系统，就可更好地分析文件系统抽象层的设计。即从具体走向抽象。ucore内核把所有文件都看作是字节流，任何内部逻辑结构都是专用的，由应用程序负责解释。但是ucore区分文件的物理结构。ucore目前支持如下几种类型的文件： 常规文件：文件中包括的内容信息是由应用程序输入。SFS文件系统在普通文件上不强加任何内部结构，把其文件内容信息看作为字节。 目录：包含一系列的entry，每个entry包含文件名和指向与之相关联的索引节点（index node）的指针。目录是按层次结构组织的。 链接文件：实际上一个链接文件是一个已经存在的文件的另一个可选择的文件名。 设备文件：不包含数据，但是提供了一个映射物理设备（如串口、键盘等）到一个文件名的机制。可通过设备文件访问外围设备。 管道：管道是进程间通讯的一个基础设施。管道缓存了其输入端所接受的数据，以便在管道输出端读的进程能一个先进先出的方式来接受数据。 在lab8中关注的主要是SFS支持的常规文件、目录和链接中的 hardlink 的设计实现。SFS文件系统中目录和常规文件具有共同的属性，而这些属性保存在索引节点中。SFS通过索引节点来管理目录和常规文件，索引节点包含操作系统所需要的关于某个文件的关键信息，比如文件的属性、访问许可权以及其它控制信息都保存在索引节点中。可以有多个文件名可指向一个索引节点。 "},"lab8/lab8_3_3_1_fs_layout.html":{"url":"lab8/lab8_3_3_1_fs_layout.html","title":"文件系统的布局","keywords":"","body":"文件系统的布局 文件系统通常保存在磁盘上。在本实验中，第三个磁盘（即disk0，前两个磁盘分别是 ucore.img 和 swap.img）用于存放一个SFS文件系统（Simple Filesystem）。通常文件系统中，磁盘的使用是以扇区（Sector）为单位的，但是为了实现简便，SFS 中以 block （4K，与内存 page 大小相等）为基本单位。 SFS文件系统的布局如下图所示。 第0个块（4K）是超级块（superblock），它包含了关于文件系统的所有关键参数，当计算机被启动或文件系统被首次接触时，超级块的内容就会被装入内存。其定义如下： struct sfs_super { uint32_t magic; /* magic number, should be SFS_MAGIC */ uint32_t blocks; /* # of blocks in fs */ uint32_t unused_blocks; /* # of unused blocks in fs */ char info[SFS_MAX_INFO_LEN + 1]; /* infomation for sfs */ }; 可以看到，包含一个成员变量魔数magic，其值为0x2f8dbe2a，内核通过它来检查磁盘镜像是否是合法的 SFS img；成员变量blocks记录了SFS中所有block的数量，即 img 的大小；成员变量unused_block记录了SFS中还没有被使用的block的数量；成员变量info包含了字符串\"simple file system\"。 第1个块放了一个root-dir的inode，用来记录根目录的相关信息。有关inode还将在后续部分介绍。这里只要理解root-dir是SFS文件系统的根结点，通过这个root-dir的inode信息就可以定位并查找到根目录下的所有文件信息。 从第2个块开始，根据SFS中所有块的数量，用1个bit来表示一个块的占用和未被占用的情况。这个区域称为SFS的freemap区域，这将占用若干个块空间。为了更好地记录和管理freemap区域，专门提供了两个文件kern/fs/sfs/bitmap.[ch]来完成根据一个块号查找或设置对应的bit位的值。 最后在剩余的磁盘空间中，存放了所有其他目录和文件的inode信息和内容数据信息。需要注意的是虽然inode的大小小于一个块的大小（4096B），但为了实现简单，每个 inode 都占用一个完整的 block。 在sfs_fs.c文件中的sfs_do_mount函数中，完成了加载位于硬盘上的SFS文件系统的超级块superblock和freemap的工作。这样，在内存中就有了SFS文件系统的全局信息。 "},"lab8/lab8_3_3_2_inode.html":{"url":"lab8/lab8_3_3_2_inode.html","title":"索引节点","keywords":"","body":"索引节点 在SFS文件系统中，需要记录文件内容的存储位置以及文件名与文件内容的对应关系。sfs_disk_inode记录了文件或目录的内容存储的索引信息，该数据结构在硬盘里储存，需要时读入内存。sfs_disk_entry表示一个目录中的一个文件或目录，包含该项所对应inode的位置和文件名，同样也在硬盘里储存，需要时读入内存。 磁盘索引节点 SFS中的磁盘索引节点代表了一个实际位于磁盘上的文件。首先我们看看在硬盘上的索引节点的内容： struct sfs_disk_inode { uint32_t size; 如果inode表示常规文件，则size是文件大小 uint16_t type; inode的文件类型 uint16_t nlinks; 此inode的硬链接数 uint32_t blocks; 此inode的数据块数的个数 uint32_t direct[SFS_NDIRECT]; 此inode的直接数据块索引值（有SFS_NDIRECT个） uint32_t indirect; 此inode的一级间接数据块索引值 }; 通过上表可以看出，如果inode表示的是文件，则成员变量direct[]直接指向了保存文件内容数据的数据块索引值。indirect间接指向了保存文件内容数据的数据块，indirect指向的是间接数据块（indirect block），此数据块实际存放的全部是数据块索引，这些数据块索引指向的数据块才被用来存放文件内容数据。 默认的，ucore 里 SFS_NDIRECT 是 12，即直接索引的数据页大小为 12 * 4k = 48k；当使用一级间接数据块索引时，ucore 支持最大的文件大小为 12 * 4k + 1024 * 4k = 48k + 4m。数据索引表内，0 表示一个无效的索引，inode 里 blocks 表示该文件或者目录占用的磁盘的 block 的个数。indiret 为 0 时，表示不使用一级索引块。（因为 block 0 用来保存 super block，它不可能被其他任何文件或目录使用，所以这么设计也是合理的）。 对于普通文件，索引值指向的 block 中保存的是文件中的数据。而对于目录，索引值指向的数据保存的是目录下所有的文件名以及对应的索引节点所在的索引块（磁盘块）所形成的数组。数据结构如下： /* file entry (on disk) */ struct sfs_disk_entry { uint32_t ino; 索引节点所占数据块索引值 char name[SFS_MAX_FNAME_LEN + 1]; 文件名 }; 操作系统中，每个文件系统下的 inode 都应该分配唯一的 inode 编号。SFS 下，为了实现的简便（偷懒），每个 inode 直接用他所在的磁盘 block 的编号作为 inode 编号。比如，root block 的 inode 编号为 1；每个 sfs_disk_entry 数据结构中，name 表示目录下文件或文件夹的名称，ino 表示磁盘 block 编号，通过读取该 block 的数据，能够得到相应的文件或文件夹的 inode。ino 为0时，表示一个无效的 entry。 此外，和 inode 相似，每个 sfs_dirent_entry 也占用一个 block。 内存中的索引节点 /* inode for sfs */ struct sfs_inode { struct sfs_disk_inode *din; /* on-disk inode */ uint32_t ino; /* inode number */ uint32_t flags; /* inode flags */ bool dirty; /* true if inode modified */ int reclaim_count; /* kill inode if it hits zero */ semaphore_t sem; /* semaphore for din */ list_entry_t inode_link; /* entry for linked-list in sfs_fs */ list_entry_t hash_link; /* entry for hash linked-list in sfs_fs */ }; 可以看到SFS中的内存inode包含了SFS的硬盘inode信息，而且还增加了其他一些信息，这属于是便于进行是判断否改写、互斥操作、回收和快速地定位等作用。需要注意，一个内存inode是在打开一个文件后才创建的，如果关机则相关信息都会消失。而硬盘inode的内容是保存在硬盘中的，只是在进程需要时才被读入到内存中，用于访问文件或目录的具体内容数据 为了方便实现上面提到的多级数据的访问以及目录中 entry 的操作，对 inode SFS实现了一些辅助的函数： sfs_bmap_load_nolock：将对应 sfs_inode 的第 index 个索引指向的 block 的索引值取出存到相应的指针指向的单元（ino_store）。该函数只接受 index blocks 的参数。当 index == inode->blocks 时，该函数理解为需要为 inode 增长一个 block。并标记 inode 为 dirty（所有对 inode 数据的修改都要做这样的操作，这样，当 inode 不再使用的时候，sfs 能够保证 inode 数据能够被写回到磁盘）。sfs_bmap_load_nolock 调用的 sfs_bmap_get_nolock 来完成相应的操作，阅读 sfs_bmap_get_nolock，了解他是如何工作的。（sfs_bmap_get_nolock 只由 sfs_bmap_load_nolock 调用） sfs_bmap_truncate_nolock：将多级数据索引表的最后一个 entry 释放掉。他可以认为是 sfs_bmap_load_nolock 中，index == inode->blocks 的逆操作。当一个文件或目录被删除时，sfs 会循环调用该函数直到 inode->blocks 减为 0，释放所有的数据页。函数通过 sfs_bmap_free_nolock 来实现，他应该是 sfs_bmap_get_nolock 的逆操作。和 sfs_bmap_get_nolock 一样，调用 sfs_bmap_free_nolock 也要格外小心。 sfs_dirent_read_nolock：将目录的第 slot 个 entry 读取到指定的内存空间。他通过上面提到的函数来完成。 sfs_dirent_search_nolock：是常用的查找函数。他在目录下查找 name，并且返回相应的搜索结果（文件或文件夹）的 inode 的编号（也是磁盘编号），和相应的 entry 在该目录的 index 编号以及目录下的数据页是否有空闲的 entry。（SFS 实现里文件的数据页是连续的，不存在任何空洞；而对于目录，数据页不是连续的，当某个 entry 删除的时候，SFS 通过设置 entry->ino 为0将该 entry 所在的 block 标记为 free，在需要添加新 entry 的时候，SFS 优先使用这些 free 的 entry，其次才会去在数据页尾追加新的 entry。 注意，这些后缀为 nolock 的函数，只能在已经获得相应 inode 的semaphore才能调用。 Inode的文件操作函数 static const struct inode_ops sfs_node_fileops = { .vop_magic = VOP_MAGIC, .vop_open = sfs_openfile, .vop_close = sfs_close, .vop_read = sfs_read, .vop_write = sfs_write, …… }; 上述sfs_openfile、sfs_close、sfs_read和sfs_write分别对应用户进程发出的open、close、read、write操作。其中sfs_openfile不用做什么事；sfs_close需要把对文件的修改内容写回到硬盘上，这样确保硬盘上的文件内容数据是最新的；sfs_read和sfs_write函数都调用了一个函数sfs_io，并最终通过访问硬盘驱动来完成对文件内容数据的读写。 Inode的目录操作函数 static const struct inode_ops sfs_node_dirops = { .vop_magic = VOP_MAGIC, .vop_open = sfs_opendir, .vop_close = sfs_close, .vop_getdirentry = sfs_getdirentry, .vop_lookup = sfs_lookup, …… }; 对于目录操作而言，由于目录也是一种文件，所以sfs_opendir、sys_close对应户进程发出的open、close函数。相对于sfs_open，sfs_opendir只是完成一些open函数传递的参数判断，没做其他更多的事情。目录的close操作与文件的close操作完全一致。由于目录的内容数据与文件的内容数据不同，所以读出目录的内容数据的函数是sfs_getdirentry，其主要工作是获取目录下的文件inode信息。 "},"lab8/lab8_3_5_dev_file_io_layer.html":{"url":"lab8/lab8_3_5_dev_file_io_layer.html","title":"设备层文件 IO 层","keywords":"","body":"设备层文件 IO 层 在本实验中，为了统一地访问设备，我们可以把一个设备看成一个文件，通过访问文件的接口来访问设备。目前实现了stdin设备文件文件、stdout设备文件、disk0设备。stdin设备就是键盘，stdout设备就是CONSOLE（串口、并口和文本显示器），而disk0设备是承载SFS文件系统的磁盘设备。下面我们逐一分析ucore是如何让用户把设备看成文件来访问。 "},"lab8/lab8_3_5_1_data_structure.html":{"url":"lab8/lab8_3_5_1_data_structure.html","title":"关键数据结构","keywords":"","body":"关键数据结构 为了表示一个设备，需要有对应的数据结构，ucore为此定义了struct device，其描述如下： struct device { size_t d_blocks; //设备占用的数据块个数 size_t d_blocksize; //数据块的大小 int (*d_open)(struct device *dev, uint32_t open_flags); //打开设备的函数指针 int (*d_close)(struct device *dev); //关闭设备的函数指针 int (*d_io)(struct device *dev, struct iobuf *iob, bool write); //读写设备的函数指针 int (*d_ioctl)(struct device *dev, int op, void *data); //用ioctl方式控制设备的函数指针 }; 这个数据结构能够支持对块设备（比如磁盘）、字符设备（比如键盘、串口）的表示，完成对设备的基本操作。ucore虚拟文件系统为了把这些设备链接在一起，还定义了一个设备链表，即双向链表vdev_list，这样通过访问此链表，可以找到ucore能够访问的所有设备文件。 但这个设备描述没有与文件系统以及表示一个文件的inode数据结构建立关系，为此，还需要另外一个数据结构把device和inode联通起来，这就是vfs_dev_t数据结构： // device info entry in vdev_list typedef struct { const char *devname; struct inode *devnode; struct fs *fs; bool mountable; list_entry_t vdev_link; } vfs_dev_t; 利用vfs_dev_t数据结构，就可以让文件系统通过一个链接vfs_dev_t结构的双向链表找到device对应的inode数据结构，一个inode节点的成员变量in_type的值是0x1234，则此 inode的成员变量in_info将成为一个device结构。这样inode就和一个设备建立了联系，这个inode就是一个设备文件。 "},"lab8/lab8_3_5_2_stdout_dev_file.html":{"url":"lab8/lab8_3_5_2_stdout_dev_file.html","title":"stdout设备文件","keywords":"","body":"stdout设备文件 初始化 既然stdout设备是设备文件系统的文件，自然有自己的inode结构。在系统初始化时，即只需如下处理过程 kern_init-->fs_init-->dev_init-->dev_init_stdout --> dev_create_inode --> stdout_device_init --> vfs_add_dev 在dev_init_stdout中完成了对stdout设备文件的初始化。即首先创建了一个inode，然后通过stdout_device_init完成对inode中的成员变量inode->__device_info进行初始： 这里的stdout设备文件实际上就是指的console外设（它其实是串口、并口和CGA的组合型外设）。这个设备文件是一个只写设备，如果读这个设备，就会出错。接下来我们看看stdout设备的相关处理过程。 初始化 stdout设备文件的初始化过程主要由stdout_device_init完成，其具体实现如下： static void stdout_device_init(struct device *dev) { dev->d_blocks = 0; dev->d_blocksize = 1; dev->d_open = stdout_open; dev->d_close = stdout_close; dev->d_io = stdout_io; dev->d_ioctl = stdout_ioctl; } 可以看到，stdout_open函数完成设备文件打开工作，如果发现用户进程调用open函数的参数flags不是只写（O_WRONLY），则会报错。 访问操作实现 stdout_io函数完成设备的写操作工作，具体实现如下： static int stdout_io(struct device *dev, struct iobuf *iob, bool write) { if (write) { char *data = iob->io_base; for (; iob->io_resid != 0; iob->io_resid --) { cputchar(*data ++); } return 0; } return -E_INVAL; } 可以看到，要写的数据放在iob->io_base所指的内存区域，一直写到iob->io_resid的值为0为止。每次写操作都是通过cputchar来完成的，此函数最终将通过console外设驱动来完成把数据输出到串口、并口和CGA显示器上过程。另外，也可以注意到，如果用户想执行读操作，则stdout_io函数直接返回错误值-E_INVAL。 "},"lab8/lab8_3_5_3_stdin_dev_file.html":{"url":"lab8/lab8_3_5_3_stdin_dev_file.html","title":"stdin 设备文件","keywords":"","body":"stdin 设备文件 这里的stdin设备文件实际上就是指的键盘。这个设备文件是一个只读设备，如果写这个设备，就会出错。接下来我们看看stdin设备的相关处理过程。 初始化 stdin设备文件的初始化过程主要由stdin_device_init完成了主要的初始化工作，具体实现如下： static void stdin_device_init(struct device *dev) { dev->d_blocks = 0; dev->d_blocksize = 1; dev->d_open = stdin_open; dev->d_close = stdin_close; dev->d_io = stdin_io; dev->d_ioctl = stdin_ioctl; p_rpos = p_wpos = 0; wait_queue_init(wait_queue); } 相对于stdout的初始化过程，stdin的初始化相对复杂一些，多了一个stdin_buffer缓冲区，描述缓冲区读写位置的变量p_rpos、p_wpos以及用于等待缓冲区的等待队列wait_queue。在stdin_device_init函数的初始化中，也完成了对p_rpos、p_wpos和wait_queue的初始化。 访问操作实现 stdin_io函数负责完成设备的读操作工作，具体实现如下： static int stdin_io(struct device *dev, struct iobuf *iob, bool write) { if (!write) { int ret; if ((ret = dev_stdin_read(iob->io_base, iob->io_resid)) > 0) { iob->io_resid -= ret; } return ret; } return -E_INVAL; } 可以看到，如果是写操作，则stdin_io函数直接报错返回。所以这也进一步说明了此设备文件是只读文件。如果此读操作，则此函数进一步调用dev_stdin_read函数完成对键盘设备的读入操作。dev_stdin_read函数的实现相对复杂一些，主要的流程如下： static int dev_stdin_read(char *buf, size_t len) { int ret = 0; bool intr_flag; local_intr_save(intr_flag); { for (; ret wakeup_flags == WT_KBD) { goto try_again; } break; } } } local_intr_restore(intr_flag); return ret; } 在上述函数中可以看出，如果p_rpos =p_wpos，则表明没有新字符，这样调用read用户态库函数的用户进程就需要采用等待队列的睡眠操作进入睡眠状态，等待键盘输入字符的产生。 键盘输入字符后，如何唤醒等待键盘输入的用户进程呢？回顾lab1中的外设中断处理，可以了解到，当用户敲击键盘时，会产生键盘中断，在trap_dispatch函数中，当识别出中断是键盘中断（中断号为IRQ_OFFSET + IRQ_KBD）时，会调用dev_stdin_write函数，来把字符写入到stdin_buffer中，且会通过等待队列的唤醒操作唤醒正在等待键盘输入的用户进程。 "},"lab8/lab8_3_6_labs_steps.html":{"url":"lab8/lab8_3_6_labs_steps.html","title":"实验执行流程概述","keywords":"","body":"实验执行流程概述 与实验七相比，实验八增加了文件系统，并因此实现了通过文件系统来加载可执行文件到内存中运行的功能，导致对进程管理相关的实现比较大的调整。我们来简单看看文件系统是如何初始化并能在ucore的管理下正常工作的。 首先看看kern_init函数，可以发现与lab7相比增加了对fs_init函数的调用。fs_init函数就是文件系统初始化的总控函数，它进一步调用了虚拟文件系统初始化函数vfs_init，与文件相关的设备初始化函数dev_init和Simple FS文件系统的初始化函数sfs_init。这三个初始化函数联合在一起，协同完成了整个虚拟文件系统、SFS文件系统和文件系统对应的设备（键盘、串口、磁盘）的初始化工作。其函数调用关系图如下所示： 文件系统初始化调用关系图 参考上图，并结合源码分析，可大致了解到文件系统的整个初始化流程。vfs_init主要建立了一个device list双向链表vdev_list，为后续具体设备（键盘、串口、磁盘）以文件的形式呈现建立查找访问通道。dev_init函数通过进一步调用disk0/stdin/stdout_device_init完成对具体设备的初始化，把它们抽象成一个设备文件，并建立对应的inode数据结构，最后把它们链入到vdev_list中。这样通过虚拟文件系统就可以方便地以文件的形式访问这些设备了。sfs_init是完成对Simple FS的初始化工作，并把此实例文件系统挂在虚拟文件系统中，从而让ucore的其他部分能够通过访问虚拟文件系统的接口来进一步访问到SFS实例文件系统。 "},"lab8/lab8_3_7_file_op_implement.html":{"url":"lab8/lab8_3_7_file_op_implement.html","title":"文件操作实现","keywords":"","body":"文件操作实现 "},"lab8/lab8_3_7_1_file_open.html":{"url":"lab8/lab8_3_7_1_file_open.html","title":"打开文件","keywords":"","body":"打开文件 有了上述分析后，我们可以看看如果一个用户进程打开文件会做哪些事情？首先假定用户进程需要打开的文件已经存在在硬盘上。以user/sfs_filetest1.c为例，首先用户进程会调用在main函数中的如下语句： int fd1 = safe_open(\"sfs\\_filetest1\", O_RDONLY); 从字面上可以看出，如果ucore能够正常查找到这个文件，就会返回一个代表文件的文件描述符fd1，这样在接下来的读写文件过程中，就直接用这样fd1来代表就可以了。那这个打开文件的过程是如何一步一步实现的呢？ 通用文件访问接口层的处理流程 首先进入通用文件访问接口层的处理流程，即进一步调用如下用户态函数： open->sys_open->syscall，从而引起系统调用进入到内核态。到了内核态后，通过中断处理例程，会调用到sys_open内核函数，并进一步调用sysfile_open内核函数。到了这里，需要把位于用户空间的字符串\"sfs_filetest1\"拷贝到内核空间中的字符串path中，并进入到文件系统抽象层的处理流程完成进一步的打开文件操作中。 文件系统抽象层的处理流程 分配一个空闲的file数据结构变量file在文件系统抽象层的处理中，首先调用的是file_open函数，它要给这个即将打开的文件分配一个file数据结构的变量，这个变量其实是当前进程的打开文件数组current->fs_struct->filemap[]中的一个空闲元素（即还没用于一个打开的文件），而这个元素的索引值就是最终要返回到用户进程并赋值给变量fd1。到了这一步还仅仅是给当前用户进程分配了一个file数据结构的变量，还没有找到对应的文件索引节点。 为此需要进一步调用vfs_open函数来找到path指出的文件所对应的基于inode数据结构的VFS索引节点node。vfs_open函数需要完成两件事情：通过vfs_lookup找到path对应文件的inode；调用vop_open函数打开文件。 找到文件设备的根目录“/”的索引节点需要注意，这里的vfs_lookup函数是一个针对目录的操作函数，它会调用vop_lookup函数来找到SFS文件系统中的“/”目录下的“sfs_filetest1”文件。为此，vfs_lookup函数首先调用get_device函数，并进一步调用vfs_get_bootfs函数（其实调用了）来找到根目录“/”对应的inode。这个inode就是位于vfs.c中的inode变量bootfs_node。这个变量在init_main函数（位于kern/process/proc.c）执行时获得了赋值。 通过调用vop_lookup函数来查找到根目录“/”下对应文件sfs_filetest1的索引节点，，如果找到就返回此索引节点。 把file和node建立联系。完成第3步后，将返回到file_open函数中，通过执行语句“file->node=node;”，就把当前进程的current->fs_struct->filemap[fd]（即file所指变量）的成员变量node指针指向了代表sfs_filetest1文件的索引节点inode。这时返回fd。经过重重回退，通过系统调用返回，用户态的syscall->sys_open->open->safe_open等用户函数的层层函数返回，最终把把fd赋值给fd1。自此完成了打开文件操作。但这里我们还没有分析第2和第3步是如何进一步调用SFS文件系统提供的函数找位于SFS文件系统上的sfs_filetest1文件所对应的sfs磁盘inode的过程。下面需要进一步对此进行分析。 SFS文件系统层的处理流程 这里需要分析文件系统抽象层中没有彻底分析的vop_lookup函数到底做了啥。下面我们来看看。在sfs_inode.c中的sfs_node_dirops变量定义了“.vop_lookup = sfs_lookup”，所以我们重点分析sfs_lookup的实现。注意：在lab8中，为简化代码，sfs_lookup函数中并没有实现能够对多级目录进行查找的控制逻辑（在ucore_plus中有实现）。 sfs_lookup有三个参数：node，path，node_store。其中node是根目录“/”所对应的inode节点；path是文件sfs_filetest1的绝对路径/sfs_filetest1，而node_store是经过查找获得的sfs_filetest1所对应的inode节点。 sfs_lookup函数以“/”为分割符，从左至右逐一分解path获得各个子目录和最终文件对应的inode节点。在本例中是调用sfs_lookup_once查找以根目录下的文件sfs_filetest1所对应的inode节点。当无法分解path后，就意味着找到了sfs_filetest1对应的inode节点，就可顺利返回了。 当然这里讲得还比较简单，sfs_lookup_once将调用sfs_dirent_search_nolock函数来查找与路径名匹配的目录项，如果找到目录项，则根据目录项中记录的inode所处的数据块索引值找到路径名对应的SFS磁盘inode，并读入SFS磁盘inode对的内容，创建SFS内存inode。 "},"lab8/lab8_3_7_2_file_read.html":{"url":"lab8/lab8_3_7_2_file_read.html","title":"读文件","keywords":"","body":"读文件 读文件其实就是读出目录中的目录项，首先假定文件在磁盘上且已经打开。用户进程有如下语句： read(fd, data, len); 即读取fd对应文件，读取长度为len，存入data中。下面来分析一下读文件的实现。 通用文件访问接口层的处理流程 先进入通用文件访问接口层的处理流程，即进一步调用如下用户态函数：read->sys_read->syscall，从而引起系统调用进入到内核态。到了内核态以后，通过中断处理例程，会调用到sys_read内核函数，并进一步调用sysfile_read内核函数，进入到文件系统抽象层处理流程完成进一步读文件的操作。 文件系统抽象层的处理流程 1) 检查错误，即检查读取长度是否为0和文件是否可读。 2) 分配buffer空间，即调用kmalloc函数分配4096字节的buffer空间。 3) 读文件过程 [1] 实际读文件 循环读取文件，每次读取buffer大小。每次循环中，先检查剩余部分大小，若其小于4096字节，则只读取剩余部分的大小。然后调用file_read函数（详细分析见后）将文件内容读取到buffer中，alen为实际大小。调用copy_to_user函数将读到的内容拷贝到用户的内存空间中，调整各变量以进行下一次循环读取，直至指定长度读取完成。最后函数调用层层返回至用户程序，用户程序收到了读到的文件内容。 [2] file_read函数 这个函数是读文件的核心函数。函数有4个参数，fd是文件描述符，base是缓存的基地址，len是要读取的长度，copied_store存放实际读取的长度。函数首先调用fd2file函数找到对应的file结构，并检查是否可读。调用filemap_acquire函数使打开这个文件的计数加1。调用vop_read函数将文件内容读到iob中（详细分析见后）。调整文件指针偏移量pos的值，使其向后移动实际读到的字节数iobuf_used(iob)。最后调用filemap_release函数使打开这个文件的计数减1，若打开计数为0，则释放file。 SFS文件系统层的处理流程 vop_read函数实际上是对sfs_read的包装。在sfs_inode.c中sfs_node_fileops变量定义了.vop_read = sfs_read，所以下面来分析sfs_read函数的实现。 sfs_read函数调用sfs_io函数。它有三个参数，node是对应文件的inode，iob是缓存，write表示是读还是写的布尔值（0表示读，1表示写），这里是0。函数先找到inode对应sfs和sin，然后调用sfs_io_nolock函数进行读取文件操作，最后调用iobuf_skip函数调整iobuf的指针。 在sfs_io_nolock函数中，先计算一些辅助变量，并处理一些特殊情况（比如越界），然后有sfs_buf_op = sfs_rbuf,sfs_block_op = sfs_rblock，设置读取的函数操作。接着进行实际操作，先处理起始的没有对齐到块的部分，再以块为单位循环处理中间的部分，最后处理末尾剩余的部分。每部分中都调用sfs_bmap_load_nolock函数得到blkno对应的inode编号，并调用sfs_rbuf或sfs_rblock函数读取数据（中间部分调用sfs_rblock，起始和末尾部分调用sfs_rbuf），调整相关变量。完成后如果offset + alen > din->fileinfo.size（写文件时会出现这种情况，读文件时不会出现这种情况，alen为实际读写的长度），则调整文件大小为offset + alen并设置dirty变量。 sfs_bmap_load_nolock函数将对应sfs_inode的第index个索引指向的block的索引值取出存到相应的指针指向的单元（ino_store）。它调用sfs_bmap_get_nolock来完成相应的操作。sfs_rbuf和sfs_rblock函数最终都调用sfs_rwblock_nolock函数完成操作，而sfs_rwblock_nolock函数调用dop_io->disk0_io->disk0_read_blks_nolock->ide_read_secs完成对磁盘的操作。 "}}